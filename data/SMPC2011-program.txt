                                                                                     SMPC 2011 Program and abstracts, Page:       
                                                                                                                               7 
                                                                              
                                                    Titles and abstracts for talks 
 
1 Effects of Musical Instrument on Absolute Pitch Ability 
 
Vivek V. Sharma* & Daniel J. Levitin 
 
McGill University, Montréal, Québec, Canada 
* = Corresponding author, vivek.sharma@mail.mcgill.ca 
 
Persons who possess Absolute Pitch (AP), the ability to name musical tones without an external reference, often report 
training on a musical instrument from a young age (Sergeant, 1969). To understand the acquisition process of AP, it would be 
useful to know how the musical instruments played by AP possessors influence the development of pitch templates in their 
long‐term memory. We hypothesize that players of fixed‐pitched instruments identify tones faster and more accurately than 
players of variable‐pitched instruments because of the former group's greater exposure to precise pitch values, and the 
consequent preferential tuning of auditory system neurons to those values. To test our hypothesis, we examined how AP 
musicians labeled in tune and detuned pitches. We tested 10 pianists and 10 violinists. Tones of 3 different timbres were 
presented: piano, violin and sinusoidal. Their frequencies formed a continuum of pitch classes that were individually 
separated by 20¢ intervals and ranged from C4 to C5, inclusive, where A4 = 440 Hz. Dependent variables were the percentages 
of correctly labeled tones and reaction times. The participants also rated the goodness‐of‐fit of each tone using a continuous 
scale. Because the piano is fixed‐pitched, it may repetitively reinforce the codification of pitch to verbal labels within the long‐
term memory more effectively than the variable‐pitched violin. We suspect that the study supports the hypothesized effects of 
tone mapping and musical training on AP acquisition, perception and memory. 
 
 
 
2 Emotional Judgment in Absolute Pitch 
 
Psyche Loui*, Anna Zamm, Matthew Sachs, and Gottfried Schlaug 
 
Beth Israel Deaconess Medical Center and Harvard Medical School, Boston, MA, USA  
* = Corresponding author, ploui@bidmc.harvard.edu 
 
Absolute Pitch (AP) is a unique phenomenon characterized by the ability to name the pitch class of any note without a 
reference. In recent years, AP has become a model for exploring nature‐nurture interactions. While past research focused on 
differences between APs and controls in domains such as pitch naming, little is known about how AP possessors tackle other 
musical tasks. In this study we asked whether AP possessors recruit different brain resources from controls during a task in 
which Aps are anecdotally similar to controls: the task of emotional judgment. Functional MRI was acquired from 15 APs and 
15 controls (matched in age, sex, ethnicity, and musical training) as they listened to musical sound clips and rated the arousal 
level of each clip on the scale of 1 (low‐arousal) to 4 (high‐arousal), relative to a silent rest condition. Additionally, we acquired 
Diffusion Tensor Imaging (DTI) data to investigate white matter differences between AP possessors and controls. Behavioral 
results showed no significant difference between APs and controls. However, a second‐level contrast between music and rest 
conditions showed that APs recruited more neural activity in left Heschl’s gyrus (primary auditory cortex). Another second‐
level contrast between high‐arousal and low‐arousal music revealed increased activity in APs in the left posterior superior 
temporal gyrus (secondary auditory cortex). DTI showed that APs had larger connections between the left posterior superior 
temporal gyrus and the left posterior middle temporal gyrus, regions thought to be involved in sound perception and 
categorization respectively. Despite a behavioral task designed to minimize differences between APs and controls, we 
observed significant between‐group differences in brain activity and connectivity. Results suggest that AP possessors 
obligatorily recruit extra neural resources for perceiving and categorizing musical sounds. 
 
                                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                                       8 
 
3 The Absolute Pitch Continuum: Evidence of Incipient AP in Musical Amateurs 
 
Elizabeth W. Marvin (1)*, Elissa L. Newport (2) 
 
(1) Eastman School of the University or Rochester, Rochester, NY USA (2) University of Rochester, Rochester, NY USA 
* = Corresponding author, bmarvin@esm.rochester.edu 
 
Musicians typically view absolute pitch (AP) as an all‐or‐nothing proposition.  Recent research reveals a different picture, 
however, suggesting that AP abilities exist along a continuum and that many listeners, some without extensive musical 
training, encode pitch information accurately in memory (e.g., Bermudez & Zatorre, 2009; Ross, Olsen, Marks & Gore, 2004; 
Schellenberg & Trehub, 2003).  This paper reports new data that support the continuum theory.  Three groups of participants 
(n = 254)—professional music theorists, freshman music majors, and liberal‐arts students—took a pitch‐naming test and an 
implicit learning test requiring them to discriminate between pitch patterns learned during a familiarization phase and their 
transpositions (see also Saffran & Griepentrog, 2001).  In a previous test of AP and non‐AP musicians, scores on the naming 
and implicit learning tests were highly correlated (Marvin & Newport, 2008).  In the current work, those who showed AP on 
pitch naming (n = 31) scored significantly higher than those who did not, verifying that the implicit learning test does measure 
pitch memory, without requiring pitch labels.   Interestingly, examination of individual scores on the implicit learning task 
revealed 12 “incipient AP” participants (some from each group), who scored 88‐100% correct on the implicit learning test (as 
high as AP participants), but averaged only 34% correct on pitch naming.  This provides the unusual opportunity to examine 
the pitch discrimination and memory abilities of a population of listeners who appear to exhibit strong AP but without 
extensive musical training or pitch labeling strategies as part of AP.  On‐going research tests these listeners for microtonal 
pitch discrimination, nonmusical memory (digit span), and musical memory (a musical digit‐span analog).  Preliminary data 
show comparable scores for AP musicians and for incipient‐AP listeners, even those who are musical amateurs.  Those with 
high scores on the implicit learning task do not score significantly higher on memory tests than controls, though they show 
better pitch discrimination in some registers. 
 
 
4 Identifying Absolute Pitch Possessors Without Using A Note­Naming Task.                                                          
 
Ronald Weisman*, Laura‐Lee Balkwill, Marisa Hoeschele, Michele Moscicki, and Christopher Sturdy 
 
Queens Unversity, Kinston, Ontario, Canada 
* = Corresponding author, ronald.weisman@queensu.ca  
 
Most researchers measure AP using note‐naming tasks that presume fluency with the scales of Western music. If note naming 
constitutes the only measure, then by fiat, only trained musicians can possess AP. Here we report on an AP test that does not 
require a note‐naming response. The participants were 60 musicians, who self‐reported AP. Our pitch chroma labeling task 
was adapted from challenging operant go, no‐go discriminations (Weisman, Niegovan, Williams, & Sturdy, 2004) used to test 
songbirds, rats, and humans with tones mistuned to the musical scale. In our pitch‐labeling task, we presented sinewave tones 
tuned to the 12‐note equal‐temperament scale, in a discrimination between the first and second 6 notes in octaves four, five, 
and six. Results were validated against Athos, Levinson, Kisler, et al.'s (2007) sinewave note‐naming test of AP. Actual AP 
possessors (n=15) began music training earlier and had more music training than nonpossessors (n=45), but 25 
nonpossessors matched to AP possessors in experience had no higher AP scores than other nonpossessors. Here for simplicity 
we report percent correct scores for the pitch labeling task, but d', A', and percent correct measures were all highly correlated, 
rs >.90. Over trials AP possessors came to label the half‐octave membership of the 36 tones with M=90% accuracy; 
nonpossessors scored only slightly better than chance, M =55% correct. Most important, the pitch‐labeling task successfully 
identified the AP status of 58 of 60 participants on Athos et al.'s test. In future studies, the pitch‐labeling task will be converted 
to a web‐based protocol to test large numbers of nonmusicians. Then, using our labeling task in conjunction with Ross's 
(2004) reproduction test, we hope to accurately identify nonmusician AP possessors or with enough participants from several 
populations cast doubt on the hypothesis that nonmusicians can possess AP. 
                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                      9 
 
5 An Empirical Test of the Honest Signal Hypothesis 
 
Lisa Chan (1)*, Lucy M. McGarry (1), Vanessa Corpuz (1), Frank A. Russo (1) 
 
(1) Ryerson University, Toronto, Canada  
* = Corresponding author, lisa.chan@psych.ryerson.ca 
 
Several theorists have proposed that music might have functioned in our evolutionary history as an honest signal (Cross & 
Woodruff, 2008; Levitin, 2008; also see Darwin, 1872). The term honest signal originates in ethology, where it has been used 
to refer to a signal that has evolved to benefit the receiver as well as the signaler (e.g., the dart frog “advertises” its chemical 
defenses to predators with conspicuous skin coloration).  In this study we assess whether music may be more “honest” than 
speech with regard to revealing a performer’s true (experienced) emotions. Performers were induced with a happy or sad 
emotion using an emotion induction procedure involving music and guided imagery. Subjective evaluation of experienced 
emotion suggested that the inductions were highly effective. Performers were subsequently asked to perform sung or spoken 
phrases that were intended to convey either happiness or sadness. The intended emotion could thus be characterized as 
congruent or incongruent with the performer’s induced emotion. Recordings of performances were evaluated by participants 
with regard to valence and believability.Valence ratings revealed that performers were successful in expressing the intended 
emotion in the emotionally congruent condition (i.e., higher valence ratings for intended happy than for intended sad) and 
unsuccessful for the emotionally incongruent condition (i.e., intermediate valence ratings for intended happy and for intended 
sad). Critically, song led to higher believability ratings than speech, owed largely to the high believability of song produced 
with sad expression. These results will be discussed in the context of the honest signal hypothesis and recent evidence for 
mimicry in perception of sung emotion. 
 
 
 
 
 
6 Defining Music as a Step Toward Explaining its Origin   
 
Richard Parncutt * 
 
University of Graz, Austria          
* = Corresponding author, richard.parncutt@uni‐graz.at                   
 
Since the breakdown of tonality (Wagner to Schoenberg) and the emergence of ethnomusicology, musicologists have been 
reluctant to define music, since definitions always depend on historical, cultural, and academic context. But these historical 
developments merely showed that music need not be tonal and that the distinguishing features of Western music should be 
absent from a general definition. They also drew attention to the different meanings of “music” and its translations in different 
cultures and periods. Today’s theories of the origin(s) of music differ in part because researchers still have different implicit 
definitions of music. The problem can be solved by specifying exactly what music is assumed to be – which incidentally also 
allows “musicology” to be defined. A definition might run as follows. Both music and language are acoustic, meaningful, 
gestural, rhythmic/melodic, syntactic, social, emotional, and intentional; music and language differ in that music is less lexical, 
more repetitive, more spiritual, less socially essential, and more expertise based. Of course all the terms in these lists need to 
be explained and if possible operationalized, and individual claims supported. Given the paucity of reliable information about 
the behavior of early humans that could have influenced music’s development, we need to explore new approaches to 
evaluating theories of its origin. One approach is to evaluate the extent to which each theory can parsimoniously account for or 
predict the listed features. Another is to evaluate the quantity and quality of relevant empirical studies that are consistent with 
the specific processes posited in the theory. I will present details of this new systematic approach and briefly show how it can 
be used to evaluate theories such as those based on mate selection, social cohesion, and motherese. 
                                                                                           SMPC 2011 Program and abstracts, Page:       
                                                                                                                                   10 
 
7 Musical Emotions: Functions, Origins, Evolution  
 
Leonid Perlovsky * 
 
Harvard University, Cambridge, and Air Force Research Lab., Hanscom AFB, MA, USA   
* = Corresponding author, Leonid.Perlovsky@hanscom.af.mil                            
 
Music seems an enigma. Existing theories cannot explain its cognitive functions or evolutionary origins. Here a hypothesis is 
proposed based on synthesis of cognitive science and mathematical models of the mind, which describes a fundamental role of 
music in the functioning and evolution of the mind, consciousness, and cultures. The talk considers ancient theories of music 
as well as contemporary theories advanced by leading authors in this field. Then it discusses a hypothesis that promises to 
unify the field and proposes a theory of musical origin based on a fundamental role of music in cognition and evolution of 
consciousness and culture. The talk considers a split in the vocalizations of proto‐humans into two types: one less emotional 
and more concretely‐semantic, evolving into language, and the other preserving emotional connections along with semantic 
ambiguity, evolving into music. The proposed hypothesis departs from other theories in considering specific mechanisms of 
the mind‐brain, which required evolution of music parallel with evolution of cultures and languages. Arguments are reviewed 
that evolution of language toward the semantically powerful tool of today required emancipation from emotional 
encumbrances. The opposite, no less powerful mechanisms required a compensatory evolution of music toward more 
differentiated and refined emotionality. The need for refined music in the process of cultural evolution is grounded in 
fundamental mechanisms of the mind. This is why today’s human mind and cultures cannot exist without today’s music. The 
proposed hypothesis gives a basis for future analysis of why different evolutionary paths of languages were paralleled by 
different evolutionary paths of music. Approaches toward experimental verification of this hypothesis in psychological and 
neuroimaging research are discussed. 
 
                   
 
8 Music as a Marker of Human Migrations: An Analysis of Song Structure vs. Singing 
Style  
 
Patrick Savage (1)*, Tom Rzeszutek (1), Victor Grauer (2), Ying‐fen Wang (3), Jean Trejaut (4), Marie Lin (4), Steven Brown (1) 
 
(1) Department of Psychology, Neuroscience & Behaviour, McMaster University, Hamilton, Canada, (2) Independent scholar, Pittsburgh, USA, (3) Graduate 
Institute of Musicology, National Taiwan University, Taipei, Taiwan, (4) Transfusion Medicine Laboratory, Mackay Memorial Hospital, Taipei, Taiwan 
* = Corresponding author, savagepe@mcmaster.ca 
 
The  discovery  that  our  genes  trace  the  migration  of  all  humans  back  to  a  single  African  “mitochondrial  Eve”  has  had  an 
enormous  impact  on  our  understanding  of  human  pre‐history.  Grauer  (2006)  has  claimed  that  music,  too,  can  trace  pre‐
historic human migrations, but critics argue that music’s time‐depth is too shallow (i.e., music changes too rapidly to preserve 
ancient  relationships).  We  predicted  that  if  any  musical  features  were  to  have  the  necessary  time‐depth,  they  would  be  the 
structural  features  –  rather  than  the  performance  features  –  of  traditional  group  songs.  To  test  this  prediction,  we  used 
Cantometric codings of 222 traditional group songs from 8 Taiwanese aboriginal tribes to create separate distance matrices 
for  music  based  on  either  song  structure  or  singing  style.  Surprisingly,  both  distance  matrices  were  significantly  correlated 
(p<0.01)  with  genetic  distances  based  on  mitochondrial  DNA  –  a  migration  marker  with  well‐established  time‐depth. 
However, in line with our prediction, structure (r2=0.27) accounted for twice as much variance as performance style (r2=0.13). 
Independent coding of these songs using a new classification scheme that focuses exclusively on structural features confirmed 
the  correlation  with  genes  (r2=0.19).  Further  exploratory  analyses  of  the  different  structural  sub‐categories  revealed  that 
features  related  to  pitch  (e.g.,  interval  size,  scale)  were  more  strongly  correlated  with  genes  (r2=0.24)  than  those  related  to 
rhythm  (r2=0.12),  text  (r2=0.05),  texture  (r2=0.08),  or  form  (r2=0.13).  These  results  suggest  that,  while  song  structure  – 
especially pitch – may be a stronger migration marker than singing style, many types of musical features may have sufficient 
time‐depth to track pre‐historic population migrations. 
                                                                         SMPC 2011 Program and abstracts, Page:       
                                                                                                                 11 
 
9 The Power of Music: The Composition and Perception of Emotion in Music 
 
Megan Trenck*, Peter Martens, & Jeff Larsen  
 
Texas Tech University, Lubbock, TX, USA 
* = Corresponding author, metrenck@gmail.com 
 
Melody has had a prominent place in recent studies on the emotional content of music such as Brower (2002) and Krumhansl 
(2002). Further, Collier and Hubbard (2001) claim “that emotional valence may be based more on the horizontal rather than 
the vertical aspect of music.” To investigate some specifics of what makes emotions attributable to melody, a combination of 
undergraduate and graduate music majors at Texas Tech University were asked to compose a melody depicting either 
happiness or sadness. No restrictions were placed on the use of time signature, key signature, or tempo, but melodies were 
restricted to one monophonic line of music. Melodies were analyzed for their distribution of first‐order intervals (intervals 
between adjacent notes), melodic spans (distance a melody travels in one direction before a contour change, measured in 
semitones), and additional ways melodies behave relative to their evolving pitch mean. The findings corroborate some of the 
perceptual conclusions of Gabrielsson (2009) and Collier and Hubbard (2001), who found that narrow ranges bring out 
sadness whereas happiness is derived from wide melodic ranges and larger leaps. Next, a perceptual study was conducted to 
help determine how well melodies portrayed the intended emotions. Forty‐nine undergraduate music majors rated their 
perceptions in each of the melodies of twelve different emotions, half sad spectrum and half happy spectrum emotions.  As 
expected, melodies depicting happiness were composed in the major mode and melodies depicting sadness were largely 
composed in the minor mode. Ratings of emotions seemed not only to be based on the mode of the melody, but also on the 
note density, which appeared to amplify or dampen effects of mode on perceived emotions. Confirming the results of the 
perceptual study, these methods of melodic analysis suggest how composers might attempt to portray different emotions 
within the musical domain of melody. 
 
 
10 The Effects of Expertise on Movement­mediated Emotional Processing in Music 
 
Lucy McGarry (1)*, and Frank Russo (1) 
 
(1) Ryerson University, Toronto, Canada 
* = Corresponding author, lmcgarry@psych.ryerson.ca 
 
Many studies have demonstrated that mimicry of emotional gestures aids in their recognition. In the context of music, mimicry 
of performance occurs automatically and has been hypothesized to mediate musical understanding (Livingstone, Thompson & 
Russo, 2009).  In the current study, we examined whether exaggerated mimicry of the emotional themes in music, such as that 
which occurs during dance, enhances understanding of emotion conveyed by music in a similar way to motor mimicry in social 
contexts. Thirty dancers and 33 non‐dancers were tested using a within‐subjects design.  Participants listened to musical clips 
from the Bach Cello Suites, selected based on pilot ratings to be high in arousal and valence (happy), or low in arousal and 
valence (sad). During music listening, participants followed randomized instructions to move hands freely to the music, sit 
still, or move in a constrained manner.  Afterwards, all song clips were heard again while physiological responses and ratings 
were taken. Results demonstrated a beneficial effect of free movement on subsequent emotional engagement with music for 
dancers only.  For each measurement we computed a polarization score by calculating the difference between responses to 
happy (high arousal, positive valence) and sad (low arousal, negative valence) music.  Zygomatic (smiling) muscle activation, 
skin conductance levels, valence and arousal ratings all showed enhanced polarization in the free movement condition.  In 
addition, zygomatic activity mediated valence and arousal ratings in dancers.  Non‐dancers did not demonstrate these 
polarizations. Our results suggest that movement experts like dancers rely more on movement to process emotional stimuli in 
music.  Future studies should examine whether this is due to a personality difference between dancers and non‐dancers, or an 
expertise effect.   
                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                    12 
11 The Emotional Connotations of Diatonic Modes 
 
David Temperley *, & Daphne Tan 
 
Eastman School of Music, Rochester, NY, USA 
* = Corresponding author, dtemperley@esm.rochester.edu 
 
Diatonic modes are the scales that result when the tonic is shifted to different positions of the diatonic (major) scale. Given the 
C major scale, for example, the tonic may be left at C (Ionian or “major” mode) or shifted to D (Dorian), E (Phrygian), F 
(Lydian), G (Mixolydian) or A (Aeolian or “natural minor”). Many musical styles employ diatonic modes beyond Ionian, 
including rock and other contemporary popular styles. Experimental studies have shown that the major mode and common‐
practice minor mode (which is not the same as Aeolian mode) have positive and negative emotional connotations, 
respectively. But what of the other diatonic modes? One possible hypothesis is that modes with more raised scale degrees 
have more positive (“happier”) emotional connotations (Huron, Yim, & Chordia, 2010). Another possible hypothesis is that the 
connotations of modes are driven mainly by familiarity, therefore the scales most similar to the major mode (the most familiar 
mode for most Western listeners) would be happiest. The predictions of these two hypotheses are partially convergent, but 
not totally: in particular, the Lydian mode is predicted to be happier than major by the “height” hypothesis but less happy by 
the “familiarity” hypothesis. In the current experiment, a set of diatonic melodies was composed; variants of each melody were 
constructed in each of the six different modes. In a forced‐choice design, non‐musician participants heard pairs of variants (i.e. 
the same melody in two different modes, with a fixed tonic of C) and had to judge which was happier. The data reflect a 
consistent pattern, with happiness decreasing as flats are added. Lydian is judged less happy than major, however, supporting 
the “familiarity” hypothesis over the “height” hypothesis. 
 
 
 
 
 
12 Emotional Responses to (Modern) Modes 
 
Peter Martens*, Jeff Larsen 
 
Texas Tech University, Lubbock, TX,  USA 
* = Corresponding author, peter.martens@ttu.edu 
 
By the 16th century, a bipartite categorization of modes and their affect was common, based on the quality of the mode’s third 
scale step.  Modes with a minor 3rd above their initial pitch in this position were grouped together as sad, those with a major 
3rd in this position, happy (e.g. Zarlino, 1558). Recent research has substantiated these associations with modern major and 
minor scales (minor=sad, major=happy).  The goal of this study is to explore if and how subjects differentiate scale structures 
that lie somewhere between major and minor scales on the basis of emotional content. Six four‐bar diatonic melodies were 
newly composed, with each melody cast in the six most historical “modern” modes: Ionian, Dorian, Phrygian, Lydian, 
Mixolydian, and Aeolian.  Stimuli were created using a classical guitar sound within Logic software, and event density was held 
constant.  In a pilot study subjects rated composers' intent in terms of eliciting happiness and sadness for three of the melodies 
in all six modes.  The stimuli were presented in one of two random orders, and subjects heard each stimulus once.  Preliminary 
results indicate that a simple major/minor categorization does not sufficiently explain subject responses.  As expected, the 
Ionian mode (major) and the Lydian mode were strongly associated with happiness overall, but not significantly more so than 
Aeolian (natural minor).  By contrast, Dorian stood alone as having a strong association with sadness.  Phrygian was weakly 
happy, while Mixolydian responses were neutral.  Why might Dorian be, to misquote Nigel Tufnel, “the saddest of all modes?”  
The Dorian mode contains a minor third and minor seventh scale step, but a major sixth.  This is a common mixture of 
characteristics from the major and minor scales (e.g. 1960s folk music), which perhaps heightened arousal when listening to 
these generally minor‐sounding Dorian melodies, and thus the enlarged effect. 
                                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                                     13 
 
13 Production and Perception of Facial Expressions during Vocal Performance 
 
Steven R. Livingstone*, Caroline Palmer and Marcelo Wanderley (1), William Forde Thompson (2) 
 
(1) McGill University, Montreal, Canada (2) Macquarie University, Sydney, Australia 
* = steven.livingstone@mcgill.ca 
 
Much empirical and theoretical research over the last decade concerns the production and perception of facial and vocal 
expression. Research has predominantly focused on static representations of facial expressions (photographs), despite the fact 
that facial and vocal expressions are dynamic and unfold over time. The role of this dynamic information in emotional 
communication is unclear. We report two experiments on the role of facial expressions in the production and perception of 
emotions in speech and song. In Experiment 1, twelve singers with vocal experience spoke or sung statements with one of five 
emotional intentions (neutral, happy, very happy, sad and very sad). Participants’ facial movements were recorded with 
motion capture. Functional Data Analyses were applied to marker trajectories for the eyebrow, lip corner, and lower lip. 
Functional analyses of variance on marker trajectories by emotion indicated significantly different trajectories across emotion 
conditions for all three facial markers. Emotional valence was differentiated by movement of the lip corner and eyebrow. Song 
exhibited significantly larger movements than Speech for the lower‐lip, but did not differ significantly for motion of the lip 
corner and eyebrow. Interestingly, movements in speech and song were found prior to the onset of vocal production, and 
continued long after vocal production had ended. The role of these extra‐vocalisation movements was examined in Experiment 
2, in which participants judged the emotional valence of recordings of speaker‐singers from Experiment 1. Listeners viewed 
(no audio) the emotional intentions (neutral, very happy, very sad) in different presentation modes: pre‐vocal‐production, 
vocal‐production, and post‐vocal‐production. Preliminary results indicate that participants were highly accurate at identifying 
all emotions during vocal‐production and post‐vocal‐production, but were significantly less accurate for pre‐vocal production. 
These findings suggest that speech and song share facial expressions for emotional communication, transcending differences 
in production demands. 
 
 
14 Differential Effects of Arousal and Pleasantness in Crossmodal Emotional 
Transfer from the Musical to the Complex Visual Domain 
 
Manuela M. Marin (1)*, Bruno Gingras (2), Joydeep Bhattacharya (1) (3) 
 
(1) Department of Psychology, Goldsmiths, University of London, UK, (2) Department of Cognitive Biology, University of Vienna, Austria, (3) Commission of 
Scientific Visualization, Austrian Academy of Sciences, Vienna, Austria 
* = Corresponding author, manuela.m.marin@gmail.com 
 
The crossmodal priming paradigm is a new approach to address basic questions about musical emotions. Recent behavioural 
and physiological evidence suggests that musical emotions can modulate the valence of visually evoked emotions, especially 
those that are induced by faces and are emotionally ambiguous (Chen, Yuan, Huang, Chen, & Li, 2008; Logeswaran & 
Bhattacharya, 2009; Tan, Spackman, & Bezdek, 2007). However, arousal plays a crucial role in emotional processing (Lin, 
Duann, Chen, & Jung, 2010; Nielen, Heslenfeld, Heinen, Van Strienen, Witter, Jonker & Veltman, 2010; Russell, 1980) and may 
have confounded these priming effects. We investigated the role of arousal in crossmodal priming by combining musical 
primes (Romantic piano music) differing in arousal and pleasantness with complex affective pictures taken from the 
International Affective Picture System (IAPS). In Experiment 1, thirty‐two participants (16 (8 female) musicians, 16 (8 female) 
non‐musicians) reported their felt pleasantness (i.e. valence) and arousal in response to musical primes and visual targets, 
presented separately. In Experiment 2, forty non‐musicians (20 female) rated felt arousal and pleasantness in response to 
visual targets after having listened to musical primes. Experiment 3 sought to rule out the possibility of any order effects of the 
subjective ratings and responses of fourteen non‐musicians were collected. The results of Experiment 1 indicated that musical 
training was associated with elevated arousal ratings in response to unpleasant musical stimuli, whereas gender affected the 
coupling‐strength between arousal and pleasantness in the visual emotion space. Experiment 2 showed that musical primes 
modulated felt arousal in response to complex pictures but not pleasantness, which was further replicated in Experiment 3. 
These findings provide strong evidence for the differential effects of arousal and pleasantness in crossmodal emotional 
transfer from the musical to the complex visual domain and demonstrate the effectiveness of crossmodal priming paradigms 
in general emotion research. 
 
                                                                                   SMPC 2011 Program and abstracts, Page:       
                                                                                                                           14 
 
15 Music Can Convey Movement like Prosody in Speech 
 
Stephen C. Hedger (1)*, Howard C. Nusbaum (1), Berthold Hoeckner (1) 
 
(1) The University of Chicago: Chicago, IL, U.S.A  
* = Corresponding author, shedger@uchicago.edu 
 
Analog variation in the prosody of speech has recently been shown to communicate referential and descriptive information 
about objects (Shintel & Nusbaum, 2007). Given that composers have used similar means to putatively communicate with 
music, we investigated whether acoustic variation of musical properties can analogically convey descriptive information about 
an object. Specifically, we tested whether temporal structure in music is integrated into an analog perceptual representation as 
a natural part of listening.  Listeners heard sentences describing objects and the sentences were underscored with accelerating 
or decelerating music.  After each sentence‐music combination, participants saw a picture of a still or moving object and 
decided whether it was mentioned in the sentence.  Object recognition was faster when musical motion matched visually 
depicted motion.  These results suggest that visuo‐spatial referential information can be analogically conveyed and 
represented by music.   
 
 
 
 
 
 
 
 
 
 
 
16 What Does Seeing the Performer Add? It Depends on Musical Style, Amount of 
Stage Behavior, and Audience Expertise 
 
Carol Lynne Krumhansl (1)*, Jennifer Huang (1, 2) 
 
(1) Cornell University, Ithaca, NY USA (2) Harvard University, Cambridge MA USA 
* =Corresponding author, clk4@cornell.edu 
 
The purpose of this study was to examine the effects of stage behavior, expertise, composer, and modality of presentation on 
structural, emotional, and summary ratings of piano performances. Twenty‐four musically trained and 24 untrained 
participants rated two‐minute excerpts of pieces by Bach, Chopin, and Copland, each performed by the same pianist, who was 
asked to vary his stage behavior from minimal to natural to exaggerated. Participants rated the performances under either 
audio‐only or audiovisual conditions.  There were strong effects of composer, stage behavior, and response scale type, as well 
as interactions involving these three variables and modality of presentation.  The composer's style had a consistently strong 
effect on the performance evaluations, highlighting the importance of careful repertoire selection.  The interaction between 
expertise, modality, and stage behavior revealed that non‐musicians perceived differences across the three degrees of stage 
behavior only audiovisually and not in the audio‐only condition.  In contrast, musicians perceived these differences under both 
audiovisual and audio‐only conditions, with the lowest ratings for minimal stage behavior.  This suggests that varying the 
degree of stage behavior altered the quality of the performance.  In addition, the participants were asked to select two 
emotions that best characterized each performance. They preferentially chose more subtle emotions from Hevner's (1936) 
Adjective Circle over the five emotions of happiness, sadness, anger, fear, and tenderness traditionally used in music cognition 
studies, suggesting that these five emotions are less apt to describe the emotions conveyed through musical performance.  
 
                                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                                     15 
 
 
17 Effects of Interactions with Young Children on Japanese Women’s Interpretation 
of Musical Babblings 
 
Mayumi Adachi* 
 
Hokkaido University, Sapporo, Japan 
* = Corresponding author, m.adachi@let.hokudai.ac.jp 
 
Japanese mothers and young women tend to interpret a Japanese toddler's babblings deriving from infant‐directed speech 
contexts as speech‐like and those from infant‐directed song contexts as song‐like. In the present study, I investigated whether 
interactions with young children could affect the use of vocal cues among Japanese mothers (Experiment 1) and Japanese 
young women (Experiment 2). In Experiment 1, 23 Japanese mothers who participated in Adachi and Ando (2010) fell into 
two groups based on the scores (0‐12) of how actively they were singing/talking to their own child: “active” (scores 8‐12, n = 
13) and “less active” (scores 3‐7, n = 10). Each mother's data were used to determine vocal cues that contributed her own 
interpretation of 50 babblings by means of step‐wise variable selection of logistic regression, with the interpretation as 
dependent variable (song­like versus speech­like) and 15 vocal features (identified in Adachi & Ando, 2010) as predictor 
variables. In Experiment 2, the same analyses will be conducted with data obtained from Japanese young women who had 
been interacting with 6‐year‐olds or younger (“experienced”) and from their matched sample without such interactions 
(“inexperienced”). Results in Experiment 1 revealed that 11 out of 13 “active” mothers were using particular cues consistently 
while only 4 out of 10 “less active” mothers were doing so, χ2(1, N = 23) = 4.960, p = .026. In addition, among the mothers using 
particular cues, the “active” mothers used the average of more than 3 cues while the “less active” mothers used the average of 
1 cue. (Results in Experiment 2 will be presented at the talk.) The present study will reveal the role of interactions with young 
children in the current and the prospective mothers' interpretations of song‐like/speech‐like babblings. Such information may 
explain why some toddlers produce more spontaneous songs than others. 
 
 
 
 
18 Age­related Changes in Children’s Singing 
 
Sandra E. Trehub (1)*, Lily Zhou (2), Judy Plantinga (1), Mayumi Adachi (3) 
 
(1) University of Toronto, Ontario, Canada, (2) McMaster University, Hamilton, Ontario, Canada, (3) Hokkaido University, Sapporo, Japan 
* = Corresponding author, sandra.trehub@utoronto.ca 
 
Several studies have documented age‐related improvements in children’s singing, usually by expert ratings rather than 
measurement. However, no study has attempted to identify factors that may contribute to such improvement. Adults sing less 
accurately with lyrics than with a neutral syllable (Berkowska & Dalla Bella, 2009), highlighting the demands of word 
retrieval. Perhaps age‐related differences in singing accuracy are attributable, in part, to age‐related changes in memory. In the 
present study we focused on interval accuracy and singing rate in children’s rendition of a familiar song (ABC, Twinkle) sung 
with words or on the syllable /la/. Children were 4‐12 years, 24 at 4‐6, 7‐9, and 10‐12 years. The first 17 beats of each 
performance were analyzed by means of Praat. Duration of the first two measures provided an estimate of singing rate. A 
regression analysis with gender, age, and singing rate revealed significant effects of age (greater pitch accuracy at older ages) 
and singing rate (greater accuracy with slower singing) in performances with lyrics. Regression analysis on songs sung on /la/ 
revealed no differences, calling into question claims of increasing singing proficiency in this age range. A two‐way ANOVA 
(interval size, lyrics/syllables) revealed significant effects of interval size (greater pitch deviations with larger intervals), 
F(4,284) = 44.79, p < 0.001, and lyrics (greater pitch deviations with lyrics), F(1,71) = 9.18, p = .003. Regression analysis also 
revealed that age and singing rate had significant effects on key stability, as reflected in deviations from the tonic, but only for 
performances with lyrics. In short, the need to retrieve lyrics and tunes has adverse consequences for children, as reflected in 
reduced pitch accuracy, poor key stability, and slow singing rate. We suggest that the development of singing proficiency in 
childhood could be studied more productively by means of pitch and interval imitation. 
                                                                                               SMPC 2011 Program and abstracts, Page:       
                                                                                                                                       16 
 
 
19 Do Infants Perceive the Beat in Music?  A New Perceptual Test 
 
Aniruddh D. Patel (1)*, John R. Iversen (1), Melissa Brandon (2), Jenny Saffran (2) 
 
(1) The Neurosciences Institute, San Diego, USA  (2) University of Wisconsin, Madison, USA  
* = Corresponding author, apatel@nsi.edu 
 
Beat perception is fundamental to music perception.  How early does this ability develop?  While infants do not synchronize 
their movements to a musical beat (Zentner & Eerola, 2010), it is possible they can perceive the beat, just as sophisticated 
speech perception abilities precede the ability to talk.  Hence evidence for infant beat perception must come from perceptual 
tests.  A recent event‐related potential (ERP) study of beat perception in sleeping newborns suggested that they recognized 
the omission of the downbeat in a drum pattern (Winkler et al., 2009), but the downbeat omission stimulus (unlike omissions 
at other positions) was created by silencing two drum parts at once, making it possible that the brain response was to a change 
in the texture of the sound rather than a reflection of beat perception.  Other studies of infant meter perception have used 
cross‐modal approaches (e.g., Phillips‐Silver & Trainor, 2005) or cross‐cultural approaches (e.g., Hannon & Trehub, 2005), but 
the sensitivities demonstrated by infants in these studies may be explainable on the basis of grouping perception and/or 
sensitivity to event duration patterns, without invoking beat perception. The current study used a novel perceptual test to 
examine beat perception in 7‐8 month old infants.  This was a simplified version of the BAT (Beat Alignment Test, Iversen & 
Patel, 2008), in which a metronomic beep track is overlaid on long excerpts of real music (popular Broadway instrumental 
tunes).  The beeps were either on the beat, too fast, or too slow.  A preferential looking paradigm was used, and it was found 
that infants preferred the music with the on‐beat overlay tracks, suggesting that they do in fact perceive the beat of complex 
music.  The presentation will include a discussion of how the BAT might be improved for future research on infant and adult 
beat perception. 
 
 
 
 
20 The Development of Sensitivity to Key Membership and Harmony in Young 
Children 
 
Kathleen A. Corrigall (1)*, Laurel J. Trainor (1,2,), 
 
(1) McMaster Institute for Music and the Mind, Hamilton, Canada, (2) Rotman Research Institute, Baycrest Hospital, Toronto, Canada 
* = Corresponding author, corrigka@mcmaster.ca 
 
Even Western adults with no formal music training have implicit knowledge of key membership (which notes belong in a key) 
and harmony (chords and chord progressions). However, little research has explored the developmental trajectory of these 
skills, especially in young children. Thus, our primary goal was to investigate 4‐ and 5‐year‐olds’ knowledge of key 
membership and harmony. On each trial, children watched videos of two puppets playing a 2‐ to 3‐bar melody or chord 
sequence and were asked to give a prize to the puppet that played the best song. One puppet played a standard version that 
followed Western harmony and voice leading rules and always ended on the tonic, while the other played one of three deviant 
versions: 1) atonal, which did not establish any particular key, 2) unexpected key, which replicated the standard version 
except for the last note or chord, which went outside the key, and 3) unexpected harmony, which replicated the standard 
version except for the last note or chord, which ended on the subdominant. Children were assigned to one of the three deviant 
conditions, and completed four trials each of melodies and chords. Our dependent measure was the proportion of trials on 
which children selected the puppet that played the standard. Results of the 35 4‐year‐olds and 36 5‐year‐olds tested to date 
revealed that 5‐year‐olds selected the standard version significantly more often than predicted by chance for both melodies 
and chords in the atonal and unexpected key conditions. In the unexpected harmony condition, 5‐year‐olds’ performance did 
not differ from chance for either melodies or chords. Four‐year‐olds performed at chance in all conditions. These results 
indicate that aspects of musical pitch structure are acquired during the preschool years. 
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    17 
 
21 Investigating Mothers’ Live Singing and Speaking Interaction with Preterm 
Infants in NICU: Preliminary Results 
 
Manuela Filippa (1)*, Maya Gratier (1) 
 
(1) Université Paris Ouest Nanterre La Défense, Paris, France,  
* mgfilippa@libero.it 
 
Vocal communication between mothers and infants is well documented in the first months of life (Gratier & Apter, 2009), but 
few observations involved preterm infants. This article reports on the theoretical underpinnings of the study of maternal 
singing, considered as an important relational based intervention in early dyadic communication between mothers and 
preterm infants. To date, 10 out of 20 preterm neonates have been studied. Their weight at birth was between 950 and 2410 g 
and the entry criteria at the time of the recordings were (1) >29 weeks PCA, (2) > 1000g, (3) stable condition (absence of 
mechanical ventilation, no additional oxygen needed, no specific pathological conditions). All infants are tested for 6 days, 
during their hospital stay in their room in their own incubators, one hour after their afternoon feeding. All mothers involved 
are asked, on 3 different days, to speak and sing to their infants. Before and between these days, a day with no stimulation 
provides comparison data. The sessions are video and audio recorded both during the stimulation and also for 5 minutes 
before and after the stimulation. Clinical parameters are automatically recorded every minute and “critical events” are 
marked; individual behavioral and interactional reaction responses are measured as infant engagement signals. During 
maternal vocal stimulation we found an increase of HR and Oxygen Saturation (p>0.05); a decrease in standard deviation of 
clinical parameters, a decrease of “critical events” and an increase of calm alertness states. The results indicate that the 
maternal live speaking and singing stimulation has an activation effect on infant, as we observe an intensification of the proto‐
interaction, of the calm alertness states (Als, 1994) in a clinically stable condition, with a significant increase of HR and Oxygen 
Saturation (p>0.05). 
 
 
22 Koechlin's Volume: Effects of Native Language and Musical Training on 
Perception of Auditory Size among Instrument Timbres 
 
Frédéric Chiasson (1)(2)(3)*, Caroline Traube (1)(2)(3), Clément Lagarrigue (1)(2), Benneth Smith (3)(4) and Stephen 
McAdams (3)(4) 
 
(1) Observatoire interdisciplinaire de recherche et création en musique (OICRM), Montréal, Canada, (2) Laboratoire informatique, acoustique et musique (LIAM), 
Faculté de musique, Université de Montréal, Canada, (3) Centre for Interdisciplinary Research in Music, Media and Technology (CIRMMT), Montréal, Canada, (4) 
Schulich School of Music, McGill University, Montréal, Canada. 
* = Corresponding author, frederic.chiasson@umontreal.ca 
 
Charles Koechlin's orchestration treatise (Traité de l'orchestration) ascribes different dimensions to timbre than those usually 
discussed in multidimensional scaling studies: volume or largeness, related to auditory size, and intensité, related to loudness. 
Koechlin gives a mean volume scale for most orchestral instruments. Studies show that auditory size perception exists for 
many sound sources, but none proves its relevance for instruments from different families. For both experiments of this study, 
we have developed methods and graphical interfaces for testing volume perception. Samples of eight orchestral instruments 
from the Electronic Music Studio of the University of Iowa, playing Bs and Fs mezzo forte in octaves 3, 4 and 5 (where A4 is 
440 Hz) were used. We kept the first second of all samples, keeping attack transients, and added fade‐out to the last 100 ms. 
For each pitch category, samples were equalized in pitch, but not in loudness, to keep the loudness differences in a concert 
situation. Task 1 required participants to order eight sets of samples on a largeness (grosseur in French) scale from "least 
large" (moins gros) to "largest" (plus gros). Task 2 required them to evaluate the sounds' largeness on a ratio scale compared to 
a reference sample with a value of 100. Participants were compared according to native language (English vs French), musical 
training (professional musicians vs amateurs and nonmusicians) and hearing (good vs minor hearing loss). Results suggest 
that participants share a common perceptual largeness among instrument timbres from different families. This common 
perceived largeness is well correlated with Koechlin's volume scale. Native language, musical training and hearing have no 
significant effect on results.  These results provide new angles for timbre research and raise questions about the influence of 
loudness equalization in most studies on timbre. 
                                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                                     18 
 
23 Perception of Dyads of Impulsive and Sustained Sounds 
 
Damien Tardieu (1)*, Stephen McAdams (2) 
 
(1) IRCAM – CNRS UMR 9912, Paris, France (2) CIRMMT, Schulich School of Music, McGil University, Montreal, Canada 
* = Corresponding author, Damien.Tardieu@ircam.fr 
 
Perception of instrumental blends is important for understanding aspects of orchestration. Previous work (Kendall & 
Carterette, 1991; Sandell 1995) has focused on dyads of sustained sounds. However a common technique in orchestration 
consists of using mixtures of impulsive and sustained sounds. The first experiment identified the factors that influence the 
blending of dyads, i.e., whether they are perceived as one or two sounds. 11 sustained and 11 impulsive sounds of the same 
pitch and loudness were used yielding a total of 121 dyads. Each participant rated each dyad four times on a continuous scale 
between “twoness” to indicate the absence of blend, and “oneness” to indicate perfect blend. We found that longer attack times 
and lower spectral centroids improve fusion. The choice of the impulsive sound thus seems more important than the choice of 
the sustained sound in controlling blend. The second experiment determined the factors that influence the perception of 
similarity between the dyads. Participants rated the dissimilarity on a continuous scale between pairs formed of 16 well‐
blended dyads chosen from the previous 121 to maximize the timbral variety. In this experiment, contrary to the first 
experiment, the sustained instrument had more influence on the perception of similarity. The mean spectral envelope of the 
dyad is the feature that best explains the similarity ratings, but the spectral envelope of the sustained sound is more important 
than the spectral envelope of the impulsive sound. A multidimensional scaling of the dyad dissimilarity ratings yields one 
dimension correlated with the attack time of the dyad and another dimension whose spectral correlate is different for two 
different clusters within the space, suggesting a combined categorical‐analogical organization of the second dimension. 
 
 
 
 
24 Blend, Identification, and Similarity of Differentially Orchestrated Wind Triads 
Correlated with Acoustical Analyses of Spectral Distribution and Roughness 
 
Roger A. Kendall (1)*, Pantelis N. Vassilakis (2) 
 
(1) Music Cognition and Acoustics Laboratory, Herb Alpert School of Music, University of California, Los Angeles,  Los Angeles, Ca. 90024, kendall@ucla.edu   
(2) Audio Arts + Acoustics,  School of Media Arts,  Columbia College Chicago,  Chicago, IL 60605   
* = Corresponding author, kendall@ucla.edu 
 
Previous experiments with orchestral triads in non‐traditional orchestrations of flute, oboe, and clarinet are extended in this 
study.  Traditional doublings within a triad are compared to less common orchestrations with separate instruments on each 
note.  Major and minor triads (C5) were orchestrated using Kontakt Silver sampled oboe, clarinet and flute tones and 
incorporated doublings suggested by orchestration monographs.  Unison doublings were enhanced with a chorus effect 
created by slightly misaligning the dyad tones’ fundamental frequency and onset time.  Music‐major subjects rated sets of 
triads on similarity, degree of blend, and participated in identification tasks for the soprano and bass instruments.  Perceptual 
spaces derived from the similarity data corresponded well to previous multidimensional scalings where the first‐dimension 
triad distribution was related to the timbre in the bass of the triad.  Correlations with long‐time‐average spectral centroid 
were high for both major and minor contexts (r = .98 and .94 respectively).   Calculations of roughness based on one of the 
authors’ formulations, using spectral time‐frequency reassignment, correlated well with the major context’s first dimension as 
well.  Higher blend ratings were obtained for major vs. minor orchestrations; additional analyses relate blend and 
identification to timbral combination.  In particular, the similarity of clarinet and oboe timbres on G5, and their corresponding 
spectral similarities, appears to lead to identification difficulties. 
                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                     19 
 
 
25 The Relevance of Spectral Shape to Perceptual Blend between Instrument 
Timbres 
 
Sven‐Amin Lembke (1)*, Stephen McAdams (1) 
 
(1) CIRMMT, Schulich School of Music, McGill University, Montreal, Canada 
* = Corresponding author, sven‐amin.lembke@mail.mcgill.ca 
 
Previous studies have suggested the perceptual relevance of stable spectral properties characterizing the timbre of orchestral 
wind instruments. Analogous to human voice formants, the existence of stable local spectral maxima across a wide pitch range 
has been reported for these instruments. Furthermore, agreement of these ‘formant regions’ between instrumental sounds has 
been suggested to contribute to the percept of blend between timbres. Our aim is to verify and validate these hypotheses 
based on a two‐stage approach comprising acoustical analysis and perceptual testing. Spectral analyses are computed on a 
broad audio sample database across all available pitches and dynamic levels. Based on the obtained spectral information, 
partial tones are identified and their frequencies and amplitudes used to build global distributions of partials across all pitches 
and between dynamic levels. A curve‐ﬁtting procedure applied to these distributions then yields spectral proﬁles from which 
characteristics such as ‘formant regions’ are identified and described. This can be complemented by signal processing 
techniques such as linear‐predictive‐coding or cepstrum analyses to attain parametric expressions for spectral shape. The 
second stage takes obtained formant characteristics and tests their perceptual relevance in an experiment employing a 
production task. Stimuli drawn from the aforementioned instrumental sounds are used as reference sounds to which a 
synthesized sound exhibiting variable formant properties is matched. Participants adjust sliders controlling synthesis 
parameters in order to achieve the maximum attainable blend, accuracy between produced and acoustically determined 
parameter values taken as dependent variable. Besides providing a validation for the contribution of ‘formant regions’ to 
perceptual blend, the experiment’s multifactorial design allows their relevance to be investigated across different instruments, 
pitch intervals and registers. Results from these two stages will provide a clearer picture of what perceptual blend 
corresponds to acoustically and would furthermore help explain its usage in orchestration practice. 
 
 
26 Using Percussive Sounds to Improve the Efficacy of Auditory Alarms in Medical 
Devices  
 
Glenn Paul* and Michael Schutz  
 
McMaster Institute for Music and the Mind.  Hamilton, Ontario CANADA 
* = Corresponding author, paulg2@muss.cis.mcmaster.ca ca 
 
Auditory alarms are a desirable feature in medical devices, allowing doctors to monitor patients’ vital signs by ear while 
simultaneously observing them visually. However, such alarms rely on a doctor’s ability to remember associations between 
sounds and messages, a task that has proven difficult (Block, 2008; Sanderson, 2009) even when using the sounds 
recommended by the International Electrotechnical Commission (IEC, 2005). One reason for this difficulty may stem from the 
amplitude envelopes (i.e. “temporal shape”) of these sounds.  Recent work in our lab has shown that sounds with percussive 
envelopes (exhibiting an immediate exponential decay with no sustain) are advantageous for tasks involving learned 
associations (Schutz & Stefanucci, 2010).  Therefore, we are now working to explore the possibility of using them to improve 
the efficacy of auditory alarms in medical devices. We have developed a new paradigm for this endeavor. In the first (study) 
phase, participants read a script describing a scenario in which the alarms may be used, and hear each twice.  In the second 
(training) phase, participants hear each of the eight alarms in turn, and are asked to identify it (with feedback). Participants 
repeat this training until they were able to identify seven of the eight alarms.  After a distracter task, participants enter the 
third (evaluation) phase, in which they are tested on their ability to name each alarm. Our pilot data indicate that this 
paradigm is viable for assessing three crucial factors: (1) the amount of training required to learn the alarms, (2) the strength 
of the association between tones and alarms, and (3) confusion between similar alarms.  We found that musical training had a 
significant influence on the ability to learn the alarms, and are now collecting data to explore whether envelope manipulations 
can help improve their efficacy in medical care.  
                                                                                         SMPC 2011 Program and abstracts, Page:       
                                                                                                                                 20 
 
 
Keynote lecture: 
 
Cognitive Factors Shape Brain Networks for Auditory Skills 
 
Professor Nina Kraus 
Hugh Knowles Professor (Communication Sciences; Neurobiology & Physiology; Otolaryngology), Northwestern University 
 
We  know  that  musicians  profit  from  real‐life  advantages  such  as  a  greater  ability  to  hear  speech  in  noise  and  to  remember 
sounds.  The extent to which these advantages are a consequence of musical training or innate characteristics that predispose 
a given individual to pursue music training is often debated.  Here, we examine the role that auditory working memory plays 
in hearing speech in noise and potential biological underpinnings of musicians’ auditory advantages.  We present a framework 
that emphasizes cognition as a major player in the neural processing of sound. Within this framework, we provide evidence for 
how music training is a contributing source of these abilities.  
 
 
                                                                                               SMPC 2011 Program and abstracts, Page:       
                                                                                                                                       21 
27 Listeners’ Images of Motion and the Interaction of Musical Parameters 
 
Zohar Eitan (1)*, Roni Y. Granot (2) 
 
(1) School of Music, Tel Aviv University, Israel (2) Department of Musicology, The Hebrew University of Jerusalem, Israel 
* = Corresponding author, zeitan@post.tau.ac.il 
 
Eitan & Granot (2006; hence E&G) examined how listeners map changes in musical parameters onto aspects bodily motion. 
Participants associated melodic stimuli with imagined motions of a human character and specified the movement directions, 
pace changes and type of these motions. Stimuli consisted of contrasting pairs of melodic figures, manipulating independently 
loudness change, pitch direction, tempo, and pitch interval size. In the current study we begin to examine systematically the 
effects of interactions between musical parameters on music‐motion mappings. Twenty brief melodic stimuli (3‐6 seconds) 
were presented to 78 participants (35 music‐trained). Participants’ task was identical to those in E&G. Stimuli systematically 
combined concurrent changes in four musical parameters: 4 stimuli combined loudness changes (crescendo/diminuendo) and 
pitch directions (up/down), 4 combined loudness and tempo changes (accelerando/ritardando), 4 combined pitch directions 
and tempo change, and 8 combined loudness change, pitch direction, and changes in interval size. Results corroborate that 
dimensions of motion imagery, rather than exhibiting one‐to‐one mappings of musical and motion parameters (pitchheight, 
tempospeed, loudnessdistance), are affected by several musical parameters and their interactions. Thus, speed change 
associates not only with tempo, but with changes in loudness and pitch direction (e.g., participants did not associate an 
accelerated stimuli with increased speed when loudness was simultaneously reduced); vertical direction (rise/fall) is 
associated not only with pitch direction but with loudness (pitch ascents in diminuendo were associated with spatial descent); 
and distance change is associated not only with loudness change but with pitch direction. Moreover, significant interactions 
among musical parameters suggest that effects of single musical parameters cannot wholly predict music‐motion mappings. 
For instance, both loudness and pitch and pitch and tempo significantly interact in conveying distance change. This 
multidimensional view of perceived musical motion may bear important implications for musical multimedia, sonification, and 
music theory and analysis. 
 
 
28 Crossmodal Analogues of Tempo Rubato 
 
Fernando Benadon (1)*, Madeline Winkler (1) 
 
(1) American University, Washington DC, USA 
* fernando@american.edu 
 
Do listeners perceive tempo rubato as concomitant with a feeling of effort?  Thirty‐six participants were presented with 12 
short descriptions depicting the actions of a hypothetical character named Stick Figure.  Each scene was paired to a soundtrack 
melody that was either rubato or non‐rubato.  Using the soundtrack melody as a cue, the task was to rate the level of effort as 
experienced by Stick Figure.  For instance, the scene “Stick Figure is lifting a box; the box is ___” required participants to 
provide a score between the slider endpoints “very light” (low effort) to “very heavy” (high effort).  A scene’s effort type was 
either motor (e.g., lifting a light/heavy box) or cognitive (e.g., solving a simple/difficult puzzle).  A two‐way repeated measures 
ANOVA showed a significant main effect of rhythm type (rubato vs. non‐rubato); there was no significant difference between 
the two effort types (motor vs. cognitive).  These results suggest that tempo rubato is crossmodally correlated with a 
metaphorical sense of effort. 
 
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    22 
 
29 Training of Instrument Recognition by Timbre in Non­musicians: A Rapid 
Learning Approach  
 
Lisa Aufegger (1)*, Oliver Vitouch (1) 
 
(1) Cognitive Psychology Unit, Dept. of Psychology, University of Klagenfurt, Austria 
* = Corresponding author, Lisa.Aufegger@uni‐klu.ac.at 
 
The question of whether attaining specific competencies in music is part of the individual human potential or rather a common 
consequence of deliberate practice is still a much debated issue. While musical abilities and their genetic bases are still 
associated with giftedness, other approaches show musicality to be the result of a 10‐year preparation period, including 
thousands of hours of training in this particular field (Ericsson, Krampe, & Tesch‐Römer, 1993). In this tradition, the "rapid‐
learning approach" of Oechslin, Läge, & Vitouch (under review) proposes that any individual is able to attain basic and specific 
competencies in music perception approaching the performance level of professional musicians in a quick and reliable manner 
by means of a specific training methodology. 
In this study, the rapid‐learning approach was tested for the case of instrument recognition by timbre in non‐musicians. N = 34 
subjects had to indentify 10 solo instruments (6 woodwind and 4 brass) from brief orchestral recordings in a pre‐/posttest 
design. Three PC‐based training units with a maximum duration of 90 min. provided an intensive examination of each 
instrument via simple feedback strategies. Results showed a significantly improved identification of instruments in the 
posttest. When compared to 19 music students (wind players and pianists) the subjects did not achieve expert recognition 
level (as compared to wind players), but semi‐expert recognition level (as compared to pianists). Given the adequate 
methodology and using a feedback‐based approach, non‐musicians are, as demonstrated, able to excel. In the context of 
perceptual learning there may indeed be a broad and general basis for quickly acquiring and improving fundamental 
competencies of music perception. 
 
 
30 More than Just Musical Ability: Regulatory Fit Contributes to Differences 
between Musicians and Non­musicians in Music Perception 
 
J. Devin McAuley (1)*, Molly J. Henry (2), Alan Wedd (3) 
 
(1) Department of Psychology, Michigan State University, East Lansing, USA, (2) Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany, 
(3) Department of Psychology, Michigan State University, East Lansing, USA 
* = Corresponding author, dmcauley@msu.edu  
 
Two experiments examined effects of regulatory fit and musical training on performance on a representative subtest of the 
Montreal Battery of Evaluation of Amusia (MBEA). In both experiments, participants made same‐different judgments about 
pairs of melodies, while either gaining points for correct answers (a gains condition) or losing points for incorrect answers (a 
losses condition). In Experiment 1, participants were told that the test was diagnostic of their musical ability and then were 
asked to identify themselves as a ‘musician’ or a ‘non‐musician’, while in Experiment 2 participants were given either a 
promotion focus prime (they were told that they had a performance‐based opportunity to gain entry into a raffle at the end of 
the experiment) or a prevention focus prime (they were given a raffle ticket at the start of the experiment and needed to 
maintain a certain level of performance in order to prevent losing their entry into the raffle). Consistent with a regulatory fit 
hypothesis, non‐musicians and promotion‐primed participants performed better in the gains condition than in the losses 
condition, while musicians and prevention‐primed participants performed better in the losses condition than in the gains 
condition. Experiment 2 additionally revealed that regulatory fit effects were stronger for musicians than for non‐musicians. 
This study is the first to demonstrate that regulatory fit impacts performance on the MBEA and highlights that individual 
differences in motivational orientation are important to consider when interpreting musician performance advantages in 
music perception. 
 
                                                                                          SMPC 2011 Program and abstracts, Page:       
                                                                                                                                  23 
 
31 Musical Aptitude Predicts Ability to Discriminate Music and Speech from 
Background Noise 
 
Jordan C. Schramm (1) &  Anne E. Luebke* (1,2) 
 
(1) Department of Neurobiology & Anatomy, (2) Department of Biomedical Engineering, University of Rochester Medical Center, Rochester, NY 14642, USA 
* = Corresponding author, Anne_Luebke@urmc.rochester.edu 
 
Approximately 17% of American adults report some of degree difficulty hearing in a noisy environment.  Recent findings 
suggest musicians with > 9 yrs of musical training have an enhanced ability to discriminate speech in the presence of 
background noises (Parbery‐Clark A, Skoe E, Lam C, Kraus N. Ear Hear. 2009).  We wondered if trained musicians also had 
enhanced abilities to discriminate music in the presence of background noises, and if subjects with high music aptitudes would 
also have enhanced discrimination in noise abilities?  To test these hypotheses, we recruited adults between 18‐29 yrs. and 
tested: i) standard audiometric thresholds; ii) speech‐in‐noise intelligibility using the Hearing‐in‐Noise Test (HINT); iii) music 
perception‐in‐noise ability using the same protocol used for the HINT, but replacing the speech with questions from the 
Montreal Battery for Evaluation of Amusia (MBEA); iv) musical aptitude using the tonal imagery melody portion of the musical 
aptitude profile (MAP T1).  In addition, all subjects completed a survey of their musical training and listening history.  Our 
results confirm previous findings that higher musical achievement correlates with enhanced speech‐in‐noise abilities.   
Furthermore, we determined that subjects with high musical aptitudes and low musical achievement also had enhanced 
hearing‐in‐noise abilities.  We conclude that enhanced ability to discriminate both speech and music in the presence of 
background noise is better predicted by musical aptitude rather than musical achievement. Supported by grants from NIH 
[DC003086 (AEL), KL2 RR024136 (JCS)].  
 
 
 
 
32 Music Processing in Deaf Adults with Cochlear Implants 
 
Mathieu R. Saindon (1)*, Sandra E. Trehub (1), E. Glenn Schellenberg (1) 
 
(1) University of Toronto, Toronto, Canada 
* = Corresponding author, mr.saindon@utoronto.ca 
 
Contemporary cochlear implants (CIs) provide relatively coarse representations of pitch and spectral detail, which are 
adequate for speech recognition in quiet but not for some aspects of speech prosody and music. The available research reveals 
that CI users have difficulty differentiating vocal emotion and that they fail to recognize familiar music on the basis of pitch 
cues alone. Nevertheless, research in these domains has been relatively limited. In the present study, we assessed a range of 
speech and music processing skills in 9 successful CI users and 12 hearing controls. The music tasks assessed the perception of 
meter, rhythm, pitch direction, isochronous melodies, timbre, and emotion as well as pitch imitation and the recognition of 
familiar music. The speech tasks assessed the recognition of monosyllabic words and vocal emotion. Overall, the performance 
of hearing individuals was substantially better than that of CI users, whose performance was highly variable. The threshold for 
pitch direction discrimination for hearing adults was substantially smaller than that of CI users, but three CI users performed 
as well as hearing listeners. Two CI users matched pitches with incredible accuracy (< 1 semitone). Surprisingly, one CI user 
readily recognized isochronous melodies and familiar music, in contrast to the very poor performance of other CI users on 
these tasks. Only two CI users performed well on timbre recognition. Although all CI users performed well on meter 
discrimination, most had difficulty with rhythm discrimination in the context of accompanied melodies, and one exhibited 
error‐free performance. Finally, CI users performed poorly at perceiving emotion in music and even worse at perceiving 
emotion in speech. Overall, our findings are in line with previous findings on melody and prosody discrimination. However, 
some CI users’ unexpectedly high performance levels imply that CIs may not be insurmountable barriers to musical pitch 
processing.    
 
                                                                           SMPC 2011 Program and abstracts, Page:       
                                                                                                                   24 
 
 
33 Contribution of Hearing Aids to Music Perception by Cochlear Implant Users 
 
Tonya Bergeson (1)*, Nathan Peterson (1) 
 
(1) Indiana University School of Medicine, Indianapolis, USA 
* = Corresponding author, tbergeso@indiana.edu 
 
Modern cochlear implant (CI) encoding strategies represent the temporal envelope of sounds well but provide limited spectral 
information.  This deficit in spectral information has been implicated as a contributing factor to difficulty with speech 
perception in noisy conditions, discriminating between talkers, and melody recognition.  One way to supplement spectral 
information for CI users is by fitting a hearing aid (HA) to the non‐implanted ear.  In this study 14 postlingually deaf adults (7 
with a unilateral CI and 7 with a CI and a HA (CI+HA)) were tested on measures of music perception (MBEA) and familiar 
melody recognition.  CI+HA listeners performed significantly better than CI‐only listeners on all pitch‐based music perception 
tasks.  The CI+HA group did not perform significantly better than the CI‐only group in the two tasks that relied on duration 
cues.  Recognition of familiar melodies was significantly enhanced for the group wearing a HA in addition to their CI.  This 
advantage in melody recognition was increased when melodic sequences were presented with the addition of harmony. 
Although both groups scored higher on rhythmic tests than on pitch‐based tests, they did not appear to use this information 
well in identifying real‐world melodies. These results show that, for CI recipients with aidable hearing in the non‐implanted 
ear, using a hearing aid in addition to their implant improves perception of musical pitch and recognition of real‐world 
melodies. 
 
 
 
 
 
 
 
34 Interspike Intervals, Subharmonics, and Harmony 
 
Peter Cariani * 
 
Dept. of Otology and Laryngology, Harvard Medical School 
* = Corresponding author, cariani@mac.com 
 
A very strong case can be made that the auditory system utilizes a temporal, interspike interval code for the early 
representation of periodicity and spectrum (Cariani & Delgutte, JASA 1996; Cariani, Neural Plasticity, 1999). The pitch of 
harmonic complex tones at the fundamental corresponds to the most numerous all‐order interspike interval present in the 
auditory nerve at any given moment. As a direct consequence of phase‐locking of spikes, all‐order interspike interval 
distributions in different frequency regions of the auditory nerve reproduce the subharmonic series of the individual 
harmonics that drive them. When all of the intervals are summed together, those associated with common subharmonics , i.e. 
the fundamental and its subharmonics, predominate, and the pitch associated with this interval pattern is heard.  
Pitch strength is qualitatively predicted by the relative fraction of pitch‐related intervals. In the case of note dyads, in 
neurophysiological studies (Tramo et al, NYAS, 2001) and simulations (Cariani, ICMPC, 2004), the predominant interspike 
intervals produce the fundamental bass of the musical interval. Estimated pitch strengths of the fundamental basses of 
different dyads (e.g. 16:15, 4/3, 45/32, 3/2) reflect degree of pitch multiplicity, fusion (Stumpf), stability, and harmonic 
tension. Results from auditory nerve simulations of triads of harmonic complexes yielded major triads and sus‐4 chords as 
most stable, followed by minor and sus‐2 chords, with augmented and diminished chords as the least stable.  
Thus, there is the possibility of a neurally‐grounded theory of basic harmony that is  based on superpositions of subharmonics 
(“undertones”). Musical intervals bear characteristic patterns of subharmonics. In the past such theories have been proposed 
(Rameau, Riemann), but abandoned because, unlike harmonics, subharmonics are generally not heard, and it was believed 
that the subharmonics do not exist either in the acoustics or in the ear.  However, we now know from auditory 
neurophysiology that subharmonic series are ubiquitous in the auditory nerve, and that perhaps these sorts of theories 
deserve re‐examination. 
                                                                                         SMPC 2011 Program and abstracts, Page:       
                                                                                                                                 25 
 
 
35 The Music of Speech: Heterometers and Melodic Arches 
 
Steven Brown (1)*, Ivan Chow (1), Kyle Weishaar (2), Jordan Milko (1) 
 
(1) Department of Psychology, Neuroscience & Behaviour, McMaster University, Hamilton, ON, Canada 
(2) Department of Linguistics and Languages, McMaster University, Hamilton, ON, Canada 
* = Corresponding author, stebro@mcmaster.ca 
 
Work on speech rhythm has been notoriously oblivious to describing actual rhythms in speech. Likewise for work on speech 
melody. Musical transcription provides a joint solution for both problems as well as a means of examining the interaction 
between rhythm and melody. A first step in our analysis of speech is to divide a sentence into a fundamental rhythmic unit, 
what we refer to as a “prominence group”, analogous to a measure in music. The defining feature of a prominence group is that 
it always begins with a strong beat (i.e., a stressed syllable in the case of English). Contrary to classic “isochrony” models in 
speech, we posit that changes in meter are central to speech rhythm, and thus that speech is “heterometric” rather than 
isochronous. By this we mean that there are local pockets of metricality in sentences but that meters can change throughout 
the course of a sentence, for example from a triple‐based to a duple‐based meter. A fundamental unit of speech melody is more 
difficult to define. However, classic work in phonology suggests that sentences are built up of a series of melodic arches or 
waves, where each arch corresponds with an intonational phrase. These arches diminish successively in register and size as a 
sentence proceeds, reflecting a general process of declination in speech. Finally, this method permits an analysis of the 
interaction between rhythm and melody in speech. A common interaction is seen in the phrase “beautiful flowers”, with a 
rising contour on beautiful and a falling one on flowers. While a stress occurs on “flow‐”, the melodic arch is centered on the “‐
ful” of beautiful. Hence, the peak of the melodic arch tends to precede the rhythmic downbeat, which itself is instantiated with 
an intensity rise rather than a pitch rise in many cases.  
 
  
 
 
36 Are Tone Languages Music? Rhythm and Melody in Spoken Cantonese 
 
Ivan Chow (1)*, Matthew Poon (2), Steven Brown (1)  
 
(1) Department of Psychology, Neuroscience & Behaviour, McMaster University, Hamilton, ON, Canada 
(2) School of the Arts, McMaster University, Hamilton, ON, Canada 
* = Corresponding author, stebro@mcmaster.ca 
 
Cantonese is a tone language with 6 level tones and 3 contour tones. Rhythmically, it is categorized as a “syllable‐timed” 
language, implying that Cantonese syllables are of equal duration throughout a sentence. However, a closer look at the rhythm 
of spoken Cantonese sentences reveals that Cantonese sentences are not simply sequences of syllables of equal duration, 
which would sound quite “robotic” to a native speaker. Rather, there are predictable rhythmic structures in Cantonese 
sentences that resemble those of so‐called “stress‐timed” languages like English, despite the fact that Cantonese has no word‐
level stress. Syllables are organized into larger metric units similar to “measures” in music. Syllabic durations within these 
metric structures can vary according to musical principles, including the formation of duplets, triplets, and polyrhythms at the 
word level. In addition, meters can change within a sentence, such as from duple to triple meters. At the same time, rhythmic 
structures make reference to syntax and to semantic focus. Regarding melody, the naive viewpoint is that lexical tones 
establish musical scales in tone languages, such that tones correspond with fixed tone‐levels. However, this is anything but the 
case, as tone languages clearly demonstrate the general phenomenon of declination, whereby the pitch level of the sentence 
declines from beginning to end. Therefore, even a Cantonese sentence comprised exclusively of high level‐tones can show an 
intervallic declination of a fifth from the first syllable to the last. Rhythm and melody interact in such a way that when the 
syllables of words are compressed into duplets and triplets, each rhythmic group forms an arch‐like micro‐melody that 
interacts with the lexical tones of the word: while the end of the rhythmic group is marked with a “final drop”, an upward 
trend is seen across the first half of the group before the drop begins. 
                                                                                           SMPC 2011 Program and abstracts, Page:       
                                                                                                                                   26 
 
37 When is speech musical? Why we need concepts of meter, melody, and motif to 
understand prosody in spoken language 
 
Laura Dilley * 
 
Department of Communicative Sciences and Disorders, Michigan State University, East Lansing, MI, USA 
* = Corresponding author, ldilley@msu.edu 
 
Parallels between speech and music have intrigued scholars for centuries. However, linguistic theories of speech prosody have 
not always emphasized probable parallels between music and language domains, leading to a degree of disconnect between 
music and linguistics proper. This talk highlights research that illustrates and builds on parallels between speech and music. 
First, the Rhythm and Pitch (RaP) transcription system for speech prosody was recently developed to incorporate key 
theoretical insights of the well‐known autosegmental‐metrical theory of linguistic tone while granting primacy to many music‐
inspired concepts (e.g., melodic contour, rhythmic regularity, repetition of motifs). A description of the RaP system for speech 
annotation will be presented, including its uses for transcribing rhythmic prominences, phrasal boundaries, pitch accents, and 
perceptual isochrony (i.e., rhythmic regularity). Second, research will be highlighted that investigates the role in spoken 
language of pitch and timing repetition, a concept that is paramount to musical description. In particular, experimental 
findings will be presented that suggest that when pitch repetition and/or perceptual isochrony occur in speech, listeners 
generate rhythmic and grouping expectations that have significant effects on the processing of subsequent spoken words. 
These findings can be modeled in terms of (a) endogenous oscillators that entrain to auditory events in the environment, 
thereby affording predictions about the timing and metrical organization of subsequent events, as well as (b) a hierarchical 
metrical structure for speech rhythm that specifies the relative prominences of phonological units. Overall, this research 
illustrates the importance of musical concepts for investigating the processes involved in spoken language understanding, as 
well as the usefulness of music‐inspired transcriptional approaches.   
 
  
 
 
 
38 Rhythmic production of speech and other behaviors 
 
Robert Port * 
 
Department of Linguistics, Indiana University, Bloomington, IN, USA 
* = Corresponding author, port@indiana.edu 
 
Human speech is frequently produced rhythmically. This means that vowel onsets, and especially stressed vowel onsets, will 
occur at nearly periodic time intervals. Under many circumstances, speakers in most languages can automatically be 
encouraged to adopt rhythmical timing in speech production. Such periodic timing in speech is similar to periodic and metrical 
patterning in many other kinds of movements of the fingers, hands, arms, legs and trunk. Dynamical system models account 
for many aspects of these kinds of behavior. It seems likely that such coupling behaviors have played a role in cementing 
 interpersonal bonds within human groups since before the historical development of spoken language. 
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    27 
 
39 Validating Emotionally­Representative Musical Selections: Relationship Between 
Psychophysiological Response, Perceived and Felt Emotion 
 
Laura A. Mitchell (1)*, Anna M.J.M. Paisley (2) and Daniel J. Levitin (3) 
 
(1) Bishop’s University, Sherbrooke, Canada (2) Glasgow Caledonian University, Glasgow, UK (3) McGill University, Montreal, Canada 
* = Corresponding author, lauramitchellwork@gmail.com 
 
Understanding the relationship between music and emotion remains a central issue in music psychology research. This 
includes investigating the recognition of musical emotion (e.g. Quintin et al., in press), the real‐time continuous physiological 
reactions to music (e.g., Chapados & Levitin, 2008), as well as subjective/introspective reports of the emotions that music 
induces (e.g. Vines et al., in press).  In a wide variety of research arenas, including research that is not addressed toward 
musical processing per se, music is frequently used as a means of emotional induction, making accessibility of pre‐tested 
stimuli fundamental for consistency between studies. Vieillard et al. (2008) established a set of stimuli using specially‐
composed music conveying emotion through musical structure, finding that four specific emotions of happy, sad, peaceful and 
scary could be distinguished. To encourage ecological validity of stimuli, the current study provides self‐report data of 
perceived musical emotion, felt emotion, liking and valence and psychophysiological measurement for 20 easily available 
pieces of music grouped by four emotional qualities. The two‐minute instrumental pieces were selected through extensive 
piloting for being reliably identified with the emotions peaceful, scary, happy and sad. Thirty‐six participants (20 females, 
mean age 29.3) listened individually on headphones to all 20 selections in one session, randomized within four emotional 
blocks counterbalanced in order. Heart rate, respiration rate and skin conductance were recorded using the Biopac MP35 
system, and subjects used three minutes between tracks to rate familiarity, liking, pleasantness, perceived emotional 
expression and felt emotional response on a visual analogue scale. Musicianship and everyday listening habits were recorded 
by questionnaire. Results will be analyzed using linear mixed effects modelling to investigate effects of emotional condition, 
perceived and felt emotion and music perception variables on psychophysiological response, while taking into account order 
effects of listening.  
 
 
40 Enjoying Sad Music: A Test of the Prolactin Hypothesis 
 
Olivia Ladinig, David Huron *, and Charles Brooks 
 
The Ohio State University, Columbus, Ohio, USA 
 * = Correponding author, huron.1@osu.edu 
 
 
This study tests the hypothesis that enjoyment of sad music is mediated by the hormone prolactin.  Prolactin is released under 
conditions of stress (such as crying) and is known to have a comforting psychological effect (Brody & Kruger, 2006).  Huron 
(2010) conjectured that prolactin might be released during music‐induced sadness, and that when the sadness is "fictional" (as 
in artistic contexts), some listeners will benefit from prolactin's positive hedonic effects without the psychic pain associated 
with true grief or sadness. Specifically, this study tests the following hypothesis: For those listeners who report enjoying 
listening to sad music, sad music will cause an increase in serum prolactin concentrations; whereas those listeners who report 
not enjoying sad music will show little or no increase in prolactin when listening to sad music. Participants had been pre‐
selected and were drawn from two groups: listeners who professed to have a strong liking for sad music andl isteners who 
professed a strong dislike for sad music. Participants listened to sad and happy music while prolactin concentrations were 
measured.  To avoid listener‐specific associations, sad and happy musical materials were pre‐selected using independent 
listeners. Two controls were used.  The first contrasted musical excerpts with silent periods.  This allows a comparison 
between prolactin levels arising from musical exposure and baseline prolactin levels in the absence of music.  The second 
control contrasted sad music with non‐sad music.  This tests whether prolactin concentrations are specifically related to sad 
music, and not merely a response to music in general. The research is pertinent to a problem posted by Aristotle regarding 
how people might enjoy artistic protrayals of negative emotions. 
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    28 
 
41 Validating Emotionally­Representative Musical Selections: Relationship Between 
Psychophysiological Response, Perceived and Felt Emotion 
 
Laura A. Mitchell (1)*, Anna M.J.M. Paisley (2) and Daniel J. Levitin (3) 
 
(1) Bishop’s University, Sherbrooke, Canada (2) Glasgow Caledonian University, Glasgow, UK (3) McGill University, Montreal, Canada 
* = Corresponding author lauramitchellwork@gmail.com 
 
Understanding the relationship between music and emotion remains a central issue in music psychology research. This 
includes investigating the recognition of musical emotion (e.g. Quintin et al., in press), the real‐time continuous physiological 
reactions to music (e.g., Chapados & Levitin, 2008), as well as subjective/introspective reports of the emotions that music 
induces (e.g. Vines et al., in press).  In a wide variety of research arenas, including research that is not addressed toward 
musical processing per se, music is frequently used as a means of emotional induction, making accessibility of pre‐tested 
stimuli fundamental for consistency between studies. Vieillard et al. (2008) established a set of stimuli using specially‐
composed music conveying emotion through musical structure, finding that four specific emotions of happy, sad, peaceful and 
scary could be distinguished. To encourage ecological validity of stimuli, the current study provides self‐report data of 
perceived musical emotion, felt emotion, liking and valence and psychophysiological measurement for 20 easily available 
pieces of music grouped by four emotional qualities. The two‐minute instrumental pieces were selected through extensive 
piloting for being reliably identified with the emotions peaceful, scary, happy and sad. Thirty‐six participants (20 females, 
mean age 29.3) listened individually on headphones to all 20 selections in one session, randomized within four emotional 
blocks counterbalanced in order. Heart rate, respiration rate and skin conductance were recorded using the Biopac MP35 
system, and subjects used three minutes between tracks to rate familiarity, liking, pleasantness, perceived emotional 
expression and felt emotional response on a visual analogue scale. Musicianship and everyday listening habits were recorded 
by questionnaire. Results will be analyzed using linear mixed effects modelling to investigate effects of emotional condition, 
perceived and felt emotion and music perception variables on psychophysiological response, while taking into account order 
effects of listening.  
 
 
 
 
42 Piece vs Performance: Comparing Coordination of Audiences' Physiological 
Responses to Two Performances of Arcadelt's "Il bianco e dolce cigno" 
 
Finn Upham (1)*, Stephen McAdams (1) 
 
(1) Centre for Interdisciplinary Research in Music Media and Technology, McGill University, Montreal, Canada 
* = Corresponding author, finn.upham@mail.mcgill.ca 
 
What aspects of music determine listeners' emotional experience? While the notes are important, performance details may 
also have a strong effect. By looking at physiological responses to two different performances of the same work, we consider 
which shared responses might be due to the common musical structure and which might depend on the performers' 
interpretations of the work or other differentiating conditions. Two audiences' biosignals were continuously recorded. One 
audience of 63 participants listened to a studio recording of a professional male choir performing Jacques Arcadelt's Il bianco e 
dolce cigno played over large stereo speakers. The second audience of 40 participants attended a live performance of the same 
work by a semiprofessional mixed choir. The two audiences shared a group listening experience of the same musical work, but 
differed in participants, hall and performance.  To quantify the audiences' response to the music, we examine the coordination 
of strong responses across each audience in features extracted from the four biosignals: skin conductance (SC), 
electromyography of the zygomaticus (EMGz), electromyography of the corrugator (EMGc) and blood volume pulse (BVP). In 
those moments when the audiences show similar degrees of coordinated physiological activity, this may be caused by the 
similarities between the two stimuli. Results show similar patterns of high and low levels of simultaneous activity in 
corrugator and zygomaticus EMG signals and skin conductance response in both audiences when responses are aligned to the 
score. Some differences in skin conductance response coordination align with differences in the two interpretations, 
particularly contrasting dynamics. Concurrent corrugator and zygomaticus activity challenge the opposite valence 
interpretation of these signals. These results suggest that the corrugator, a muscle in the brow, may be an indication of tension 
rather than unpleasantness. 
                                                                                               SMPC 2011 Program and abstracts, Page:       
                                                                                                                                       29 
 
43 Ragas of Hindustani classical music and tempo on appraisal of happy and sad 
emotion: A developmental study 
 
Shantala Hegde (1)*, Bhargavi Ramanujam (1), Arya Panikar (1) 
 
(1) Cognitive Psychology & Neuroscience Laboratory, Cognitive Psychology Unit, Center for Cognition and Human Excellence, Department of Clinical 
Psychology, National Institute of Mental Health and Neurosciences, Bangalore, India  
* = Corresponding author, shantalah@nimhans.kar.nic.in 

Studies examining emergence of sensitivity for components of music in appraisal of emotion are very few. We examined the 
role of pitch distribution in ragas of Hindustani classical music (HCM) and presence/ absence of pulse in appraisal of happy 
and sad emotion. Sample included musically untrained children aged 5‐6 years (n=30), 10‐11 years (n=39). Data was 
compared with ratings by musically untrained adults (n=30). Six ragas of HCM, three to evoke positive emotion (happy‐ragas) 
and three to evoke negative emotion (sad‐ragas) Two excerpts from each raga, one from the raga‐elaboration phase (Alap) 
without pulse and the other (Jor‐ Jhala) with pulse (~ 64 pulses per minute) formed the stimulus. Timbre was kept constant. 
Participants rated the stimulus on a five point Likert scale. Children aged 5‐6 years could differentiate the happy‐ragas from 
sad‐ragas based on the degree of happiness (p<0.01 level). Tempo did not influence their ratings. Children aged 10‐11 years 
could distinguish the two sets of ragas above level of chance (p<0.01) similar to the adult group. Happy‐raga excerpts with 
tempo were perceived as happier than excerpts without pulse and sad‐ raga excerpts without pulse were perceived as sadder 
than excerpts with tempo (p <0.01). This is the first study examining appraisal of musical emotion using ragas of HCM in 
musically untrained native Indian children and to examine the influence of presence or absence of pulsated tempo on 
appraisal of musical emotion. Distribution of major (Shuddh‐swaras) and major pitches (komal‐swaras) in raga influenced 
appraisal of emotion. Presence of pulse seemed to influence the perception of emotion differently for the two emotions. 
Findings of this study will enable us to understand development of abilities to appraise musical properties and appraise the 
emotional content even without formal musical training. (Grant funding: Department of Science and Technology SR/FT/LS‐
058/ 2008, India.) 

 

44 Literacy makes a difference: A cross­cultural study on the graphic representation 
of music by communities in the United Kingdom, Japan, and Papua New Guinea  
 
George Athanasopoulos (1)*, Nikki Moran (2), Simon Frith (3) 
 
(1) University of Edinburgh, Edinburgh, U.K., (2) University of Edinburgh, Edinburgh, U.K., (3) University of Edinburgh, Edinburgh, U.K., 
* = Corresponding author, s0880167@sms.ed.ac.uk. 
 
Cross‐cultural research involving 102 performers from distinct cultural backgrounds observed how musicians engage with the 
textual representation of music, considering in particular the effect of literacy. The project took place at five fieldwork sites in 
three countries, involving classical musicians based in the UK; traditional Japanese musicians both familiar and unfamiliar 
with western standard notation; and members of the BenaBena tribe, a non‐literate rural community in Papua New Guinea. 
Performer responses were examined in two studies which used original visual and auditory stimuli to explore distinctions 
between cultural and musical factors of the visual organization of musical sounds. Participants heard up to 60 short stimuli 
that varied on three musical parameters (pitch, duration and attack rate). The instructions were simply to represent these 
visually so that if another community member saw the marks they should be able to connect them with the sounds.  2. A 
forced‐choice design required that participants select the best shape to describe a sound (24 trials). Additionally, ethnographic 
interviews were carried out at fieldwork sites to provide richer, qualitative data regarding the participants’ response to the 
research. Three styles of symbolic representation emerged: linear‐notational (x‐y axial representation, with time located on x 
axis and variable parameter on y axis); linear‐pictorial (axial time indication, variable parameter represented pictorially); and 
abstract‐pictorial (no axial representation). Results for 1) showed that over 90% literate participants used linear‐notational 
representation. Of the non‐literate participants, just 22.3% of responses were linear‐notational, but 39.3% were linear‐
pictorial. The remainder (38.4%) consisted of abstract‐pictorial responses. 2) Again, over 90% of the literate groups’ 
responses showed a preference for linear‐notational. No trends emerged in the non‐literate group’s responses. Results will be 
discussed with reference to qualitative data from the content analysis of the ethnographic material.  
                                                                           SMPC 2011 Program and abstracts, Page:       
                                                                                                                   30 
 
 
45 Cross­cultural differences in meter perception 
 
Beste Kalender *, Sandra E. Trehub, & E. Glenn Schellenberg. 
         
University of Toronto, Ontario, Canada 
* = Corresponding author, bestekalender@yahoo.com  
 
The perception of metrical patterns is influenced by musical enculturation. For example, listeners tap more accurately to 
culturally familiar than to unfamiliar pop music that is similar in metrical structure. Musical enculturation can also interfere 
with the perception of novel metrical categories. We asked whether exposure to complex meters from one musical culture 
facilitates the perception of complex meters from a foreign musical culture. The participants (n=57) were monolingual and 
bilingual adults with exclusive exposure to Western music (simple meters) or exposure to non‐Western (simple and complex 
meters) and Western music. Adults were tested with MIDI instrumental versions of Turkish folk melodies, two in simple meter 
(2/4) and two in complex meter (5/8). After familiarization (2 min) with each melody, participants heard a meter‐preserving 
alteration, and a meter‐violating alteration (random order) and rated the rhythmic similarity of each to the familiarization 
stimulus. Scores consisted of rating differences between meter‐preserving and meter‐violating alterations. Simple‐meter 
scores exceeded chance levels for bimusical listeners, t(23) = 4.71, p < .001, and for monomusical listeners, t(32) = 5.60, p < 
.001, but complex‐meter scores exceeded chance levels only for bimusical listeners, t(23) = 2.78, p < .05. In short, only 
listeners with exposure to complex meters detected changes in complex metrical patterns from an unfamiliar musical culture. 
Multiple regression analysis revealed a significant effect of music background (monomusical or bimusical), t(54) = 2.82, p < 
.01, but no effect of language background (monlingual or bilingual). Specifically, musical enculturation influenced performance 
with complex meters when language status (monolingual, bilingual) was held constant, but language status had a negligible 
effect on performance when musical background was held constant. Moreover, musical training did not account for the 
advantage of bimusical listeners. In short, the findings reveal transfer of training from a familiar to an unfamiliar musical 
culture. 
 
 
 
46 Auditory structural parsing of Irish jigs: The role of listener experience 
  
Christine Beckett * 
 
Concordia University, Montreal QC Canada 
* = Corresponding author, beckettchristine@hotmail.com 
 
A literature search (Beckett, 2009) showed that few perceptual studies have examined listeners’ experiences of Irish music. In 
recent perception/production work, it was found that after 30m listening, listeners can create novel jigs but that non‐
musicians reproduce formal structure less accurately than musicians (Beckett, 2010). This raised the question of how 
participants perceived structure. Here, 48 participants in 4 groups (musicians/non‐musicians, with low/high listening 
experience of Irish music) heard 7 unaccompanied Irish jigs twice each (4 traditional jigs on flute, 3 novel jigs sung). After the 
second hearing, participants used a computer tool to pinpoint divisions between what they heard as structural units of the 
melody. Data were recorded in ms of lapsed music. Timeline analysis showed that musicians and non‐musicians with low 
exposure to Irish music mostly marked large regular units similar to typical Euro‐western phrases (4 or 8 bars). Musicians and 
non‐musicians with high exposure to Irish music tended to mark shorter, more numerous, and somewhat irregular units (2 
bars, 1 bar, or less). Results were to some extent driven by discrepant terminology, especially amongst Irish performers. 
Replication with musicians and non‐musicians in Ireland, all highly familiar with their own traditional music, is planned for 
March and April 2011. An emerging conclusion is that increasing familiarity with Irish traditional music leads to more fine‐
grained perception and judgments of structure. In contrast, increasing familiarity with Euro‐western music arguably leads to 
perceiving ever‐larger units of structure. An important implication is that when working with another musical tradition, 
researchers must not assume Western norms as valid for people experienced in that tradition. 
  
                                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                                     31 
 
 
47 How Does Culture Affect Perception of Rhythm? 
 
Naresh N. Vempala (1)*, Frank A. Russo (1) 
 
(1) SMART Lab, Department of Psychology, Ryerson University, Toronto, Canada 
*  = Correponding author, nvempala@psych.ryerson.ca 
                                                                  
Previous research suggests that listeners internalize the music of their culture to create a conceptual representation that 
reflects statistical regularities (Bharucha, 1987; Krumhansl, 1987). When listening to a piece of music, listeners use their 
existing mental template as shaped by their musical experience. Their enculturated representation of rhythm and meter 
interacts with available sensory information gathered from the stimulus to influence the perception of structure (Palmer & 
Krumhansl, 1990). Our goal for this study was to understand how musical enculturation could affect a listener’s sensitivity to 
rhythmic structure conveyed by intensity accents. We examined goodness‐of‐fit judgments of probes in different rhythmic 
contexts across two different cultural groups, Canadians and Ecuadorians. We presented four different types of rhythmic 
stimuli comprising symmetric and asymmetric rhythmic groupings to both sets of participants. Our hypothesis was that 
culture would influence perception of rhythmic groupings leading to an interaction of probe with culture for each type of 
rhythmic grouping. Because Canadians would have predominantly been enculturated to symmetric groupings, we predicted 
them to show sensitivity to the intensity accent structure present in symmetric rhythmic stimuli. In contrast, because of their 
enculturation to asymmetric rhythmic groupings, we predicted that Ecuadorians would show sensitivity to the intensity 
accent structure present in asymmetric rhythmic stimuli but not in symmetric stimuli. Our results showed that Canadian but 
not Ecuadorian participants were more sensitive to surface structure in symmetric rhythmic groupings than to asymmetric 
rhythmic groupings. Our results indicated the strong effect of internalized rhythmic schemas as a product of enculturation, in 
perceiving rhythmic structure. Based on the results of this study, we propose an initial set of rules for developing a theoretical 
model of rhythmic perception that focuses on the influence of previous enculturation. 
 
 
 
48 Individual Differences in Neuronal Correlates of Imagined and Perceived Tunes 
 
Sibylle C. Herholz (1), Andrea R. Halpern (2)*, Robert J. Zatorre (1) 
 
(1) Montreal Neurological Institute, McGill University; International Laboratory for Brain, Music and Sound Research (BRAMS); Centre for Interdisciplinary 
Research in Music Media and Technology (CIRMMT); (2) Department of Psychology, Bucknell University  
* = Corresponding author, ahalpern@bucknell.edu 
 
 
We investigated auditory imagery of familiar melodies to determine the overlapping and distinct brain areas associated with 
auditory imagery and perception. We also studied individual differences in auditory imagery vividness.  We scanned ten 
healthy right‐handed people using sparse sampling fMRI. The Bucknell Auditory Imagery Scale (BAIS) assessed individual 
ability to evoke and manipulate auditory images. During encoding, participants listened to or imagined familiar melodies, cued 
by karaoke‐like presentation of their lyrics. A silent visual condition served as baseline. During recognition tune titles served 
as cues.  Conjunction analysis for encoding showed overlap of activity during imagery and perception in anterior and posterior 
parts of secondary auditory cortices. The contrast of imagery vs perception revealed an extensive cortical network including 
prefrontal cortex, supplementary motor area, intraparietal sulcus and cerebellum. BAIS scores correlated with activity in right 
anterior superior temporal gyrus, prefrontal cortex and anterior cingulate. Auditory cortices and SMA showed greater 
functional connectivity with frontal areas during imagery compared to perception. During recognition, inferior frontal cortex 
and anterior cingulate cortex, as well as left STS, were active compared to baseline. BAIS scores correlated with activity in left 
ACC, anterior middle temporal sulcus and prefrontal cortex. The overlap of activity in secondary auditory areas during musical 
imagery and perception indicates that similar neuronal networks are involved in both processes. In addition, during imagery, 
areas implicated in working memory, mental manipulation, and motor preparation are recruited. Reported vividness during 
encoding was directly related to activity in secondary auditory cortex, indicating that vividness of imagery is related to 
processing areas for the input modality. The correlation during retrieval suggests that more vivid imagers use that ability to 
facilitate memory retrieval. That the correlation was obtained using a questionnaire suggests that people’s impression of their 
own abilities matches the neural resources they devote to these tasks. 
 
                                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                                     32 
 
 
49 Neural Specialization for Music 
 
Samuel Norman‐Haignere (1)*, Josh McDermott (2), Evelina Fedorenko (1), Nancy Kanwisher (1) 
 
(1) McGovern Institute for Brain Research, Massachusetts Institute of Technology, Cambridge, USA, (2) Center for Neural Science, New York University, USA 
 * = Corresponding author, svnh@mit.edu 
 
Do specialized neural mechanisms exist in the human brain for processing music? Here we use fMRI to address this question 
by searching for regions in individual subjects with a strong response to music compared with speech stimuli, and then testing 
the responses of these regions to manipulations of musical and linguistic structure. Six subjects were scanned for 2 hours 
while listening to short, 9‐second clips of music and speech.  Music stimuli consisted of professionally recorded instrumental 
music that varied both in genre (classical vs. movie/tv scores) and familiarity (famous music vs. matched novel controls). 
Speech stimuli consisted of pairs of spoken sentences and nonword strings matched for prosody, thereby allowing us to also 
examine differences in higher‐level linguistic structure.  MIDI representations of music were used to examine responses to 
musical structure by scrambling the higher‐order pitch and rhythmic organization in half of the clips while preserving lower‐
level acoustic properties.  In all six subjects tested, we observed a region in the anterior part of the superior temporal gyrus 
with a strong, invariant responses to all 4 types of music compared with speech. This region also showed a higher response to 
intact compared with scrambled MIDI music, but no difference between normal sentences and nonword strings.  These results 
suggest that music perception may be in part subserved by specialized neural mechanisms that are sensitive to musical 
structure.  Current work is exploring the selectivity of this region to other types of music and auditory stimuli as well as to 
more general auditory features such as pitch.  
 
 
 
 
 
 
  
50 Music as an aid to learn new verbal information in Alzheimer's disease 
 
Moussard. A. (1, 2)*, Bigand, E. (2), & Peretz, I. (1) 
 
 (1) Université de Montréal, Montréal, Canada, and BRAMS  laboratory; (2) Université de Bougogne, Dijon, France, and LEAD  laboratory. 
* = Corresponding author, Aline.Moussard@u‐bourgogne.fr 
 
The goal of this study is to determine if music can improve   verbal memory in Alzheimer's patients. There are multiple 
observations  and anecdotes suggesting that music is a privileged material to  support and stimulate memory in Alzheimer 
patients. There is as yet no  experimental evidence and hence no explanation of how and why music  could be useful to 
individuals with dementia. In normal subjects,  verbal information (i. e., words or texts) is not necessarily better  learned when 
it is sung as compared to being spoken. The melodic  component often represents an additional load for learning, creating a  
dual task situation (Racette &Peretz, 2007). However, if the tune is ‐  or becomes ‐ familiar, the preexisting representation of 
the familiar  melody can provide support for associating new verbal information  (Purnell‐Webb & Speelman, 2008). Our study 
compare, in a mild  Alzheimer's disease patient, different learning conditions of an  unknown text: spoken, and sung on an 
unfamiliar, mild familiar, and high familiar melody. Moreover, the spoken and unfamiliar sung  conditions are relearned 4 
times again (with one week interval between  each), and once again after 4 weeks. The results show that the  excerpts sung on 
a mild or highly familiar melody are better learned  than the unfamiliar one. Moreover, while the spoken condition seams to  
be better performed than the condition sung on the unfamiliar melody  for the first learning session, this seams to be reversed 
over the  relearning sessions, especially after the 4 weeks delay, where the  sung condition become significantly better 
memorized. We discuss these  results depending on their therapeutic implications for clinical care  of dementia. 
                                                                          SMPC 2011 Program and abstracts, Page:       
                                                                                                                  33 
 
51 Examining the Role of Training and Movement on Rhythm Perception in Disc 
Jockeys Using EEG and Behavioural Thresholds 
 
Blake Butler (1)*, Laurel Trainor (1) 
 
(1) McMaster University, Hamilton, Canada 
* = Corresponding author, BUTLERBE@MCMASTER.CA 
 
Professional disc jockeys (DJs) maintain a consistent beat by altering the tempo of one or more tracks.  Studies have shown 
that non‐musician listeners find it difficult to maintain a steady rhythm in the absence of an external acoustic stimulus, but 
that trained musicians are significantly better.  The goals of this project were to determine whether DJs show an advantage in 
maintaining a steady imagined beat, and whether movement helps in this regard.  Ten professional DJs, ten age‐matched 
controls, and ten percussionists participated.  In Part I, participants heard a sequence of alternating downbeats and upbeats, 
were asked to imagine the rhythm continuing for four additional downbeats, and to determine if a target downbeat following 
the silence was on‐time or early.  In one condition subjects were allowed to move with the rhythm while in another they 
remained still. The onset of the target varied from 0 to 62.5% early, and psychometric functions were created to determine 
detection thresholds.  DJs showed greater accuracy in determining whether the target beat was early or on‐time than control 
subjects (p=0.039), but were not significantly different than the percussionist group (p>0.05).  Accuracy of each group was 
significantly impaired when asked to perform the task in the absence of movement (all p<0.05).  In Part II, event‐related 
potentials (ERPs) were examined to real and imagined beats and to the occasional early start of a beat new sequence.  
Responses to early beats were compared to standard downbeats.  Preliminary results indicate group differences, with larger 
ERP responses to imagined and early beats for DJs and percussionists compared to controls.  These results suggest that 
rhythmic experience, whether as a DJ or percussionist, changes the brain.  We are currently testing the effects of experience in 
a controlled way by studying DJ students before and after an intensive week of training. 
 
 
52 Neural dynamics of beat perception 
 
John Iversen*, Aniriddh Patel 
 
The Neurosciences Institute, San Diego, CA USA 
* = Corresponding author, iversen@nsi.edu 
 
Our perceptions are jointly shaped by external stimuli and internal interpretation. The perceptual experience of a simple 
rhythm, for example, strongly depends upon its metrical interpretation (where one hears the beat). Such interpretation can be 
altered at will, providing a model of the voluntary cognitive organization of perception. Where in the brain do the bottom‐up 
and top‐down influences in rhythm perception converge? Is it purely auditory, or does it involve other systems? To 
understand the neural mechanisms responsible for beat perception and the metrical interpretation, we measured brain 
responses as participants listened to a repeating rhythmic phrase, using magnetoencephalography. In separate trials, listeners 
(n=11) were instructed to mentally impose different metrical organizations on the rhythm by hearing the downbeat at one of 
three different phases in the rhythm. The imagined beat could coincide with a note, or with a silent position (yielding a 
syncopated rhythm). Since the stimulus was unchanged, observed differences in brain activity between the conditions should 
relate to active rhythm interpretation. Two effects related to endogenous processes were observed: First, sound‐evoked 
responses were increased when a note coincided with the imagined beat. This effect was observed in the beta range (20‐30 
Hz), consistent with earlier studies. Second, and in contrast, induced beta responses were decoupled from the stimulus 
and instead tracked the time of the imagined beat. The results demonstrate temporally precise rhythmic modulation of beta 
responses that reflect the active interpretation of a rhythm. Given the suggested roles of beta in motor processing and in long‐
range intracortical coordination, it is hypothesized that the motor system might drive these temporal effects, even in the 
absence of overt movement. Preliminary independent component localization analysis (n=4) supports this view, consistently 
finding beat‐related activity in motor areas. (Supported by Neurosciences Research Foundation.) 
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    34 
 
53 Tapping to Hear: “Moving to the Beat” Improves Rhythmic Acuity                                                                           
 
Fiona Manning*, Michael Schutz 
 
McMaster Institute for Music and the Mind, McMaster University, Hamilton, Canada 
* = manninfc@mcmaster.ca 
 
There has been increasingly interest in recent years in investigating links between perception and action (Hommer, Mussler 
Aschersleben, & Prinz, 2001). Here, we explore this issue through a novel paradigm designed to measure the effect of “moving 
to the beat” when listening to rhythmic music.  Specifically, we examined the effect of tapping on participants’ sensitivity to 
changes in the temporal location of a probe tone. In doing so, we have documented a novel instance of a perception‐action link 
in an objective task, using participants selected without regard for musical training. In this experiment, participants heard a 
series of isochronous beats, and were asked to identify whether the final tone after a short silence was consistent with the 
timing of the preceding rhythm. On half the trials, participants tapped along with the pulse on an electronic drum pad, and on 
half the trials they were asked to listen without tapping. When the probe tone was late (i.e., after the expected beat), 
performance in the tap condition was significantly better than performance in the no‐tap condition (an 87% improvement).  
However, there was no effect of tapping on performance when the probe tone was early or on time. This asymmetric effect of 
tapping on performance is consistent with previous work documenting a tendency for taps that “anticipate” the target beat 
(reviewed in Aschersleben, 2002). The effect of movement on the perception of rhythmic information is consistent with 
previous work showing that movement can affect the perception of metrically ambiguous stimuli (Phillips‐Silver & Trainor, 
2005; 2007). Therefore this data extends previous research by demonstrating that movement can objectively improve the 
perception of rhythmic information, which suggests that part of our propensity to move while listening to music may stem 
from this movement’s ability to improve our understanding of its rhythmic structure.  
 
 
 
54 Effect of movement on the metrical interpretation of ambiguous rhythms: 
Phillips­Silver and Trainor (2007) revisited. 
 
J. Devin McAuley (1)*, Molly J. Henry (2), Prashanth Rajarajan (3), Karli Nave (4) 
 
(1) Department of Psychology, Michigan State University, East Lansing, USA, (2) Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany, 
(3) Department of Psychology, Michigan State University, East Lansing, USA, (4) Department of Psychology, Michigan State University, East Lansing, USA 
* = Corresponding author, dmcauley@msu.edu  
 
Phillips‐Silver and Trainor (2005, 2007, 2008) reported that moving the body (or having one’s body moved) in synchrony with 
a metrically ambiguous rhythm biases the encoding of the rhythm in a manner consistent with the movement in both infants 
and adults.  The current study reports the results of a series of experiments where we failed to replicate the reported effect of 
metrical movement on auditory rhythm processing.  All experiments used the same rhythms and general procedure described 
in Phillips‐Silver and Trainor (2007, Experiment 1).  In an initial encoding phase, participants were ‘bounced’ to an ambiguous 
rhythm in time with beats that corresponded to either a duple or triple metrical interpretation.  In a subsequent test phase, 
participants listened to pairs of rhythms – one accented in a duple fashion and the other accented in a triple fashion – and 
judged which of the two rhythms best matched what they heard during the encoding phase. Inconsistent with Phillips‐Silver 
and Trainor (2007), no reliable effect of bouncing meter was observed on proportions of ‘duple’ responses; i.e., duple and 
triple bouncing in the encoding phase did not lead to duple and triple preferences in the test phase.  The same null effect of 
bouncing meter was observed for samples of undergraduate students, ballroom dancers, and members of the marching band.  
Changing the task so that participants judged their movement in the encoding phase, rather than what they heard, revealed 
that participants were able to accurately recall the way they were bounced.  Possible reasons for the failure to replicate 
Phillips‐Silver and Trainor (2007) will be discussed. 
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    35 
 
55 The Effect of Melodic Structure and Event Density on Perceived Tempo 
 
Emily Cogsdill and Justin London* 
 
Carleton College, Northfield MN USA 
Corresponding author: jlondon@carleton.edu 
 
Multiple cues contribute to our sense of musical tempo, including melodic interval, durational patterning (IOI), sequence 
complexity, and event density, and perceived beat rate. We examine the interaction between event density and melodic motion 
in the context of a tempo judgment task.  Stimuli consisted of melodic vs. percussive stimuli, the latter created by replacing the 
pitch information with a repeated wood‐block sound.  Stimuli also had two levels of surface activity (dense vs. sparse). Stimuli 
were presented at six tempos (72, 80, 89, 100, 120, and 133 bpm). 24 Participants tapped along at two controlled rates and 
made tempo ratings using a 9‐point scale.  A main effect was observed for density, t(46) = 2.40, p < .05. No main effect was 
found for melodic structure.  In the fast tap condition percussive stimuli elicited faster tempo judgments at 100 and 120 bpm 
(t(190) =  1.00, p < .001; t(190) = 0.99, p < .05). In the slow tap condition a similar pattern emerged at 72 and 80 bpm, (t(190) 
= 0.99, p < .05; t(190) = 0.97, p < .05).  No other interaction effects were observed. Greater event density consistently led to 
faster tempo judgments; greater melodic movement did not.  Indeed, melodic motion led to slower tempo judgments in some 
contexts.  The effect of melodic structure on perceived tempo is influenced by listeners’ self‐motion, suggesting that the 
interaction between melodic motion and other parameters merits further study. 
 
 
 
 
 
 
56 Can Musicians Track Two Different Beats Simultaneously? 
 
Ève Poudrier (1)* & Bruno H. Repp (2) 
 
(1) Yale University, New Haven, Connecticut, USA, (2) Haskins Laboratories, New Haven, Connecticut, USA 
* = Corresponding author, eve.poudrier@yale.edu 
 
The simultaneous presence of different metrical structures is not uncommon in Western art music and the music of various 
non‐Western cultures. However, it is unclear whether it is possible for listeners and performers to cognitively establish and 
maintain different beats simultaneously without integrating them into a single metric framework. The present study 
attempted to address this issue empirically. Two simple but non‐isochronous rhythms, distinguished by widely separated 
pitch registers and representing different meters (2/4 and 6/8), were presented simultaneously in various phase 
relationships, and participants (classically trained musicians) had to judge whether a probe tone fell on the beat in one or both 
rhythms. In a selective attention condition, they had to attend to one rhythm and to ignore the other, whereas in a divided 
attention condition, they had to attend to both. We conducted three experiments of increasing complexity. In Experiment 1, 
the rhythms shared the lowest articulated metrical level even when they were out of phase.  Participants performed 
significantly better in the divided attention condition than predicted by the null hypothesis that they would be able to attend 
to only one rhythm at a time (taking possible guessing into account). In Experiment 2, the rhythms were interleaved, and here 
the null hypothesis could not be confidently rejected, although some participants did perform better than predicted. In 
Experiment 3, the rhythms were interleaved and changed from trial to trial. Here, performance in the divided attention 
condition was clearly at chance level. In Experiment 1, participants may have relied on the composite beat pattern (the non‐
isochronous combined sequence of the two underlying beats), rather than tracking the two beats independently, a strategy 
that was difficult to apply in Experiments 2 and 3. Thus, we do not so far have clear evidence that two different beats can be 
tracked independently. 
                                                                           SMPC 2011 Program and abstracts, Page:       
                                                                                                                   36 
 
57 Spontaneous goal inference without concrete external goals: Implications for the 
concept of dance 
 
Adena Schachner (1)*, Susan Carey (1) 
 
(1) Department of Psychology, Harvard University, Cambridge, MA, USA 
* = Corresponding author, amschach@fas.harvard.edu 
 
We automatically represent others’ actions in terms of their goal—for actions with concrete external goals (e.g. reaching for an 
object; Woodward, 1998). Is this true for dance actions? We hypothesized that our mental concept of dance is not simply a list 
or collection of movement features, but is primarily based on the actors' goal. We also predicted that actions with no concrete 
external goal (like dance) would lead observers to infer that the goal is to perform the movements themselves. In three 
between‐subject experiments, participants saw either (a) movements with no external goal (animated character moving in an 
empty room); or (b) the exact same movements, with an external goal (manipulating objects). In experiment 1, we asked 
participants whether the actions they saw were dance. Those participants who saw the actions performed without objects 
present were significantly more likely to say that the actions were dance. Since the movements themselves were identical 
across conditions, this suggests that the movement features were not driving this categorization. Experiments 2 and 3 directly 
tested the hypothesis that goal inferences drive categorization of actions as dance. We first determined what goal participants 
inferred, if any. Then, we asked participants how likely it was that the actions were actually dance. We found that nearly half of 
participants in the no‐external‐goal condition inferred that the movements themselves were the goal. In addition, participants 
who inferred movement‐as‐goal later rated the actions as more likely to be dance, exercise, or ritual, even compared to other 
participants who saw the exact same stimuli. Thus, our concept of dance rests primarily on the inferred goal of the actor, not 
solely on features of the movement. These data also suggest that idea of movement as the goal forms an important part of our 
concept of dance. 
 
 
 
58 Linking perception to cognition in the statistical learning of tonal hierarchies 
 
Dominique Vuvan (1)*, Mark Schmuckler (1) 
 
(1) University of Toronto Scarborough, Toronto, Canada 
* = Corresponding author, dominique.vuvan@utoronto.ca 
 
By what mechanisms do listeners learn the rules that govern their musical system?  One potential mechanism is statistical 
learning, referring to the human perceptual system's exquisite sensitivity to the statistical properties of the environment. 
Tonality describes the rules that hierarchically organize musical pitch. A handful of studies have demonstrated listeners’ 
responsiveness to frequency properties of novel tone organizations. However, no research has yet linked listeners' perception 
of these properties to their cognitive processing of those tone structures in a manner consistent with the cognitive processing 
of musical tonality. Three experiments sought to establish this link. In Experiment 1, listeners were exposed to a continuous 
stream constructed of three‐tone diatonic “tone‐words” (Saffran, et al., 1999). After exposure, listeners parsed this stream into 
its constituent tone‐words, but their melody memory performance did not differ between melodies composed of the tone‐
words and melodies composed randomly. In Experiment 2, listeners were exposed to diatonic chord sequences composed 
using a novel harmonic grammar (Jonaitis & Saffran, 2009). After exposure, listeners discriminated chord sequences that 
obeyed the grammar from those that did not, but did not display tonal priming effects for grammar obeying sequences. In 
Experiment 3, 20 undergraduate listeners were exposed to a series of melodies that followed a harmonic grammar 
constructed from the Bohlen‐Pierce (non‐diatonic) system (Loui & Wessel, 2008). After exposure, listeners’ melody memory 
performance was affected by whether the melody followed the harmonic grammar of the melodies during the exposure phase. 
Therefore, tonal structures acquired through statistical learning can have cognitive consequences, as long as their constituent 
units are not already implicated in other highly‐learned tonal systems (i.e., Western diatonic tonal hierarchies). 
 
                                                                          SMPC 2011 Program and abstracts, Page:       
                                                                                                                  37 
 
59 Inattentional Deafness in Music: The Role of Expertise and Familiarity 
 
Sabrina Koreimann (1)*, Oliver Vitouch (2) 
 
(1) Klagenfurt, Austria, (2) Klagenfurt, Austria 
* = Corresponding author, sabrina.koreimann@uni‐klu.ac.at 
 
Contrary to inattentional blindness effects, phenomena of inattentional deafness are less well‐known. We here present, to the 
best of our knowledge, the first tests of inattentional deafness in music under controlled experimental conditions. Participants 
listened to a modification of the first 1’50” of Richard Strauss’ Thus Spake Zarathustra, with the experimental group having the 
task of counting the number of tympani beats in the piece and the control group just listening. An e‐guitar solo served as the 
unexpected event. In our initial study, experimental data of N = 115 subjects (18‐63 years, M = 26 years, 64% female) were 
analyzed. To investigate the impact of expertise, musicians (n = 57) were compared with non‐musicians (n = 58). To test 
familiarity effects, two further experimental groups were investigated which were previously familiarized with the original 
piece of music and the primary task (counting task). In our initial study results demonstrate an inattentional deafness effect 
under dynamic musical conditions (χ2 [total] = 18.8, p < .001, N = 115). Unexpectedly, our examinations show structurally 
equivalent, although less extreme results even in the musicians group. The experimental variation of familiarity with the piece 
and the primary task had moderate effects, but did not make inattentional deafness effects disappear. Based on these findings, 
subsequent experiments will aim at elucidating open questions such as the levels of perceptual processing involved, and 
individual and situational performance predictors. 
 
 
 
 
 
60 Identification of familiar melodies from rhythm or pitch alone 
 
Kathleen D. Houlihan (1)*, Daniel J. Levitin (1) 
 
(1) McGill University, Montréal, Canada 
* = Corresponding author, Kathleen.houlihan@mail.mcgill.ca 
 
To what extent are pitch and rhythm by themselves sufficient retrieval cues for identifying well‐known songs? That is, in a 
Roschian context, how might we evaluate their cue validity for accessing specific examples of songs? We report new data from 
an experiment based on the seminal work of White (1960) which examined identification of transformed musical information, 
and the consequent research on identification of songs based on rhythm or pitch information alone by Hebert & Peretz (1997). 
Our aim was to discover the extent to which rhythm alone or pitch alone can be used for identifying songs, and to establish a 
set of validated materials for future researchers to use. To accomplish this, we examined the efficacy of song identification 
based on impoverished musical information. Specifically, we examined rhythm information and pitch information alone 
crossed with the sub‐variables of quantization and dynamics. Participants (n=534) in a between‐subjects design were 
presented with excerpts of well‐known songs with solely the rhythmic information or pitch information intact. The results 
revealed a low percentage of correct identification overall, and greater identification in the pitch conditions; however, we 
observed a wide range of accuracy in identification across songs. This indicates a previously unreported phenomenon that was 
masked in earlier reports that used averaging techniques: certain songs (presumably as a function of their underlying 
compositional features) are more readily identified by rhythm alone or pitch alone than others. Additionally, the present study 
provides the research community with a useful tool, a corpus of well‐known songs a subset of which are reliably identifiable 
by rhythm or pitch alone. 
 
 
                                                                          SMPC 2011 Program and abstracts, Page:       
                                                                                                                  38 
 
 
61 Our Varying Histories and Future Potential:   
Models and Maps in Science, the Humanities, and in Music Theory 
 
Eugene Narmour * 
 
University of Pennsylvania, Philadelphia, USA 
* = Corresponding author, enarmour@sas.upenn.edu 
 
Part 1 briefly recounts the influence of social unrest and the explosion of knowledge in both psychology and the humanities 
from 1970‐1990.  As the sciences rely on explicit top‐down theories connected to bottom‐up maps and models, whereas the 
humanities build on bottom‐up mappings within malleable top‐down “theories” (approaches, themes, theses, programs, 
methods, etc.), the changes in the sciences during this period contrasted sharply with those in the humanities.  Both domains 
witnessed a surge of new journals and, in the case of music cognition, the establishment of a new sub‐field.   In this regard, 
Music Perception, as originally envisioned by Diana Deutsch, was crucial.   Part 2 discusses in detail how these two social 
transformations affected the histories of music theory and cognitive music theory.   The former fractiously withdrew from its 
parent organization (AMS), whereas the latter was welcomed into SMPC.  Inasmuch as both music theory and cognitive music 
theory also construct maps and models, Part 3, the heart of the study, examines the metatheoretical importance of these terms 
for music cognition, music theory, and cognitive music theory.   Parts 1‐3 deal with the recent past and make little attempt to 
cover the present state of research from 1990‐2010.  Part 4, however, speculates about the future—how music cognition, 
cognitive music theory, and music theory contribute to the structure of musical knowledge.  The intellectual potential of this 
unique triadic collaboration is discussed:  psychology provides a commanding theoretical framework of the human mind, 
while music theory and cognitive music theory model the moment‐to‐moment temporal emotions and auditory intellections at 
the core of the musical art. 
 
 
 
62 Twenty­Six Years of Music Perception: Trends in the field  
 
Anna K. Tirovolas & Daniel J. Levitin 
 
McGill University, Montreal, Quebec, Canada 
* = Corresponding author, anna.tirovolas@mail.mcgill.ca 
 
In this review, we sought to document the longitudinal course of empirical studies (a meta‐trend analysis) published in the 
journal Music Perception, dating from the journal’s first issue published in 1983 to the present. The aim of this project was to 
systematically characterize the nature of empirical research published in one of the principal peer‐reviewed outlets for work 
in our field, and consider these data as a sample representing the overall course of music perception research across the last 
three decades. Specific areas examined within each paper were: research topic, the types of participants used (including levels 
of musical training), the nature of the stimuli presented, materials used to present stimuli, types of outcome measures and 
measurement approaches, as well as geographic and disciplinary (departmental) distribution of the authors.  In total, 384 
empirical papers in the journal were examined, as well as a full set of 574 articles, and relevant details extracted. 
                                                                           SMPC 2011 Program and abstracts, Page:       
                                                                                                                   39 
 
63 Past and present conceptions of music in the mind: An introduction to Ernst 
Kurth’s Musicpsychologie (1931) 
 
Daphne Tan * 
 
Eastman School of Music, Rochester, NY, USA, 
* = Corresponding author, daphne.tan@rochester.edu 
  
Ernst Kurth (1886–1946) is recognized as an influential figure in the history of music theory: his analytical monographs—
which describe the music of Bach, Wagner, and Bruckner in terms of kinetic and potential energy and dynamic waves of 
motion—continue to resonate with musicians. Though some recent studies acknowledge Kurth’s music‐cognitive 
contributions, his ideas remain relatively unknown to North American scholars engaged in empirical research. Indeed, Kurth’s 
views on music in the mind are rarely discussed. The present talk provides an introduction to Kurth’s psychological outlook as 
expressed in his last monograph, Musikpsychologie (1931). In particular, I consider recent claims that Kurth’s approach to 
music anticipates the work of modern cognitive research and moreover that Kurth “founded the field of music psychology” 
(Rothfarb 1988). This discussion highlights ideas presented in Musikpsychologie that have received some attention in modern 
empirical literature: musical tension, implicit learning, and categorical perception. It also suggests how Kurth’s ideas may 
encourage new experimental research, particularly pertaining to the elasticity of perceptual categories, contextual effects on 
pitch perception, and musical metaphors.  
 
 
 
 
 
 
64 Toward a Unified Theory of Music Cognition 
 
Eugene Narmour * 
 
University of Pennsylvania, Philadelphia, USA 
* = Corresponding author, enarmour@sas.upenn.edu 
 
We experience music holistically, as a unity, where psychophysical, perceptual, cognitive, and emotional processing all come 
together.  Yet our manifold musical analyses and heterogeneous collections of empirical data belie this, where music theorists 
juggle many different and incommensurate kinds of stylistic theories to analyze music, and where psychologists invoke many 
different kinds of disconnected experiments to explain how listeners cognize music.  To address this surfeit of theories and 
data, I identify some twenty‐five musical parameters and show that they are all scalable, whether we order their elements 
ordinally or by rank, categorically or nominally, by ratios, or intervallically . Such parametric scaling suggests a possible path 
toward theoretical unity.  I hypothesize that all these orderings entail degrees of similarity and difference along with degrees 
of closure (weak implication) and nonclosure (strong implication).  Motions on all these scales in every parameter create 
structures of process and reversal.  Through such scaled movement, we can identify partially isomorphic, analogous 
structuring between parameters.  We can also measure the multiple and frequent noncongruences that are responsible for 
producing strong musical affects.  Since both structure and affect contribute to memory, parametric interactions thus 
necessitate analyzing two simultaneous analytical tracks, one for congruent syntactic structuring and one for noncongruent 
affective structuring.  I construct an analytical symbology for tracking the interaction between multi‐parametric congruence 
and noncongruence in order to explain more precisely the aesthetic, temporal relationship between structure and affect.  The 
generated analyses are minimally reductive with respect to the score (already a reduction).  And they are produced under one 
unified theory, the IR‐model.   
 
                                                                                          SMPC 2011 Program and abstracts, Page:       
                                                                                                                                  40 
 
65 Visual Anticipation Aids in Synchronization Tasks 
 
Aaron Albin (1)*, Sang Won Lee (2), Parag Chordia (3) 
 
(1) Georgia Tech Center for Music Technology, Atlanta, USA, aalbin3@gatech.edu(2) Georgia Tech Center for Music Technology, Atlanta, USA (3) Georgia Tech 
Center for Music Technology, Atlanta, USA  
* = Corresponding author, aaronalbin@gmail.com 
 
Sensorimotor synchronization (SMS) to visual cues, typically flashing lights, has been shown to have higher mean asynchrony 
and lower upper‐rate limit (i.e. larger IOI period) compared with auditory stimuli. However, flashing lights may be difficult for 
SMS, and “spatial displacement” or biological motion might facilitate SMS (Repp, 2006). Here we introduce such dynamic 
visual cues and compare SMS in a variety of auditory, visual, and combined conditions. We conducted a fingertapping 
synchronization task to isochronous patterns (Radil et al., 1990) (Patel et al., 2005) (Repp & Penel, 2002)(Repp & Penel, 
2004).  IOI of the pattern was 0.5 sec which lies within rate limits of both visual and auditory stimuli (Repp, 2007). Fourteen 
subjects, all of whom had musical experience, participated. In addition to a flashing light, we looked at two anticipatory 
gestures, one with a constant velocity and another with an acceleration component in which a square falls down onto the 
middle of the participant's screen; these were conducted with and without audio.  Stimuli with constant velocity and 
acceleration anticipation outperformed audio alone or flash in terms of mean asynchrony. The acceleration component 
without audio had the least standard deviation and then combined with audio had the least absolute mean asynchrony.  The 
differences of mean asynchrony are statistically significant (p < .0001) using a multiple comparison test with the Tukey‐
Kramer statistic, with the exception of the Flash and Flash+Audio conditions.  The current work suggests that disparity in SMS 
performance in visual vs. auditory modalities may not be due to the modality itself but rather to the nature of the visual 
stimuli. For anticipatory visual conditions, the consistency of taps as measured by the standard deviation of tap times is 
significantly less than both audio alone and flashing visual cues. The implications for internal timing mechanisms reducing 
asynchrony in the dynamic visual condition are intriguing.  
 
 
 
66 Explaining Snowball's Dancing: a Simpler Alternative to the Vocal­learning 
Hypothesis 
 
Mark S. Riggle * 
 
Causal Aspects, LLC, Charlottesville VA, USA    
 * = Corresponding author, markriggle@alumni.rice.edu 
 
Some parrots dance to music thereby showing entrainment is not unique to humans. This observation lead to the 'vocal‐
learners hypothesis' (VLH) for entrainment capacity. For birds, since entrainment is not expressed in the wild (meaning 
entrainment has no fitness value), that therefore, entrainment could not be directly selected by evolution. For the origin of 
entrainment capacity, the VLH hypothesis states that selection for vocal‐learning would develop strong auditory‐motor 
coupling, and that this coupling supplies the capacity for entrainment. That is, entrainment is a direct side‐effect of selection 
for vocal‐learning. This same reasoning line applied to humans tightly constrains the evolutionary relationship of music and 
language. We show, with  supporting evidence, a viable alternate hypothesis for entrainment capacity, and furthermore, 
explain why only parrot species (and humans) naturally entrain. This hypothesis entails two conditions: 1) if entrainment 
creates an internal pleasure, and 2) if the brain's neural plasticity can adapt to the task, then entrainment can be a learned 
skill. Specifically, we postulate that a neural circuit exists that produces an internal reward, such as dopamine, when an 
auditory beat occurs synchronously with a periodic vestibular jolt. We show evidence from human behaviors consistent with 
both the circuit's existence and with entrainment as a learned skill.  Assuming a similar, but not homologous circuit in birds, 
we can show why parrots specifically may also learn to entrain.  Sufficient neural plasticity to adapt to the entrainment task 
may be indicated by the vocal‐learning ability, and additionally, during music, the bird's social movement mimicing of a human 
partner's movement may expose the bird to the entrainment pleasure.  The bird will then learn to entrain to music. We 
examine why should only birds and humans posses the neural circuit for entrainment pleasure. This alternate hypothesis 
implies a vastly different relationship of music and language. 
                                                                              SMPC 2011 Program and abstracts, Page:       
                                                                                                                      41 
 
67 Movement Rate Affects Tempo Judgments for Some Listeners 
 
Justin London* and Emily Cogsdill 
 
Carleton College, Northfield MN, USA 
* = Corresponding author: jlondon@carleton.edu 
 
Converging evidence from neuroscience and behavioral studies points to an intimate link between rhythm perception and 
production.  The current study examines the effect of motor behavior on the perceived speed an auditory rhythm.  We posit a 
positive correlation between movement rate and perceived tempo.  Participants tapped at two controlled rates in two blocks 
of trials, either every beat (relatively fast) or every other beat (relatively slow).  Stimuli consisted of (a) artificial melodies in 
2/4 meter, 8 bars long, and (b) percussive stimuli, created by replacing the pitch information with a repeated wood‐block 
sound.  Randomly ordered stimuli were presented at six tempos (72, 80, 89, 100, 120, and 133 bpm), four percussive and four 
melodic stimuli for each tempo. Participants made tempo ratings using a 9‐point scale.  For all participants a main effect of 
tapping mode was only found at the 89 bpm tempo, t(23) = ‐2.289; p = .016 (1‐tailed).  However, nine participants showed 
significant results at all tempo conditions (average p = .004), four had near significant results (average p = .058), while eleven 
were non‐significant (average p = .314).  Nonmusician participants gave significantly slower tempo ratings (p = .034) when 
tapping every other beat, but only at the 89 bpm tempo.  Thus movement rate affects tempo judgment, but only for some 
listeners; there is also an interaction between musical training, movement rate, and perceived tempo.  While more studies are 
needed to determine the differences between movement sensitive versus insensitive listeners, we tentatively propose that the 
perceived speed of an auditory rhythm in part depends on how fast you have to move to keep up with it. 
 
 
 
 
 
 
68 Melodic Motion as Simulated Action: Continuation Tapping With Triggered 
Tones 
 
Paolo Ammirante (1)*, William F. Thompson (1) 
 
(1) Macquarie University, Sydney, Australia  
* = Corresponding author, paolo.ammmirante@psy.mq.edu.au 
 
It is well‐known that melodies are perceived to have motional qualities. According to Common Coding theory, these perceptual 
qualities should be reflected in actions. In a series of continuation tapping experiments, non‐musicians used their index finger 
to tap a steady beat on a single key. Each tap triggered a sounded tone, and successive triggered tones were varied in pitch to 
form musical melodies. Although instructed to ignore the tones, participants produced systematic deviations in timing and 
movement velocity that mirrored the implied velocity of melodic motion. Where unidirectional pitch contour and large pitch 
distances between successive tones implied faster melodic motion, the inter‐tap interval (ITI) initiated by the just‐triggered 
tone was shorter and the velocity of the tap that followed (TV) was faster; where changes in pitch direction and smaller pitch 
distances implied slower melodic motion, longer ITI and slower TV followed. These findings suggest that, due to overlap 
between perceptual and motor representations, participants failed to disambiguate the velocity of melodic motion from finger 
movement velocity. Implications of these findings are discussed with respect to two topics: melodic accent and performance 
expression. 
 
                                                                          SMPC 2011 Program and abstracts, Page:       
                                                                                                                  42 
 
 
69 The Perception of Cadential Closure in Mozart’s Keyboard Sonatas  
 
David Sears (1)*, William E. Caplin (1), Stephen McAdams (1) 
 
(1) McGill University, Montreal Canada 
* = Corresponding author, david.sears@mail.mcgill.ca 
 
         Although studies in music perception and cognition provide ample evidence for the importance of cadential closure in 
the experience of tonal music for both trained and untrained listeners, there remains a glaring lack of research on how 
listeners differentiate amongst the cadential categories proposed by music theorists, as well as on how various musical 
parameters contribute to the perception of cadential strength. This study explores the underlying mechanisms responsible for 
the perception of both genuine cadences (perfect authentic, imperfect authentic, half) and failed cadences (deceptive, evaded) 
in Mozart’s keyboard sonatas. Twenty musicians and twenty non‐musicians heard fifty short excerpts (10s) that contained an 
equal number of perfect authentic (PAC), imperfect authentic (IAC), half (HC), deceptive (DC), and evaded cadences (EV). Each 
cadential category was also further subdivided according to issues of formal location (PAC, HC), melodic dissonance at the 
cadential arrival (IAC), and harmony at the cadential arrival (EV). For all excerpts, performance variables were neutralized so 
as to consider only compositional parameters of closure. Thus for these stimuli, cadential arrival provided the crucial 
independent variable distinguishing genuine cadences from non‐cadences. After listening to each excerpt, participants rated 
the strength of completion of each excerpt on a 7‐point analogical‐categorical scale. Results indicated that musicians and non‐
musicians did not differ in their ratings for genuine cadences but differed significantly in their ratings for failed cadences. 
Formal location (PAC), melodic dissonance at cadential arrival (IAC), and harmony at cadential arrival (EV) also significantly 
affected participant ratings in both groups. Finally, a regression analysis indicated the musical parameters that significantly 
contributed to participant ratings of completion for each cadential category. 
          
          
          
 
 
 
 
70 The Psychological Representation of Musical Intervals in a Twelve­Tone Context 
 
Jenine Brown (1)* 
 
(1) Eastman School of Music, Rochester, NY 
* = Corresponding author, jenine.lawson@rochester.edu 
 
This study investigates whether listeners implicitly attune to the repetitive adjacent interval patterns found in twelve‐tone 
rows.  Listeners (n=10) were freshmen and sophomore music majors at the Eastman School of Music.  The familiarization 
phase consisted of the 48 versions of a twelve‐tone row.  In this row, intervals 1 and 3 occurred most often, followed by 
intervals 2 and 5.  Interval 8, for example, never occurred within the row.  For each randomly ordered trial, listeners heard a 
probe‐melodic‐interval.  On a 1‐7 scale, listeners rated how idiomatic the melodic interval was in comparison to what they 
heard during the familiarization phase.  Listeners rated intervals from +/‐ 1 to 12 semitones.  Listeners rated within‐row 
intervals significantly higher than not‐in‐row intervals.  Moreover, listeners rated common within‐row intervals significantly 
higher than less common within‐row intervals.  Listener responses correlate with the “Interval Distribution,” which illustrates 
the occurrences of surface intervals within the familiarization phase.  Results demonstrate that listeners implicitly attune to 
repetitive intervals in a twelve‐tone musical language.  Moreover, they suggest that listeners create a hierarchy of more 
common and less common intervals, an initial step to hearing structure in twelve‐tone music.  The experiment was duplicated 
with a new set of participants (n=10), where listeners heard a row in the familiarization phase with a different intervallic 
structure.  In this row, intervals 2 and 5 occurred most often.  Listener responses did not correlate with the Interval 
Distribution, and within‐row intervals were not rated higher than not‐in‐row intervals.  These results suggest that implicit 
learning of intervals in a twelve‐tone context only occurs when within‐row intervals are perceptually salient. 
                                                                                              SMPC 2011 Program and abstracts, Page:       
                                                                                                                                      43 
 
71 Pitch Salience in Tonal Contexts and Asymmetry of Perceived Key Movement  
 
Richard Parncutt (1)*, Craig Sapp (2) 
 
(1) Centre for Systematic Musicology, University of Graz, Austria, (2) CCARH, Department of Music, Stanford University  
* = Corresponding author, parncutt@uni‐graz.at  
 
Thompson and Cuddy (1989) found that perceived key distance is greater for modulations to flat‐side keys (in chord 
progressions but not individual voices). Cuddy and Thompson (1992) explained the asymmetry with probe‐tone profiles. Flats 
relative to a key signature may be more salient simply because they lie at perfect fifth and/or major third intervals below scale 
steps (Terhardt). That could explain why, relative to key signatures, sharps are more common than flats. In 200 songs with 
piano accompaniment (Deutscher Liederschatz, 1859‐1872, Vol. 1, Ludwig Erk), in 196 songs in major keys, 1016 notes are 
sharpened and 459 flatted relative to the starting key; in 4 minor songs, 115 notes are sharpened and none are flatted. In 370 
Bach four‐part chorales, 185 are major with 1534 sharps and 465 flats; 139 are minor with 2628 sharps and 208 flats; 37 are 
Dorian (classified by Burns, 1995) with 656 sharps and 608 flats; and 9 are Mixolydian with 110 sharps and 18 flats. To test 
directly whether flats are more perceptually salient than sharps, we presented diatonic progressions of five chords to 
musicians and non‐musicians. All chords were major or minor triads of octave‐complex tones. The first was the tonic; the 
others were ii, IV, V and vi in major keys and ii, IV, v and VI in minor. The last four chords were presented in all 24 different 
orders. In half of all trials, the penultimate chord was changed from major to minor or vice‐versa. All listeners heard all trials 
in a unique random order and rated each progression's unusualness. Musicians were separately asked whether the last chord 
contained an accidental. We predict that a chord with a flat will sound more unusual and that accidentals will be identified 
more often if they are flats. 
 
 
 
 
 
 
72 Ancient music and modern ears: The perception and discrimination of Nicola 
Vicentino’s 31­tone tuning system 
 
Mikaela Miller (1)*, Jonathan Wild (1), Stephen McAdams (1) 
 
(1) CIRMMT, Schulich School of Music, McGill University, Montreal, Canada 
* = Corresponding author, mikaela.miller@mail.mcgill.ca 
 
Nicola Vicentino is recognized as a sixteenth‐century musical revolutionary; a wealth of scholarly work has been devoted to 
his 31‐tone tuning system and his listener‐oriented approach to the theory and practice of music. Attempts to analyze 
Vicentino’s compositions are limited in number, however, and empirical studies of the perception of his music are non‐
existent.  The current paper tests the hypothesis that trained musicians can hear the microtonal nuances of Vicentino’s music 
(as Vicentino claims), and that certain musical and acoustical parameters can affect listeners’ ability to perceive these 
nuances.  This hypothesis was tested with a pair of experiments in which highly trained musicians from the Montreal area had 
to discriminate between short excerpted passages from Vicentino's vocal compositions presented in the original 31‐tone equal 
temperament (31‐TET) and in a minimally recomposed 12‐tone equal temperament version.  Previous studies have estimated 
discrimination thresholds for absolute pitch and music interval magnitude generally below the differences found between the 
microtonally inflected pitches and melodic intervals of 31‐TET and the pitches and melodic intervals of 12‐TET.  The results 
from Experiment 1 show that listeners can reliably discriminate between the two systems in most cases, but that harmonic 
and voice‐leading contexts can greatly affect discrimination ability. Post‐hoc comparisons provided further evidence that 
excerpts with similar voice‐leading patterns elicit similar performance even though differences in pitch height vary.   In 
Experiment 2, the pitches of the 12‐TET versions were raised by a fifth of a tone to alter the differences in pitch height 
between the 12‐TET and 31‐TET versions of the excerpts. Shifting the differences in absolute pitch height significantly 
changed performance for some excerpts, but did not do so for others. This was attributed partially to the nature of the 
experimental design, and partially to the harmonic and voice‐leading contexts of the excerpts. 
 
                                                                           SMPC 2011 Program and abstracts, Page:       
                                                                                                                   44 
 
73 A Bayesian Theory of Musical Pleasure  
 
David Temperley  
 
Eastman School of Music, Rochester, NY, USA 
* = Corresponding author, dtemperley@esm.rochester.edu 
 
An important part of music perception is the identification of underlying structures from the musical surface—structures such 
as meter, harmony, key, motive, and phrase structure. When an analysis is found that is high in probability, this naturally 
results in a feeling of reward and pleasure for the perceiver.  From a Bayesian perspective, the probability of an analysis can be 
represented by its joint probability with the surface: for example, the probability of a metrical structure given a note pattern is 
represented by its probability in combination with the note pattern. If this quantity is high, and particularly if it rises fairly 
suddenly from one moment to the next, a sense of pleasure is predicted to result. I will argue that this explains well‐known 
pleasurable phenomena in music, such as a return to diatonic harmonies after a chromatic interruption, or the highly 
syncopated flourish in a tabla performance leading up to a structural downbeat. Of particular interest, under the current 
theory, is the phenomenon of reanalysis.  Let us suppose that, in some cases, the optimal analysis of an event is not found 
initially, but is only brought to the listener's attention by subsequent events. In this case, the probability of structure‐plus‐
surface for the event may actually be increased in retrospect. This offers an explanation for the effectiveness of some well‐
known expressive devices in music, such as augmented sixth chords. 
 
 
 
 
 
 
 
74 Key­Finding Algorithms for Popular Music 
 
David Temperley * & Trevor de Clercq 
 
Eastman School of Music, Rochester, NY, USA 
* = Corresponding author, dtemperley@esm.rochester.edu 
 
A new corpus of harmonic analyses of rock songs allows investigations into cognitive issues pertaining to popular music. The 
corpus (which has been reported elsewhere) contains 200 songs from Rolling Stone magazine's list of the “500 Greatest Songs 
of All Time”; both authors analyzed all 200 of the songs in Roman numeral notation. The current study focuses on the issue of 
key induction: what are the cues to tonality in rock? A series of probabilistic key‐finding algorithms was implemented and 
tested. In one algorithm, a distribution of relative roots was gathered from the corpus, showing the proportion of chords with 
roots on each scale‐degree (1, #1/b2, 2, and so on); we call this a “root profile.” Given a progression of absolute roots, a root 
profile can be used to find the key that generates the progression with highest probability; by Bayesian logic, this is the most 
probable key given the progression. Several variants of this approach were tried. In one variant, each chord was counted once; 
in another, each chord was weighted according to its duration; in another variant, metrical position was considered, giving 
higher probability to tonic chords on hypermetrically strong positions. The third algorithm yielded the best performance; this 
supports claims by some rock theorists that the metrical placement of harmonies in rock is an important cue to tonality. (We 
also tried several other algorithms based on pitch‐class distributions rather than root distributions; these were less 
successful.) Examination of some of the model's errors reveals some other factors that may influence key judgments in popular 
music.  
                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                    45 
 
75 Neurodynamics and Learning in Musical Tonality 
 
Edward Large* & Felix Almonte 
 
Center for Complex Systems & Brain Sciences, Florida Atlantic University 
* = Corresponding author, large@ccs.fau.edu 
 
Tonal relationships are foundational in music, providing the basis upon which musical structures, such as melodies, are 
constructed and perceived. As Zuckerkandl observed, “… musical tones point to one another, attract and are attracted” and 
these dynamic qualities “make melodies out of successions of tones and music of acoustical phenomena.” The goal of this work 
is to uncover the basic neurodynamic principles that underlie tonal cognition and perception. First, a new theory of musical 
tonality is proposed, which treats the central auditory pathway as a complex nonlinear dynamical system. It predicts that, as 
networks of auditory neurons resonate to musical stimuli, stability and attraction relationships develop among frequencies, 
and these dynamic forces correspond to feelings of stability and attraction among musical tones. Next, a rule for Hebbian 
synaptic modification is introduced, which can change these responses in some ways, but not in others.  Finally, a strong 
prediction about auditory neurophysiology is evaluated. In humans, auditory brainstem recordings, generated in the midbrain 
inferior colliculus (IC), reveal nonlinear population responses to combinations of pure tones and to musical intervals 
composed of complex tones. It is shown that a canonical model of phase‐locked neural oscillation predicts complex nonlinear 
population responses to musical intervals that have been observed in the brainstems of both musicians and nonmusicians. 
This observation provides strong support for the theory. Implications for the existence of a musical ‘universal grammar’ are 
discussed. 
 
 
 
76 Exploring Melodic Formulaic Structure Using a Convolutional Neural Network 
Architecture 
 
Panos Mavromtis (1)* 
 
(1) New York University, New York, USA 
* = Corresponding author, panos.mavromatis@nyu.edu 
 
Melodic formulaic systems define the dynamic (time‐dependent) modal environment of many world music idioms, and 
typically reflect the internalized knowledge of expert native carriers, as manifest in oral transmission and improvisation.  This 
paper explores the formulaic system of modern Greek church chant in a connectionist framework designed to emulate a 
Hidden Markov Model.  Replacing the discrete symbolic representation of the latter with a flexible distributed one offers 
significant improvement on computational efficiency.  A four‐layer convolutional feed‐forward neural network architecture is 
employed.  The first two layers map an n‐gram input pattern to a hidden state of inner layer activations; the next two layers 
map the hidden state to the prediction, namely the next output symbol.  The model is trained by back‐propagation.  The 
framework of deep learning is evoked to fine‐tune the network by preprocessing the input with a denoising autoencoder, 
trimming down unnecessary connections using the Optimal Brain Damage algorithm.  To facilitate interpretation, the space of 
hidden states is discretized using Gaussian mixture clustering.  By considering the much smaller set of state‐classes obtained 
in this way, one can separate the formulaic parts of the melody from the non‐formulaic ones, and can classify melodic formulas 
in a hierarchical tree. This classification reflects each formula’s internal structure, as well as its function within the phrase 
schema in which it occurs.  The analysis can be applied to any melodic idiom that is characterized by formulaic structure, such 
as Latin plainchant, British‐American folk song, as well as Middle‐Eastern and Indian classical or folk music.  The proposed 
formulaic analysis and classification is intended to recover structured representations of the cognitive schemata that 
characterize internalized expert knowledge of the musical idiom in question. 
 
                                                                                                SMPC 2011 Program and abstracts, Page:       
                                                                                                                                        46 
 
77 Repetition as a Factor in Aesthetic Preference 
 
Elizabeth Margulis (1)* 
 
(1) University of Arkansas, Fayetteville, AR, USA (2)  
* = Corresponding author, ehm@uark.edu 
 
Empirical work in a variety of domains has shown that multiple exposures of a stimulus can increase preference. Indeed, this 
effect has been shown for repeated exposures of a musical piece. But the effect of internal repetition – repetition of elements 
within the piece – has not been well studied. This experiment exposed 33 participants without musical training to short 
excerpts of solo music by contemporary composers Luciano Berio and Elliott Carter. Each excerpt could be heard in one of 
three conditions: unmodified, modified so that phrases were made to repeat immediately, and modified so that phrases were 
made to repeat later in the piece. Participants were asked to rate on a scale of 1 to 7 how much they enjoyed the excerpt, how 
interesting they found it, and how likely it was to have been written by a human artist rather than randomly generated by a 
composer. Participants found excerpts that had been modified to contain repeated phrases more enjoyable, more interesting, 
and more likely to have been written by a human artist than unmodified excerpts. Excerpts in the immediate repetition 
condition were found to be more enjoyable than excerpts in the delayed repetition condition, but excerpts in the delayed 
repetition condition were found to be more interesting than excerpts in the delayed repetition condition, illustrating an 
interesting dissociation between these two descriptors. These results are particularly striking because the original, 
unmodified excerpts all stem from the canon of twentieth‐century art music, and the modifications made to them were 
relatively primitive: audio segments were simply extracted and reinserted later in the sound file. Yet this simple action 
significantly improved the participants’ aesthetic assessments of the excerpts. These findings provide new support for the 
aesthetic role of within‐piece repetitions. 
 
78 Encoding and Decoding Sarcasm in Instrumental Music: A Comparative study   
 
Joseph Plazak  (1, 2) * 
 
(1) The Ohio State University, Columbus, Ohio, USA, (2) Illinois Wesleyan University, Bloomington, Illinois, USA 
* = Corresponding author, plazak.1@osu.edu 
 
Music is often regarded as being similar to language.  Further, it has been found to be capable of conveying affective 
information.  In order to investigate the musical cues employed by performers and listeners to encode/decode affective 
information, two empirical studies were performed.  Fourteen musicians recorded various musical passages with the intention 
of expressing one of four affects: joy, sadness, sarcasm, and sincerity.  Each recording was analyzed and four acoustic features 
were found to be consistently employed by performers to distinguish between the four target affects: 1) duration, 2) 
maximum pitch, 3) noisiness, and 4) number of voice breaks.  In a follow up study, the recordings were heard by 20 musicians 
who were tasked with identifying the performer's intended affect.  The results revealed that participants used a greater 
variety of acoustic cues to decode musical affect, including:  duration, pitch standard deviation, minimum pitch, maximum 
pitch, noisiness, mean harmonic to noise ratio, and various timbre cues.   Surprisingly, sarcasm was the best identified of the 
four recorded affects, followed closely by sadness and then joy.  The main acoustical features used to encode and decode 
musical sarcasm were a distinctive noisy timbre, a large amount of articulation variability, greater pitch variability, and 
somewhat shorter durations.  These findings are consistent with the literature on the acoustic cues of sarcastic speech, further 
suggesting a similarity between music and language processing.  
 
                                                                                               SMPC 2011 Program and abstracts, Page:       
                                                                                                                                       47 
 
 
 
79 The Serious Minor Mode: A Longitudinal­Affective Study 
 
David Huron* & Katelyn Horn 
 
The Ohio State University, Columbus, Ohio, USA 
* = Corresponding author, huron.1@osu.edu 
 
Hevner (1935) showed that Western‐enculturated listeners judge minor‐mode passages as sounding sadder than their major‐
mode counterparts.  Less well‐known is that Hevner's study also showed that listeners judge minor‐mode versions to be more 
"serious" sounding.  Using a sample of 8,117 excerpts, a series of score‐based empirical studies is presented tracing 
longitudinal changes in the use of the minor mode in Western art music between 1700 and 1950. Beginning around 1760, the 
minor mode became increasingly common, peaking in the 1890s after which the proportion of minor‐mode works steadily 
declined.  At the same time, the minor mode became increasingly associated with louder dynamics, faster tempos, and larger 
melodic intervals (Post & Huron, 2009; Ladinig & Huron, 2010).  In fact, minor‐mode works in the 19th century are, in general, 
louder and faster than major‐mode works from the same period ‐‐ a complete reversal of the pattern evident in other 
centuries. The empirical results reported here are consistent with longstanding historical‐stylistic interpretations of the rise of 
"Romanticism" beginning with the "Sturm und Drang" period of late Haydn, where passion, aggression, or seriousness became 
an increasingly popular musical expressions (Post & Huron, 2009).  At the same time, the acoustical features are shown to be 
consistent with an ethological model developed for interpreting animal signals (Morton, 1994).  Both sadness and aggression 
are associated with lower than normal pitch (a defining property of the minor scale). However, while *sadness* is associated 
with quieter, slower, more mumbled articulation, and smaller pitch variance, *seriousness* is associated with louder, faster, 
more ennunciated articulation, and larger pitch variance.  That is, the use of the minor mode for expressing "seriousness" 
appears to exhibit similar acoustic features for animal expressions of aggression. 
 
 
 
80 The Effect of Pitch Exposure on Sadness and Happiness Judgments: further 
evidence for “lower­than­normal” is sadder, and “higher­than­normal” is happier 
 
Parag Chordia (1)*, Avinash Sastry (2) 
 
(1) Georgia Institute of Technology, Atlanta, USA (2) (1) Georgia Institute of Technology, Atlanta, USA  
* = Corresponding author, PPC@GATECH.EDU 
 
It is widely known that certain prosodic features are characteristic of sad speech. Motivated by this, music researchers have 
suggested that the affective connotation of scales is in part due to departures from scale norms. For example, minor scales are 
sad because they are in some sense “lower” than the more common major scale. To test this hypothesis, we designed an 
experiment in which subjects were asked to rate a common set of melodies, after exposure to two different scales types, as 
described below.  A web‐based survey was conducted in which participants were asked to judge the happiness and sadness of 
various short musical excerpts (15‐20s). A total of 544 participants responded; their average age was 37 years and 88% were 
male. Stimuli were melodies that belonged to one of three scale types from North Indian classical music (NICM), which differed 
in the number of lowered scale degrees. What we term the LOW, MEDIUM, HIGH, scales contained 3, 2, and 1 flat scale degrees 
(Raag Todi, Raag Kafi, Raag Khamaj respectively). Each subject heard 10 exposure melodies and 5 test melodies. Subjects were 
split into two groups; the first group was exposed to the HIGH scale melodies, while the second group was exposed to the LOW 
scale melodies. All subjects were then give 5 test melodies from the MEDIUM scale.  Raw scores were converted to z‐scores on 
a per subject basis. Consistent with our hypothesis, HIGH exposure subjects, compared with LOW exposure subjects, judged 
the test melodies to be more sad (0.33 vs. ‐0.03, p < .001) and less happy (‐0.18 vs. 0.22 p < .001). All tests were corrected for 
multiple comparison using the Tukey‐Cramer statistic. These results are consistent with the hypothesis that judgements of 
sadness and happiness are related to departures from pitch norms, with “lower‐than‐normal” corresponding to sadness and 
“higher‐than‐normal” to happiness.  
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    48 
 
81 Reduced sensitivity to emotional prosody in a group of individuals with 
congenital amusia 
 
William Forde Thompson (1)*, Manuela Marin (2) & Lauren Stewart (2) 
 
(1) Department of Psychology, Macquarie University, Sydney, Australia, (2) Department of Psychology, Goldsmiths, University of London, London, UK 
* = Corresponding author, bill.thompson@mq.edu.au 
 
An important question surrounding congenital amusia is whether the impairment is restricted to music or whether it extends 
to domains such as speech prosody (tone of voice). Music and speech prosody are both auditory signals that acquire emotional 
meaning through changes in attributes such as pitch, timing, intensity, and timbre, and such acoustic attributes may be 
handled by similar mechanisms. If the capacity to process and interpret acoustic attributes is impaired when they occur in 
music, it might also be impaired when they occur in speech prosody, leading to reduced sensitivity to emotional prosody. 
Twelve British individuals with congenital amusia (diagnosed by the MBEA) and 12 matched controls judged the emotional 
connotation of 96 spoken phrases. Phrases were semantically neutral but prosodic cues (tone of voice) communicated each of 
six emotions: happiness, sadness, fear, irritation, tenderness, and no emotion. Two pitch threshold tasks were also 
administered to determine participants’ thresholds for detection of pitch change and discrimination of pitch direction. 
Classification of emotions conveyed by prosodic stimuli was significantly less accurate among the amusic group (M = 77.87) 
than among matched controls (M = 88.19), p < .01. The amusic group was significantly poorer than the normative group at 
classifying prosodic stimuli intended to convey happiness, sadness, tenderness, and irritation. The amusic group also exhibited 
reduced discrimination of pitch direction, but not pitch change. The results suggest that the impairment in congenital amusia 
occurs at a stage of processing that is relevant to both music and speech. Results are consistent with the hypothesis that music 
and speech prosody draw on overlapping cues for communicating emotion. Thus, when individuals are impaired at perceiving 
pitch‐related attributes in music, they exhibit reduced ability to decode emotional connotations from tone of voice. 
 
 
 
 
 
82 The Role of Metrical Structure in the Acquisition of Tonal Knowledge 
 
Matthew Rosenthal (1)*, Rikka Quam (1), Erin Hannon (1) 
 
(1) University of Nevada Las Vegas­ Las Vegas, Nevada­ United States 
* = Corresponding author‐ rosent17@unlv.nevada.edu 
 
Experienced listeners possess a working knowledge of pitch structure in Western music, such as scale, key, harmony, and 
tonality, which develops gradually throughout childhood. It is commonly assumed that tonal representations are acquired 
through exposure to the statistics of music, but few studies have attempted to investigate potential learning mechanisms 
directly. In Western tonal music, tonally stable pitches not only have a higher overall frequency‐of‐occurrence, but they may 
occur more frequently at strong than weak metrical positions, providing two potential avenues for tonal learning. Two 
experiments employed an artificial grammar‐learning paradigm to examine tonal learning mechanisms. During a 
familiarization phase, we exposed nonmusician adult listeners to a long (whole‐tone scale) sequence with certain 
distributional properties. In a subsequent test phase we examined listeners’ learning using grammaticality or probe tone 
judgments. In the grammaticality task, participants indicated which of two short test sequences conformed to the 
familiarization sequence.  In the probe tone task, participants provided fit ratings for individual probe tones following short 
“reminder” sequences. Experiment 1 examined learning from overall frequency‐of‐occurrence. Grammaticality judgments 
were significantly above chance (Exp. 1a), and probe tone ratings were predicted by frequency of occurrence (Exp 1b). In 
Experiment 2 we presented a familiarization sequence containing one sub‐set of pitches that occurred more frequently on 
strong than on weak metrical positions and another sub‐set that did the opposite. Overall frequency‐of‐occurrence was 
balanced for both sub‐sets. Grammaticality judgments were again above chance (Exp. 2a) and probe tone ratings were higher 
for pitches occurring on strong metrical positions (Exp. 2b).  These findings suggest that in principle, meter may provide a 
framework for learning about the tonal prominence of pitches in Western music.  
 
                                                                         SMPC 2011 Program and abstracts, Page:       
                                                                                                                 49 
 
 
83 Memory for musical sequences beyond pitch:   Grammatical and  associative 
processes 
 
Ric Ashley 
 
Northwestern University, Evanston, Illinois, USA 
*  = Corresponding author, ric.ashley@sbcglobal.net 
 
The human capacity for musical memory has interested researchers for over a century.  Memory for music can be detailed and 
concrete even without conscious effort to encode. We hypothesize that this is because of a symbiotic relationship between 
memory and musical structure and explore this hypothesis in the experiments reported here. The stimuli used in these 
experiments are drum patterns taken from a published multimedia corpus of audio recordings, video recordings, and 
transcriptions. Viewed as probablistic grammars, these patterns have a number of important features: they are signficantly 
asymmetric in their transition probabilities; they are sparse in their matrix structures; and they are referential, in that one 
sound (a particular instrument in the drum set) is centrally connected to all other sonic elements.  These grammatical features 
parallel salient aspects of tonal harmony, allowing us to test the roles of these different features with regard to musical 
memory. Participants (university students without significant musical training) hear drum sequences of taken from the real‐
world corpora, or versions of these manipulated for symmetry/asymmetry, hierarcy/ referentiality, and sparseness/density, 
in a standard staistical learning paradigm. They then hear pairs of examples, one of which they have heard before and one of 
which is a lure; they identify which of the two examples was previously heard. Data collection is ongoing. Results to date 
demonstrate that memory is enhanced for sequences with grammars that are asymmetrical, referential, and sparse;  
manipulation of any of these factors degrades memory for these sequences.  These features work independent of pitch, 
suggesting that they are primary cognitive elements of perception beyond the domain of musical pitch. Parallels with linguistic 
structures are considered, as well as the way in which musical grammars related to basic aspects of associative memory 
(chaining, buffering, hierarchic structure, and effects of context). 
 
 
 
84 More Than Meets the Ear: Memory for Melodies Includes the Meter 
 
Sarah C. Creel (1)* 
 
(1) UC San Diego, Cognitive Science, La Jolla, CA 92093­0515 
* = Corresponding author, creel@cogsci.ucsd.edu 
 
Recent work suggests that listeners have very detailed memory for individual pieces of music, and that this detail influences 
seemingly structural aspects of music perception such as meter. The current study examines whether such detailed musical 
knowledge may underpin comprehension of different musical styles. Across multiple experiments, listeners, unselected for 
musical training, heard six melodies each in two different musical “styles” which were distinct in instrumentation (saxophone 
and harp/French horn and accordion) and meter (34/68). Melodies were constructed so that, when played alone, they were 
ambiguous between 34 and 68 meters, but contexts (accompaniments) suggested either 34 or 68 meter. Across participants, 
each melody was heard in all possible combinations of timbre and meter. First, listeners heard each melody several times in 
style‐specific context while doing a cover task. Then, in a “probe” phase, listeners heard each melody without its instrumental 
context, followed by a probe drumbeat continuation in either 34 time or 68 time. Previous work (Creel, in press) suggests that 
listeners prefer the metrical probe matching their previous experience with that melody. The twists in the current study are 
that some melodies were similar to one another (in timbre), and that listeners were tested not only on familiar melodiesm but 
also new melodies with meter‐associated timbres. Across experiments, listeners selected exposure‐consistent metrical 
continuations for familiar melodies when the timbre matched the original presentation (p<.001), but not when timbre 
changed (p>.2). Listeners generalized metrical knowledge to new melodies only when both timbre and motif content matched. 
This suggests that listeners encode timbre, melody, and meter information in integrated fashion, and that multiple co‐varying 
musical properties may give rise to style‐specific expectations. 
 
                                                                          SMPC 2011 Program and abstracts, Page:       
                                                                                                                  50 
 
 
85 The Effect of Scale­Degree Qualia on Short­Term Memory for Pitch 
 
Panayotis Mavromatis (1)*, Morwaread Farbood (1) 
 
(1) New York University, New York, USA 
* = Corresponding author, panos.mavromatis@nyu.edu 
 
We present an experimental investigation of how the tonal interpretation of a pitch sequence affects its retention in short term 
memory.  When a pitch sequence is perceived in an unambiguous tonal context, each pitch is qualified with a mental 
representation for the underlying scale degree qualia.  We hypothesize that this tonal interpretation, whenever available and 
unambiguous, improves the pitch’s retention in short‐term memory.  Two experiments are presented, employing the 
experimental paradigm developed by Diana Deutsch, in which a target tone is presented as the first member of a pitch 
sequence followed by brief silence followed by a probe tone to be compared with the target. The probe was chosen to be the 
same, a semitone higher, or a semitone lower than the target.  Subjects were asked to determine whether the first pitch and 
the final pitch were the same or different. Accuracy of identification was considered to be a measure of short‐term retention of 
the original target tone.  In Experiment 1 (a pilot), pitch sequences fit into three contexts: triadic, diatonic, and atonal.   
Listeners were more accurate in the triadic context, followed by the diatonic context, and least of all, the atonal context.  For 
Experiment 2, the three conditions were (1) diatonic, clear tonal implications, (2) diatonic, tonally ambiguous, (3) non‐
diatonic.  Register and set cardinality were fixed, there was no pitch repetition, and pitches a semitone distant from the target 
were not allowed prior to the final probe tone.  Contour and transposition level were randomized and different scale degree 
qualia for the target pitch in Condition 1 were utilized. Results indicated that the stronger the tonal context, the greater the 
accuracy of pitch recall. Moreover, scale degree qualia appears to have an unconscious effect on pitch memory retention 
within a tonal context. 
 
 
 
 
86 The Verse­Chorus Question: 
How Quickly and Why Do We Know Verses From Choruses in Popular Music? 
 
Benjamin Anderson (1)*, Benjamin Duane (1), Richard Ashley (1) 
 
(1) Northwestern University, Evanston, IL, USA 
* = Corresponding author, ben‐anderson@northwestern.edu 
 
A number of recent studies have investigated what information listeners can glean from less than one second of music. From 
genre (Gjerdingen & Perrott, 2008), to emotion (Ashley, 2008), to song titles (Krumhansl, 2010), listeners gather considerable 
information from short sounds. This study extends this topic to the perception of musical form, using a brief exposure 
paradigm to investigate how quickly listeners can distinguish between verses and choruses in popular music.  We also 
investigate what information listeners might use to make this distinction. We recruited experienced participants who either 
knew the difference between verses and choruses or played in a rock band.  They heard 300 randomly selected excerpts from 
30 popular songs, ten from each song, with five different lengths taken from the verses and choruses. The lengths were 100, 
250, 500, 1000, and 2000ms. The stimuli were taken from unfamiliar tracks by familiar artists and from familiar tracks by 
familiar artists. Participants were instructed to push buttons based on whether an except came from a verse or a chorus. We 
hypothesized that participants would perform better on the longer samples and that there would be a performance benefit 
when participants recognized the song. On the basis of 8 participants to date, using a chi‐squared test, performance was 
significantly above chance (p<0.001 for all lengths) suggesting that participants can determine verses from choruses even at 
100ms. Using a z‐test, performance at 100ms was significantly worse than performance for 250 (p=0.01), 500(p<0.001), 1000 
(p<0.01), and 2000ms (p<0.001). What was surprising, however, was that performance was significantly better for unfamiliar 
examples (p<0.001). In the familiar condition, participants may have needed to consider the specific song rather than simply 
determining whether they heard a verse or a chorus. 
 
 
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    51 
 
 
87 The Relationship Between Music Aptitude and the Ability to Discriminate Tone 
Contours in the Cantonese Language 
 
Alice Asako Matsumoto* & Caroline Marcum  
 
Eastman School of Music, University of Rochester, Rochester, NY, USA 
* = Corresponding author, aliceasakomatsumoto@gmail.com 
 
Previous studies have indicated that there is a correlation between music aptitude and lexical tone discrimination (Copeland, 
2009; Dankovicová et al., 2007; Kolinsky et al., 2009).  This study builds upon that research by testing non‐tonal language 
speakers’ ability to identify lexical tones and comparing their results with music aptitude scores.  A Cantonese Tone Contour 
Discrimination Test was administered in a classroom setting to 47 participants from the Eastman School of Music in 
Rochester, NY.  Participants were undergraduates enrolled in either a remedial or accelerated music theory class, based on 
placement testing.  In the first task, listeners heard an isolated word and identified it as rising, falling, or level tone.  In the 
second, listeners heard a pair of sentences and were asked for a same‐different discrimination.  In half of the items, the 
sentences differed only by the contour of the final syllable.  Participants identified the contour of the final syllable of the 
second sentence by circling arrows on an answer sheet for the three types of lexical tone.   Music aptitudes were measured by 
the Advanced Measures of Music Audiation (AMMA) test (Gordon, 1989).  After the test, participants completed a 
questionnaire regarding background in language and music. Results showed a main effect for music achievement (theory class 
placement): means were significantly higher for students in the accelerated music theory class, compared with the remedial 
class, F(1,39) = 9.38, p < .004.  There was also a main effect for task, with performance on the isolated words higher than the 
sentence context, F(1,78) = 110.57, p < .001.  There was no significant correlation between the AMMA score and the ability to 
discriminate tone contours (Pearson correlation r = .036 n.s.).  This finding thus supports research in music education that 
aptitude and achievement are distinct aspects of musicianship (Gordon, 2007).  
 
 
88 Why does Congenital Amusia Only Affect Speech Processing in Minor Ways? 
Evidence from a Group of Chinese Amusics 
 
Fang Liu (1)*, Cunmei Jiang (2), William Forde Thompson (3), Yi Xu (4), Yufang Yang (5), Lauren Stewart (6) 
 
(1) Center for the Study of Language and Information, Stanford University, Stanford, USA, (2) Music College, Shanghai Normal University, Shanghai, China, (3) 
Department of Psychology, Macquarie University, Sydney, Australia, (4) Department of Speech, Hearing and Phonetic Sciences, University College London, London, 
UK, (5) Institute of Psychology, Chinese Academy of Sciences, Beijing, China, (6) Department of Psychology, Goldsmiths, University of London, London, UK 
* = Corresponding author, LIUFANG@UCHICAGO.EDU 
 
Congenital amusia is a neuro‐developmental disorder of pitch processing that causes severe problems with music perception 
and production. Recent research has indicated that this disorder also impacts upon speech processing in subtle ways for 
speakers of both tone and non‐tonal languages. This study further investigated why congenital amusia mainly manifests itself 
in the music domain, but rarely in the language domain. Thirteen Chinese amusics and thirteen matched controls whose native 
language was Mandarin Chinese participated in a set of tone and intonation perception tasks and two pitch threshold tasks. 
The tone perception tasks involved identification and discrimination of Chinese words that shared the same segments but 
differed in tone. The intonation perception tasks required participants to identify and discriminate statements and questions 
that differed in various acoustic characteristics (pitch, rhythm, and intensity) across the entire utterances. The pitch threshold 
tasks involved the use of adaptive‐tracking, forced choice procedures to determine participants’ thresholds for detection of 
pitch change and discrimination of pitch direction. Compared with controls, amusics showed impaired performance on word 
discrimination in both natural speech and their gliding tone analogs. They also performed worse than controls on 
discriminating gliding tone sequences derived from statements and questions, and showed elevated thresholds for both pitch‐
change detection and pitch‐direction discrimination. However, they performed as well as controls on word identification, and 
on statement‐question identification and discrimination in natural speech. Overall, amusia does not appear to affect Chinese 
amusics’ performance on tasks that involved multiple acoustic cues to communicative meaning. Only when the tasks contained 
mainly pitch differences between stimuli, which seldom occur in everyday speech, did amusics show impaired performance 
compared to controls. These findings provide insight into why amusics rarely report language problems in daily life, and help 
understanding of the non‐domain‐specificity of congenital amusia. 
 
 
                                                                                              SMPC 2011 Program and abstracts, Page:       
                                                                                                                                      52 
 
 
89 Music­Language Correlations and the “Scotch Snap” 
 
Nicholas Temperley (1)*, David Temperley (2) 
 
(1) School of Music, University of Illinois at Urbana­Champaign, USA, (2) Eastman School of Music, University of Rochester, USA 
* = Corresponding author: ntemp@illinois.edu 
 
The Scotch Snap (SS) is a rhythmic pattern featuring a sixteenth‐note on the beat followed by a dotted eighth‐note. It has often 
been said to be characteristic of Scottish music; informal observation suggests that it is common in English music as well. We 
present a musical corpus analysis showing that, indeed, the SS is common in both Scottish and English songs, but virtually 
nonexistent in German and Italian songs. We explore possible linguistic correlates for this phenomenon. Our reasoning is that 
languages in which stressed syllables are often short might tend to favor the SS pattern. The traditional distinction between 
long and short vowels correlates partly with the SS pattern across languages, but not completely. (German allows short 
stressed vowels, but the SS pattern is not common in German music.) We then examine the duration of stressed syllables in 
four modern speech corpora: one British English, one German, and two Italian. British English shows a much higher 
proportion of very short stressed syllables (less than 100 msec) than the other two languages. Four vowels account for a large 
proportion of very short stressed syllables in British English, and also constitute a large proportion of SS tokens in our English 
musical corpus. This suggests that the SS pattern may have arisen from attempts to match the natural rhythm of English 
speech. Taken together with other recent work, our study provides additional evidence for the influence of linguistic rhythm 
on musical rhythm. 
 
 
 
 
 
90 A comparison of speech vs. singing in foreign vocabulary development 
 
A. Good(1)*,  J. Sullivan (2), & F.A. Russo (1) 
 
 (1) Department of Psychology, Ryerson University; (2) Department of Psychology, Saint Francis Xavier University 
* = Corresponding author, agood@arts.ryerson.ca 
 
The current study extends to second language learning the popular notion that memory for text can be supported by song. In 
the context of a second language classroom, singing can be intrinsically motivating, attention focusing, and simply enjoyable 
for learners of all ages. For native text, the melodic and rhythmic context of song enhances recall of text (Wallace, 1994). 
However, there is limited evidence that these benefits extend to learning of foreign text. In this study, Spanish‐speaking 
Ecuadorian children learned a novel English passage for two weeks. Children in a sung condition learned the passage as a song 
and children in the spoken condition learned the passage as an oral poem. After the third learning session, children were 
asked to reproduce the passage in the method in which they were taught (sung or spoken) while reading the lyrics and were 
tested on their ability to correctly pronounce the foreign words. After the fourth session, children were tested on their ability 
to recall verbatim as many of the words as possible and they were asked to translate 10 target words (or terms) from the 
passage into Spanish. As predicted, children in the sung condition demonstrated superior verbatim recall. In addition, their 
pronunciation of vowel sounds and translation success were enhanced. These findings have important implications for second 
language instruction. The presentation will also consider mechanisms such as dual encoding and automatic rehearsal that may 
be responsible for the gains observed in learning second language through song. 
                                                                                         SMPC 2011 Program and abstracts, Page:       
                                                                                                                                 53 
 
91 Playing in a Dialect: a Comparison of English and American Vowels and 
Trombone Timbres 
 
Katie Cox (1, 2) 
 
(1) Eastman School of Music, Rochester NY, (2) Truman State University, Kirksville MO 
* = Corresponding author, kac751@gmail.com 
 
While the relationship between music and language has been extensively explored in the areas of rhythm, melody, harmony, 
and cognitive processing, the easily drawn parallel between vowels and timbre has been largely ignored. This project has two 
aims: first, to confirm that the reported distinction in timbre does exist between English and American trombonists; second, to 
correlate that distinction with relevant differences in the vowel content of each language group. Participants were asked to 
submit a recording of a prescribed standard etude; this etude contained pre‐determined target notes which were measured at 
formant 1 and 2. From this data, an inventory of timbres was constructed for each subject group. This inventory was compared 
to the vowel distribution (also measured at F1 and F2) for each language. For timbre, both F1 and F2 showed a trend to be 
lower among English subjects than Americans. This correlates with the tendency for low back vowels in British English to 
cluster in a lower position than in American English, implying that the sounds naturally occurring a player’s language may 
impact his preferred musical sound options. 
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    54 
 
92 Expectation and Emotion in a Live Concert Experiment 
 
Hauke Egermann (1)*, Marcus Pearce (2), Geraint Wiggins (2), Stephen McAdams (1) 
 
(1) CIRMMT, Schulich School of Music, McGill University, Montreal, Canada, (2) Centre for Cognition, Computation and Culture 
Goldsmiths, University of London, London, UK  
* = Corresponding author, hauke@egermann.net 
 
We investigated the often theorized role of musical expectations in inducing listener’s emotions and present results from a live 
flute concert experiment with 50 musically trained participants. Using the CIRMMT Audience Response System, we measured 
continuously subjective experience (using 50 wireless iPods) and peripheral psychophysiological changes. To confirm the 
existence of the link between expectation and emotion, we used a three‐fold approach. (1) Based on an information‐theoretic 
model, musical expectancies are predicted by analyzing the musical stimuli used (six pieces of solo flute music). (2) A 
continuous expectation rating scale was employed by half of the audience to measure the unexpectedness of the music heard. 
(3) Finally, emotions also were measured using a multi‐component approach: subjective feeling (rated continuously by the 
other half of audience members), expressive behavior and peripheral arousal (both measured on all 50 participants). We 
predicted and observed a relationship between high‐information‐content musical events, the violation of musical expectations 
(in corresponding ratings) and continuously measured emotional reactions.  Thus, musical structures leading to expectation 
reactions are also thought to be manifested in emotional reactions at different emotion component levels. These results 
emphasize the role of musical structures in emotion induction, leading to a further understanding of the frequently 
experienced emotional effects of music in everyday life. 
 
 
 
 
 
93 A Synthetic Approach to the Study of Musically­Induced Emotions 
 
Sylvain Le Groux (1)* & Paul F.M.J. Verschure (1,2) 
 
(1) Universitat Pompeu Fabra, Barcelona, Spain, (2) ICREA, Barcelona, Spain 
* = Corresponding author, sylvain.legroux@upf.edu 
 
Music appears to deeply affect emotional, cerebral and physiological states.Yet, the relationship between specific musical 
parameters and emotional responses is still not clear. While it is difficult to obtain reproducible and independent control of 
musical parameters from human performers, computer music systems can generate fully parameterized musical material. In 
this study, we use such a system, called the SMuSe, to generate a set of well‐controlled musical stimuli, and analyze the 
influence of parameters of musical structure, performance and timbre on emotional responses. Thirteen students (5 women, 
M: 25.8 , range: 22‐31) took part in the experiment. They were asked to rate three blocks of sound samples in terms of the 
emotion they felt on a 5 points SAM scale of valence, arousal and dominance. These blocks corresponded to changes in the 
structure parameter: 3 modes (Minor, Major, Random ) * 3 registers (Bass, Tenor, Soprano); performance level: 3 tempi 
(Lento, Moderato, Presto) * 3 dynamics (Piano, Mezzo Forte, Forte) * 3 articulations (Staccato, Regular, Legato); and timbre: 3 
Attack time (Short, Medium, Long) * 3 Brightness (Dull, Regular, Bright) * 3 Damping (Low, Medium, High). For each block, we 
followed a repeated measure design where the conditions were presented in random order. Repeated measure MANOVAs 
showed that minor and random modes were more negative, while soprano register was more arousing. Staccato articulations, 
presto tempi and forte dynamics felt more arousing but also more negative. Presto tempi and forte dynamics were perceived 
as more dominant. Bright sounds with short‐attack and low damping were more arousing. Longer attacks and brighter sounds 
felt more negative. Finally, bright and low damping sounds were perceived as more dominant. This study shows the potential 
of synthetic music systems for analyzing and inducing musical emotion. In the future, interactive music systems will be highly 
relevant for therapeutic applications but also for sound‐based diagnosis, interactive gaming, and physiologically‐based 
musical instruments. 
 
 
                                                                                                SMPC 2011 Program and abstracts, Page:       
                                                                                                                                        55 
 
94 Affective Analysis Using the Progressive Exposure Method: 
The second movement of Beethoven’s Pathétique sonata (Op. 13)  
 
Joshua Albrecht  (1)*, David Huron (1), Shannon Morrow (2) 
 
(1) Ohio State University, Columbus, OH, USA,  (2) Westminster College of the Arts at Rider University, Princeton, NJ, USA 
* = Corresponding author, cbmajor7@gmail.com 
 
Recently, a number of approaches have been used to measure a listener’s perception of musical affect using continuous self‐
report throughout a listening experience.  While this method retains some degree of ecological validity, there are some 
significant methodological difficulties with using this approach and the resulting data can be very difficult to interpret. In this 
study, our principal aim is to further extend the approach of measuring perceived musical affect throughout a work by using a 
progressive exposure method in which listeners hear short excerpts of music with some musical context surrounding each 
excerpt. This method allows the listener time to introspect on a clearly defined excerpt before responding. We report results 
from two exploratory studies. In the first free‐response study, 5 musician listeners provide descriptive terms for 5‐second 
recorded excerpts of Beethoven’s Pathétique. A content analysis on the terms yields 15 affective dimensions appropriate for 
this specific work. In the second study, each listener uses these dimensions to rate randomly‐ordered 5‐second excerpts for 
three of the 15 affective dimensions. The ratings for each excerpt, when reassembled in temporal order, provide a diachronic 
portrait of the perceived affect in the piece. Preliminary results indicate a high level of intra‐ and inter‐subject reliability, 
consistent with the idea that Western‐enculturated listeners evaluate musical affect in similar ways based on objective musical 
features. These musical features are measured for each excerpt and a multiple regression analysis is carried out to model how 
listeners encode musical affect. Several representative passages are then examined in detail, illustrating the role that affective 
analysis can play in providing insight to traditional music analysis. 
 
 
95 A new scale to identify individuals with strong emotional responses to music: 
Absorption in Music Scale (AIMS) 
 
F.A. Russo (1)* & G. M. Sandstrom (2) 
 
(1) Department of Psychology, Ryerson University, Toronto, Canada, (2) Department of Psychology, University of British Columbia 
* = Corresponding author, russo@psych.ryerson.ca 
 
A new scale to identify individuals with strong emotional responses to music: Absorption in Music Scale (AIMS) Studies of 
emotional responses to music have often focused on the musical characteristics used to convey emotion (e.g., Hevner, 1936; 
Grewe, Nagel, Kopiez & Altenmuller, 2007). Other studies have focused on the moderating effects of culture (e.g., Balkwill & 
Thompson, 1999) and preference (e.g., Blood & Zatorre, 2001; Menon & Levitin, 2005). Little work has looked at individual 
differences that might affect emotional responses (with the exception of music training, which has not been found to relate to 
the strength of emotional responses). The current study fills this gap, providing support for the notion that absorption may be 
an important moderator of emotional responses to music. We created the Absorption in Music Scale (AIMS), a 34‐item 
measure of individuals’ ability and willingness to allow music to draw them into an emotional experience. It was evaluated 
with a sample of 166 people, and exhibits good psychometric properties. The scale converges well with measures of similar 
constructs, and shows reliability over time. Importantly, in a test of criterion validity, emotional responses to music were 
correlated with the AIMS scale but not correlated with measures of empathy or music training. We foresee this scale being 
used to select participants for studies involving emotional judgments; researchers may wish to exclude individuals whose 
AIMS scores are unusually low. Alternately, or additionally, researchers may wish to use the AIMS score as a covariate, to 
account for some of the variability in emotional responses. Absorption in music has the potential to be a meaningful individual 
difference variable for predicting the strength of emotional responses to music. 
                                                                                        SMPC 2011 Program and abstracts, Page:       
                                                                                                                                56 
 
96 The Development of Interpretation During Practice and Public Performance: A 
case study 
 
Tânia Lisboa (1)*, Alexander P. Demos (2), Roger Chaffin (2), & Kristen T. Begosh (2), 
 
(1) Royal College of Music, London, UK, (2) University of Connecticut, Storrs CT USA 
* = Corresponding author, tlisboa@rcm.ac.uk  
 
How does technique and interpretation develop over the course of learning a new piece?  Over a two year period, we studied 
an experienced cellist learning the Prelude from J.S. Bach’s Suite No. 6 for solo cello. We measured bar‐to‐bar changes in sound‐
level and tempo for 19 practice and 6 live performances. Cross‐correlation was used to track the development of similarities 
between adjacent performances and the similarity of each performance to the final public performance, considered the best by 
the cellist.  Cross‐correlations for tempo increased monotonically over time; sound level showed a more complex pattern. 
Variations in tempo became progressively more consistent from one performance to the next as practice progressed. In 
addition, we examined tempo and sound level fluctuations during performance using linear mixed‐effects (growth curve) 
modeling to identify effects of technical and musical decisions that the cellist reported making during practice and the 
performance cues (PCs) that she reported using as mental landmarks during performance.  As expected, tempo followed an 
arch‐shaped function both across the entire piece and within phrases. Playing slowed at expressive and interpretive PCs and 
speeded up more at PCs for bowing and fingering and did so more in polished than in practice performances.  At  locations 
where there were memory hazards, playing slowed only in practice performances.  Sound level increased from practice to 
polished performances at places where the cellist reported PCs for fingering and hand position as well as at technical 
difficulties.  Over both polished and practice performances, sound level and tempo decreased at interpretive PCs, and sound 
level was more variable at expressive PCs. This suggests that  PCs provided points of control that allowed the cellist to 
introduce variation in the highly prepared motor sequences of her performance.     
 
 
 
 
97 Effects of motor learning on auditory memory for music 
 
Rachel M. Brown (1)*, Caroline Palmer (1)* 
 
(1) McGill University, Montreal, Canada 
* = Corresponding authors, rachel.brown2@mail.mcgill.ca, caroline.palmer@mcgill.ca 
 
Most people can remember melodies, but not all people can perform the melodies they know.  Motor theories of perception 
and recent evidence suggest that the ability to produce sensory outcomes can influence the perception of similar outcomes; 
other views suggest that motor learning is not crucial to perception.  Three experiments examined how skilled performers’ 
ability to recognize music is influenced by the type of auditory‐motor learning and individual differences in mental imagery 
abilities.  In each experiment, trained pianists learned short, novel melodies in four conditions: 1) auditory‐only (listening to 
recordings), 2) motor‐only (performing without auditory feedback), 3) strongly‐coupled auditory‐motor (performing with 
normal auditory feedback), 4) and weakly‐coupled auditory‐motor (performing along with recordings).  Pianists subsequently 
listened to recordings of melodies and indicated those they recognized and completed auditory and motor imagery tests.  In 
Experiment 1, pianists practiced each melody either three or six times.  In Experiment 2, pianists heard and performed along 
with acoustically‐varying melodies.  Results from both experiments indicated that auditory‐only learning yielded better 
recognition scores than motor‐only learning, and strongly‐coupled auditory‐motor learning yielded better recognition than 
auditory‐only learning.  Greater amounts of auditory or motor practice enhanced recognition.  Imagery abilities modulated 
learning effects: auditory imagery abilities correlated positively with recognition following motor‐only learning (Experiment 
1), suggesting that imagery skills compensated for missing feedback at learning.  Recognition scores correlated positively with 
slower tempi and decreased timing and intensity variation in the melodies with which pianists performed (weakly‐coupled 
learning condition; Experiment 2).  In a third experiment, motor feedback was enhanced at learning.  These findings 
demonstrate that motor learning can influence auditory memory beyond auditory learning alone, and that motor learning 
effects are modulated by the strength of auditory‐motor coupling, imagery abilities, and acoustic variation at learning. 
 
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    57 
 
98 Measuring Cognitive Thrift in the Improvisational Technique of Bill Evans 
 
Austin Gross * 
 
Independent scholar, Lancaster, Pennsylvania 
* = Corresponding author, austin.gross@rochester.edu 
 
In their study of Yugoslavian epic singers, Harvard scholars Milman Parry and Albert Lord noted that singers repeatedly used 
the same phrase structures again and again, but varied these models to fit the given idea of the story.  In their view, singers 
would create a way to express one idea, then alter the wording within this model to express a new idea.  Parry and Lord 
suggested that the reason for this was that a singer would not search for an entirely new phrase structure if another could be 
adapted, because this would be unnecessary.  They referred to these phrase structures as formulas, and suggested that the 
purpose behind this process of adaptation was thrift.  Of course, this process may or may not be conscious.  Psychologist Jeff 
Pressing, in his review of other work in cognitive science as well as his own work on improvisational behavior, noted that with 
increased practice controlled processing develops into automatic motor processing.  These automatic motor movements exist 
as specific motor programs that an improviser draws upon in performance.  This paper utilizes these two areas as a 
framework to interpret analyses of improvised solos by jazz pianist Bill Evans.  Other authors, such as Gregory Smith and 
Barry Kenny, have noted local formulas in Evans’s playing, but these are relatively fixed.  The present work locates flexible 
melodic models that occur in multiple solos by Evans, but which are elaborated differently.  These models fulfill the variability 
requirement of Parry and Lord’s “formula,” exist at a comparable level of structure, and provide musical evidence to support 
claims by Pressing and others about the cognitive aspects of improvised behavior.  Through these analyses, this work provides 
musical data to examine expert behavior in the area of musical improvisation.   
 
 
 
 
 
99 How do singers tune? 
 
Johanna Devaney (1,2)*, Jonathan Wild (1,2), Peter Schubert (2), Ichiro Fujinaga (1,2) 
 
(1) Center for Interdisciplinary Research in Music Media and Technology (CIRMMT), (2) Schulich School of Music, McGill University, Montreal Canada,  
* = devaney@music.mcgill.ca  
 
Singers’ intonation practices have been theorized since the Renaissance. There remain, however, a number of open questions 
about how consistently singers tune in performance and how their tuning relates to idealized tuning systems, such as Just 
Intonation, Pythagorean tuning, and equal temperament. The results from this experiment will be considered in the context of 
other findings on singing and instrumental intonation practices, as well as the literature on the just noticeable difference in 
pitch perception. This project consisted of two experiments. In the first, groups of six non‐professional and professional 
singers performed Schubert’s “Ave Maria” both a cappella and with accompaniment. In the second, three different SATB 
quartets performed a set of exercises and Praetorious’s “Es ist ein Ros’ entsprungen”. Interval size data was extracted from the 
recordings with a set of automated methods. The analysis of this data focused on the tuning of melodic semitones and whole 
tones and vertical m3, M3, P4, TT, P5, m6, M6, m7, and P8. The data collected from these recordings provides a wealth of 
detailed information about the singers’ self‐consistency and the amount of variation between singers. Overall the singers in 
the experiments tended towards equal temperament, although there was a wide range in interval size for the both the melodic 
and vertical intervals. In the solo experiment, there was an effect for training amongst the two groups, with the professionals 
tending to be closer to equal temperament. In the ensemble experiment, there emerged a slight, but significant, trend towards 
Just Intonation in cadential contexts for vertical intervals. 
                                                                                  SMPC 2011 Program and abstracts, Page:       
                                                                                                                          58 
 
100 "Debussy’s “Hommage à Haydn,” Ravel’s “Menuet sur le nom d’Haydn,” and the 
Probabilistic Key­Finding Model" 
 
Andrew Aziz*  
 
Eastman School of Music, University of Rochester; Rochester, NY, USA.  
* = Corresponding author, aaziz@u.rochester.edu 
 
Temperley (2007) proposed a Probabilistic Key‐Finding (PKF) Model, which, based on underlying probability distributions, 
can 1) compute the likelihood of a certain tonal event 2) place such an event in a tonal context by categorizing it in a particular 
key. While the program was conceived to analyze common‐practice tonal music, much of the algorithm's power lies in its 
flexibility; by merely changing parameters, the program may be transformed from one which analyzes the output of a tonal 
work to that of a "hybrid" composition. The current study considers the applications of the PFK Model to works by Debussy 
and Ravel. This type of "computational analysis," in fact, confirms that the works of these composers do not conform to 
traditional tonal models; this is based, in part, on the fact that the underlying pitch‐class distributions vary considerably. 
Results of the original PKF Model simulation reveal that the key schema for Debussy and Ravel are highly ambiguous; namely, 
there are few instances when the traditional major/minor dichotomy reveals a confident key "decision." In the case of 
Debussy, however, the analysis reveals that "minor" keys are considerably harder to categorize, and thus the paper considers 
an adaptation of the model. Due to the prevalence of the tri‐tone in post‐tonal contexts, the paper prescribes the acoustic scale 
(e.g. C‐D‐E‐F#‐G‐A‐Bb) as a substitution for the common‐practice "minor" distribution; in particular, there is an "axis" tri‐tone 
which proves to be the most significant in such acoustic contexts. The probability distributions are altered in a very specific 
way, and the regions revealed by the PBK model are considerably more confident (lower in ambiguity) than in the traditional 
setting. This shows that probabilistic methods can provide a useful, objective way of analyzing pitch organization in twentieth‐
century music. 
 
 
 
101 Pitch­continuity based Music Segmentation  
 
Yingjia Liu*, Sisi Sun, Parag Chordia 
 
Center for Music Technology, Georgia Institute of Technology, Atlanta, GA, USA 
* = Corresponding author: yliu385@gatech.edu  
 
We present a computational model of music segmentation in symbolic polyphonic music, based on pitch continuity. This 
approach is related to rule‐based and probabilistic methods that look for discontinuities, typically in pitch and rhythm. A 
human segmentation experiment was conducted to evaluate the algorithm. We find that the current algorithm is more 
strongly correlated with user responses when compared with several well‐known algorithms. In our experiment, 15 subjects 
were asked to indicate phrase boundaries in 5 pieces, which were synthesized from a symbolic score. The raw segmentation 
time data was converted into a density function (kernel density estimation) using non‐parametric estimation techniques, 
essentially indicating the segmentation likelihood at each time step. To model human segmentation, we first extract the 
melody using a simple “skyline” approach by using the highest pitches. For each note, we calculate the relation with its left 
neighbors (LR) and right neighbors (RR) using a 1st‐order pitch transition matrix computed on the song. The intuition is to 
find notes that are highly related to previous notes, but not to subsequent notes. Specifically, LR is the sum of transition 
probabilities from each note in the time window to the current note. The same method applied in RR. Here we use a time 
window eight times the average note duration for the song. The value H = LR/(RR*(LR+RR))  gives an estimate of the 
likelihood that a note is a phrase boundary. To evaluate the algorithm, the correlation (Pearson) between H and KDE is 
computed. Using this measure, the proposed algorithm outperforms local‐boundary, gestalt based and a probabilistic model, 
as implemented in the MIDI MATLAB toolbox.  For the five pieces in this test, the average correlation was 0.31 for the 
proposed algorithm, 0.20 for Gestalt, 0.13 for probability‐based and 0.12 for local boundary decision. Although there is 
substantial room for improvement, this work is consistent with the idea that phrase boundaries are perceived when local 
entropy increases.  
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    59 
 
102 The Dynamic Implications of Simple and Recursive Patterns in Music 
 
Andie Sigler (1, 2), Eliot Handelman (1) 
 
(1) Centre for Interdisciplinary Research in Music Media and Technology, Montreal, Canada, (2) School of Computer Science, McGill University, Montreal, Canada 
 * = Corresponding author, andrea.sigler@Mail.mcgill.ca 
 
A current research trend places listener expectations at the center of music perception, cognition, and emotion. Hypotheses as 
to the provenance of these expectations include statistical learning of note‐to‐note or interval‐to‐interval frequencies, and the 
learning of schemata at the local and phrasal levels. We propose a third origin for listener expectations: the dynamic 
implications of simple and recursive patterns inherent in music. We offer a mathematically elegant, stylistically neutral 
approach to the analysis of pattern (configurations affording rule inference) in melodic shape. Our theory is based on the 
perceptual salience of simple structures and their natural recursive combination into more complex structures that remain 
uniform and predictable. The notion of expectancy violation is realized when the patterns that have been set in motion in a 
particular melody are broken. We propose that the effect of the pattern‐breaking ‘surprise’ in music can be related to what 
might have happened had the pattern continued. Our approach generates a set of testable hypotheses about listener 
expectations generated by musical pattern and the effects of their realization or violation. For example, we hypothesize that 
longer and simpler patterns, as well as those coordinating several musical parameters, produce stronger expectations, and 
therefore their pattern breaks are more salient. 
 
 
 
 
 
 
103 Some new data, suggestions and implications regardign key finding as a 
cognitive task 
 
Art Samplaski * 
 
Independent scholar, Ithaca, NY, USA 
* = Cprresponding author, agsvtp @ hotmail.com 
 
Metrically‐weighted distributions of scale degrees (SDs) and counts of SD pairs from a large sample of diatonic fugue subjects 
and melody incipits from Scarlatti, Bach, and Mozart keyboard movements (N=487) were examined to investigate possibilities 
for reconciling Krumhansl’s (1990) tonal hierarchy (TH) and Brown & Butler’s (1989) intervallic rivalry (IR) competing 
models for tonic inference. Multiple compositional strategies apparently were employed, whose SD distributions variously 
accorded with TH, IR, or seemingly neither; when aggregated by mode, distributions accorded with TH. Semitone frequencies 
were consistent with IR; tritones almost never occurred. While both inflections of 6 and 7 in minor were considered diatonic, 
La and Te almost never occurred, i.e., these melodies use “harmonic” minor. Distributions in initial measures overweighted 
Do+Sol, and underweighted Re‐Fa. These suggest the TH/IR “disconnect” is a mirage caused by: 1) examining compositions 
differently (broadly, melody vs. melody+accompaniment), and 2) inadvertently conflating multiple populations. Several tactics 
appear to identify tonic immediately, e.g., Scarlatti often begins compositions by a solo tonic followed by another some octaves 
higher. Whether these reflect low‐level Gestalt processes or higher cultural‐specific knowledge is unclear. Potential problems 
for music cognition research due to possible conflation of several levels of auditory cognition are discussed briefly. 
                                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                                     60 
 
104 Impact of musical demands on measures of effort and tension during trumpet 
performance 
 
Jonathan Kruger (1,2), James McLean (2), & Mark Kruger (3)* 
 
(1) Rochester Institute of Technology, New York, USA, (2) SUNY­Geneseo, New York, USA, (3) Gustavus Adolphus College, Saint Peter, Minnesota, USA 
* = Corresponding author, mgk@gustavus.edu 
 
What is the relationship between the difficulty of a musical passage and measures of effort or tension in the body? Does the 
anticipated difficulty, dynamic, or tempo of a musical passage increase generalized activation in muscles not directly involved 
in performance?  How do professionals differ from students? Research on muscle tension in brass performance has focussed 
primarily on the embouchure and face (White and Basmajiian, 1973; Hauser and McNitt‐Gray, 1991; Lapatki, Stegeman, & 
Jonas, 2002). Henderson (1979) demonstrated that a trumpet player’s throat tension changes with pitch. Kruger, Mclean, and 
Kruger (2009) found ability predicts less overall unnecessary muscle tension, but whether musical demands increase general 
effort and tension is largely unknown. Effects of extraneous body and throat tension on air support are also unknown. Nine 
student and three professional trumpeters played a concert Bb scale, arpeggios differing in articulation and dynamics, and 
excerpts from the Haydn Trumpet Concerto. Extraneous shoulder and back tension was measured electromyographically. 
Measures were also made of expansion and contraction of the upper chest and abdomen, air pressure inside the performers 
mouth, and mouthpiece pressure on the embouchure. Tension in the throat was assessed by examining closure of the glottis in 
the throat with an electroglottograph. We expect our data to replicate our earlier finding that successful student performers 
demonstrated selective control of extraneous muscle activity and created the highest levels of air pressure at the embouchure. 
Selective control was most apparent in high ranges of the arpeggios and during difficult passages of the Haydn concerto. The 
current data set will allow us to compare students and professionals and to look at throat tension in addition to shoulder and 
back tension. We will also be able to look to see when tension interferes with performance. We are in the process of further 
analyzing this data. 
 
 
105 Musical Improvisation in Indian and Western Singers  
 
Meagan Curtis (1,3)*, Shantala Hegde (2), and Jamshed Bharucha (3) 
 
(1)Purchase College, SUNY, Purchase, NY, USA, (2)National Institute of Mental Health and Neuro Sciences, Bangalore, India,  (3)Tufts University, Medford, MA, USA 
* = Corresponding author, meagan.curtis@purchase.edu 
 
Musical acculturation has been observed to influence the perception of music, shifting musical expectations so as to match the 
tonal rules of one’s culture. We extend this research by examining the extent to which acculturation influences musical 
improvisation.  We examined the musical improvisations of 20 highly trained singers, 10 from India and 10 from the United 
States. We asked the singers to listen to recordings of 48 short, vocalized melodies and sing an improvised continuation of 
each melody. Half of the stimuli were tonally consistent with the Indian raag Bhairav and half were consistent with the 
Western major mode. Each stimulus was sung by a trained Indian singer and also by a trained Western singer, so as to control 
the cultural cues in the stimuli, such as cultural differences in stylization and vocal technique. We examined the pitch class 
distributions of the participants’ improvised vocalizations. Many of the participants in both cultures were able to adjust their 
vocalizations to match the tonality of each stimulus condition, but differences also emerged in the pitch class distributions for 
each culture. These similarities and differences will be discussed. 
 
 
                                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                                     61 
 
106 The Emotional Piano:  Exploring the Use of Pitch Height and Articulation Rate in 
Major and Minor Keys 
 
Matthew Poon*, Michael Schutz 
 
McMaster Institute for Music and the Mind, McMaster University, Hamilton, Canada 
* = Corresponding author, matthew.poon@rogers.com 
 
Music and speech are known to communicate emotions using a number of acoustic cues, specifically articulation rate (e.g. 
syllables‐per‐second in speech) and fundamental frequency.  Based on the premise that happiness is generally associated with 
major keys and sadness with minor keys (Hevner, 1935), this project examines composers’ use of articulation rate and pitch 
height in the first book of the Well‐Tempered Clavier by J.S. Bach (both the preludes and fugues); and Chopin’s Preludes (Op. 
28) to convey happiness and sadness in piano music.  Analyzing the first eight measures of each piece yielded 192 individual 
data points per‐corpus, for a total of 576 data points for each of the two acoustic cues of interest.  Our analysis revealed two 
points of interest.  First, the major‐key pieces were, on average, a major second higher than the minor‐key pieces.  Second, the 
articulation rate (as measured in notes‐per‐second) of the major‐key pieces was, on average 28 percent faster than the minor 
key pieces (Major=5.86, minor=4.56).  This shows that, within our corpus, music that is nominally considered to be happy is 
both “higher” and “faster” than music considered to be sad.  This finding is consistent with the acoustic cues observed in 
speech, where happiness is conveyed through the use of higher pitch height and faster rate of articulation. 
 
 
 
 
 
107 Singing with yourself: Assessing the influence of self­similarity and 
prototypicality in vocal imitation 
 
Peter Q. Pfordresher (1),* Alex Tilton (1), James T. Mantell (1), & Steven Brown (2) 
 
(1) University at Buffalo, State University of New York, USA (2) McMaster University, Hamilton, Ontario, Canada 
* = Corresponding author, pqp@buffalo.edu 
 
Replicating a melody by singing it is a formidable imitation task in that one must reproduce laryngeal gestures that are usually 
unobservable. A minority of individuals exhibit poor‐pitch singing, a general tendency to mistune while imitating a melody, 
and previous evidence suggests that these deficits may reflect the difficulty of planning actions based on a target outcome (a 
version of the inverse modeling problem). We report three experiments that explored two factors that may influence imitation 
accuracy: Self‐similarity and prototypicality. Participants heard and then vocally imitated 4‐note melodies based on imitative 
target recordings that were either recordings of themselves (from earlier in the session) or of another singer. Recordings of 
other‐singer targets were selected from a database and represented a wide range of accuracies, including accurate 
(prototypical with respect to musical structure) and poor‐pitch (non‐prototypical) singers. Results suggested an overall 
advantage for self‐targets as opposed to others (an effect of self‐similarity). The self‐advantage remained in an experiment 
that reduced self‐recognition presenting only pitch‐time information from the recording (i.e., timbre cues were eliminated). 
Among imitations of other targets, there was an advantage for imitating more prototypical (accurate) rather than less 
prototypical (inaccurate) targets. These effects were found for the imitations of participants who were good and poor‐pitch 
singers, though the self‐advantage was magnified for singers who were less accurate overall. These results argue for a self‐
similarity advantage, possibly due to enhanced perception‐action associations for one’s own vocal gestures, and a secondary 
advantage for prototypicality in vocal imitation. Furthermore, these results support the view that poor‐pitch singing is based 
on weak associations specific to translating perceptual contents to actions. 
                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                     62 
                                                                  
                            Titles and authors for poster session 1 (Friday) 
                            Arranged alphabetically by last name of first author 
 
A. 1 The Effect of Musical Expectation in Background Music on Short­Term Phonological Memory 
Elizabeth Aguila , Richard Randall  
 
A. 2 Tempo­based music treatment for inducing rhythmic entrainment, systemic pacing, and redirection of repetitive behaviors for 
children of the autism spectrum 
Dorita S Berger, PhD , Matthew Goodwin , Daniella Aube , Eliza Lane , Brittany L Croley  
 
A. 3 Effects of musical context on the perception of pitch differences between tones of same and different timbre. 
Elizabeth M. O. Borchert , Andrew J. Oxenham  
 
A.4 The Effect of Voice Numerosity on Perceived Musical Loneliness 
Yuri Broze , Kate Guarna , Brandon Paul  
 
A.5 The Right Place at the Right Time: An Analysis of Spectral and Temporal Pitch Mechanisms Using Event­Related 
Potentials and Source Localization 
Blake Butler , Laurel Trainor  
 
A.6 Eyeblinks as an Index of Attentional Chunking in Music Listening 
Matthew Campbell, Niall Klyn 
 
A.7 The neurochemical basis for musical regulation of emotion, cognition, and health: A review 
Mona Lisa Chanda , Daniel J. Levitin  
 
A.8 Semantic Priming Effects in the Cross­communication of Emotional Concepts by Music and Language 
Jacob Morgan , Tsee Leng Choy, & John Connolly 
 
A.9 Explicit and Implicit Knowledge of Rock Harmony in Nonmusicians 
 
A.10 Applying Principles of Music Scene Analysis to Simultaneous Auditory Warning Signals 
Matthew Davis  
 
A.11 A new model of perceived information content in melodic and non­melodic lines 
Ben Duane  
 
A.12 Neural processing of dissonance distance in melodies as revealed by magnetoencephalography (MEG) 
Roger Dumas, Arthur Leuthold, & Apostolos Georgopoulos  
 
A.13 Does Music Induce or Just Represent Emotions? The Role of Episodic Memories in Emotional Responses to Music 
Laura Edelman,  Patricia Helm,  Alan Bass,  Laura Brehm,  Melissa Katz, & Melissa Wolpow  
 
A.14  Musical Fan’ Stereotypes Activation and Mental Performance 
Marek Franek , Roman Mlejnek , Jan Petruzalek  
 
A.15 When Push Comes to Shove: Rhythmic Behavior in Improvised Walking Bass  
Ofer Gazit , Eric Battenberg , David Wessel  
 
A.16 Directional Asymmetry in Tonal Space and the Dramatic Use of bII: Theoretical Underpinnings, Empirical 
Evidence, and Musical Applications 
Bruno Gingras 
 
A.17 The Effect of Orchestration Changes on Continuous Responses of Emotional Intensity 
Meghan Goodchild , Jonathan Crellin , Jonathan Wild , Stephen McAdams  
 
A.18 What musical distinctiveness says about the organization of compositional memory 
Eliot Handelman  
                                                                 SMPC 2011 Program and abstracts, Page:       
                                                                                                         63 
 
A.19 An Acoustic Model for Chord Voicings in Post­Tonal Music 
Robert Hasegawa  
 
A.20 Brain correlates of happy and sad musical emotions using ragas of Hindustani classical music: An 
electrophysiological study 
Shantala Hegde , Shobini L Rao  
 
A.21 Influence of practiced instruments on the automatic encoding of polyphonic melodies.  
C. Marie, L. Herrington, & L. J. Trainor  
 
A.22 The Influence of Absolute Pitch to Three­Dimension Mental Rotation And Related Processing Characteristics  
Jian‐cheng Hou ,  Qi Dong , Qing‐hua He , Chun‐hui Chen ,  He Li , Chuan‐sheng Chen ,  Gui Xue    
 
A.23 The Influence of Absolute Pitch to Tone Language Working Memory and Related Processing Characteristics 
Jian‐cheng Hou ,  Qi Dong , Qing‐hua He , Chun‐hui Chen ,  He Li , Chuan‐sheng Chen ,  Gui Xue    
 
A.24 Vocal Range Normalization and its Role in the Perception of Emotion in Different Voice Types 
Randolph Johnson , Elizabeth Lagerstrom  
 
A.25 Investigating listeners’ preference and brain responses of multichannel­reproduced piano music 
Sungyoung Kim , Tomasz M. Rutkowski  
 
A.26 The Perception of Non­chord Tones vs. Unexpected Chord Tones in Tonal Melodies: Influence of Melodic Context 
on Implied Harmony Perception 
Jung Nyo Kim  
 
A.27 A Critical Examination of the Theory of Tonal Hierarchy and Arguments for a New Theoretical Framework for 
Explaining Tonality Perception 
Ji Chul Kim 
 
A.28 Pitch and Eyebrow Height; a Transcultural Phenomenon? 
Niall Klyn , Matthew Campbell  
 
A.29 Functional neuroimaging of musical emotions: a review and meta­analysis 
Thomas Kraynak 
 
A.30 Effects of Contour Change on Memory Encoding for Minuets: An ERP Study  
Shannon L. Layman , Ramiro R. Lopez , W. Jay Dowling  
 
A.31 Relationship between basic auditory abilities and performance on the MBEA 
J. Devin McAuley , Elizabeth Wieland  
 
A.32 Neural mimicry during perception of emotional song 
Lucy McGarry , Lisa Chan , and Frank Russo  
 
A.33 Investigating the Role of Musical Expertise in Phonetic Analogies of Guitar Timbre 
Audrey Morin , Nathalie Gosselin , Caroline Traube  
 
A.34 Harmonic Function from Voice­Leading: A Corpus Study 
Ian Quinn , Panayotis Mavromatis  
 
A.35 Composing by Selection: Can Nonmusicians Create Emotional Music?   
Lena Quinto , William Forde Thompson , Alex Chilvers  
 
A.36 Deconstructing Evolution's Sexual Selection Shows Music Could Arise Without Becoming Sex Dimorphic: Music is 
Not a Fitness Indicator 
Mark S. Riggle  
 
A.37 Musical Training, Working Memory, and Foreign Language Learning 
                                                                SMPC 2011 Program and abstracts, Page:       
                                                                                                        64 
Matthew Schulkind  and Laura Hyman  
 
A.38 Surveying the Temporal Structure of Sounds Used in Music Perception Research 
Michael Schutz , Jonathan Vaisberg  
  
A.39 Context Dependent Pitch Perception in Consonant and Dissonant Harmonic Intervals 
George Seror III , Jeremy Gold, W. Trammell Neill  
 
A.40 Primitive Hierarchical Processes and the Structure of a Single Note 
David Smey  
 
A.41 The Ineffability of Modern Art Music 
Cecilia Taher  
 
A.42 Music and the Phonological Loop 
Lindsey Thompson , Margie Yankeelov  
 
A.43 The Effect of Training on Melody Recognition 
Naresh N. Vempala , Frank A. Russo , Lucy McGarry  
 
                                                                     SMPC 2011 Program and abstracts, Page:       
                                                                                                             65 
                                                            
                        Titles and authors for poster session 2 (Saturday) 
                         Arranged alphabetically by last name of first author 
 
 
B.1 Does the change of a melody’s meter affect tonal pattern perception? 
Stefanie Acevedo , David Temperley , & Peter Q. Pfordresher  
 
B.2 The Melody of Emotions 
Michel Belyk , Steven Brown  
 
B.3 Expression in romantic piano music: Critera for choice of score events for emphasis  
Erica Bisesi , Richard Parncutt  
 
B.4 Melodies and Lyrics: Interference Due to Automatic Activation 
Jack Birchfield  
 
B.5 Musical Expertise and the Planning of Expression During Performance 
Laura Bishop, Freya Bailes, Roger T. Dean 
 
B.6 Perceptual grouping: The influence of auditory experience 
Keturah Bixby , Joyce McDonough , Betsy Marvin  
 
B.7 Song Style and the Acoustic Vowel Space of Singing 
Evan D. Bradley  
 
B.8 Orff­Schulwerk approach and flow indicators in Music Education context: A preliminary study in Portugal  
João Cristiano  & R. Cunha  
 
B.9 Movement during Performance: A Hunt for Musical Structure in Postural Sway 
Alexander P. Demos, Till Frank, Topher Logan 
 
B.10 Developing a Test of Young Children’s Rhythm and Metre Processing Skills 
Kathleen M. Einarson , Laurel J. Trainor  
 
B.11 Effects of musical training on speech understanding in noise 
Jeremy Federman , Todd Ricketts  
 
B.12 Differentiating people by their voices: Infants’ perception of voices from their own culture and a foreign species  
Rayna H. Friendly , Drew Rendall , Laurel J. Trainor (1,3) 
                                                                 
B.13 Signs of infants' participatory­ and musical behavior during infant­parent music classes 
Helga Rut Gudmundsdottir  
 
B.14 The Effect of Amplitude Envelope on an Audio­Visual Temporal Order Judgment Task 
Janet Kim, Michael Schutz 
 
B.15 Motion Capture Study of Gestural­Sonic Objects 
Mariusz Kozak , Kristian Nymoen , Rolf Inge Godøy  
 
B.16 Interactive Computer Simulation and Perceptual Training for Unconventional Emergent Form­bearing Qualities 
in Music by Ligeti, Carter, and Others 
Joshua B. Mailman 
 
B.17 Automatic Imitation of Pitch in Speech but not Song 
James Mantell, Peter Pfordresher, Brian Schafheimer 
 
B.18 Sequence Context Affects Memory Retrieval in Music Performance 
Brian Mathias , Maxwell F. Anderson , Caroline Palmer , Peter Q. Pfordresher  
                                                                     SMPC 2011 Program and abstracts, Page:       
                                                                                                             66 
 
B.19 Developing a window on infants’ structure extraction 
Jennifer K. Mendoza , LouAnn Gerken , Dare Baldwin  
 
B.20 The Effect of Visual Stimuli on Music Perception 
Jordan Moore , Christopher Bartlette  
 
B.21 An Experiment on Music Tempo Change in Duple and Triple Meter 
Yue Ouyang    
 
B.22 Listener­defined Rhythmic Timing Deviations in Drum Set Patterns 
Brandon Paul , Yuri Broze , Joe Plazak  
 
B.23 The Effects of Altered Auditory Feedback on Speech and Music Production. 
Tim A. Pruitt , Peter Pfordresher ,  
 
B.24 Does Note Spacing Play Any Role in Music Reading? 
Bruno H. Repp , Keturah Bixby , Evan Zhao  
 
B.25 Bayesian modelling of time interval perception 
Ken‐ichi Sawai , Yoshiyuki Sato , Kazuyuki Aihara (3,1) 
 
B.26 Linguistic Influences on Rhythmic Preference in the Music of Bartok 
Andrew Snow, Heather Chan 
 
B.27 Infants prefer singers of familiar songs 
Gaye Soley and Elizabeth Spelke 
 
B.28 Learning to sing a new song: Effects of native  English or  Chinese language on learning an unfamiliar tonal 
melody having  English or Chinese lyrics  
Leah C. Stevenson, Bing‐Yi Pan, Jonathan Lane, & Annabel J. Cohen 
 
B.29 Exploring Real­time Adjustments to Changes in Acoustic Conditions in Artistic Piano Performance  
Victoria Tzotzkova  
  
B.30 The role of continuous motion in audio­visual integration 
Jonathan Vaisberg, Michael Schutz 
 
B.31 The Effect of Rhythmic Distortion on Melody Recognition 
David Weigl , Catherine Guastavino , Daniel J. Levitin  
 
B.32 Perception of entrainment in apes (pan paniscus) 
 Philip Wingfield , Patricia Gray  
 
B.33 Transfer Effects in the Vocal Imitation of Speech and Song 
Matthew G. Wisniewski , James T. Mantell , Peter Q. Pfordresher  
 
B.34 The single voice in the choral voice: How the singers in a choir cooperate musically 
Sverker Zadig 
                                                                                                                      SMPC 2011 Program and abstracts, Page:       
                                                                                                                                                              67 
 
                                           Titles and abstracts for poster session 1 (Friday) 
                                                            Arranged alphabetically by last name of first author 
 
A. 1 The Effect of Musical Expectation in Background Music on Short­Term 
Phonological Memory 
 
Elizabeth Aguila (1)*, Richard Randall (1) 
 
(1) Carnegie Mellon University 1, Pittsburgh, U.S 
* = Corresponding author, elizabeth.aguila5@gmail.com 
 
This study examines the effect of musical expectation on short‐term phonological memory. Working from the premise that the 
processing of diatonic melodies with expectation violations requires greater cognitive resources than the processing of 
melodies with no violations, this experiment asked subjects to perform a simple memory task while listening to a melody in 
the background. If a melody violates an expectation, we hypothesized that the increased cognitive demand would draw 
resources away from the foreground memory task and negatively effect short‐term phonological memory. Specifically, we 
isolate the melodic expectations of pitch proximity and post‐skip reversal (Huron, 2006). Pitch proximity is when listeners 
expect a following pitch to be near a current pitch. Post‐skip reversal is when listeners expect a large interval to be followed by 
a change in direction. For each trial, subjects were shown a series of seven random numbers ranging from 1 to 7 for one 
second for each number. The numbers were presented with a background diatonic melody of 16 notes (major mode and 
beginning and ending on scale‐degree 1). The melodies used included a control melody, three types of broken pitch‐proximity 
melodies, two types of post‐skip reversal violation melodies, and two post‐skip reversal lure melodies. The background 
melodies played were randomized within the entire experiment. Following the presented number series, the subjects were 
asked to remember the numbers in the exact order they were presented. After all the tasks were completed, subjects were 
asked to complete a survey. There were a total of 28 subjects, including 13 males and 15 females. Although there was no 
significant effect of music condition on percent error of the memory task, there was a significant effect of serial position. Also, 
there were significant interactions between one melody with a broken pitch proximity expectation and the two post‐skip 
reversal lure melodies. 
 
 
A. 2 Tempo­based music treatment for inducing rhythmic entrainment, systemic pacing, and 
redirection of repetitive behaviors for children of the autism spectrum 
Dorita S Berger, PhD (1)*, Matthew Goodwin (2), Daniella Aube (3), Eliza Lane (3), Brittany L Croley (4) 
 
(1) Music Therapy Clinic, Norwalk, CT; (2), Consultant to Project; Groden Center, Providence, RI; (3), Research Assistants;  Groden Center, Providence, RI; (4) NYU Music Therapy,, New York, NY  
* = Corresponding author, dsberger@mags.net 
 
Many behaviors in children on the Autism spectrum resemble fight‐or‐flight avoidance responses as a result of habitual states 
of fear, possibly induced by sensory integration issues causing on‐going stress and deregulation of systemic pacing. Structured 
tempo‐based rhythm interventions at 60‐beats per minute, designed for entraining systemic regulation in autism can serve to 
induce systemic pacing, reduction or redirection of repetitive behaviors, yielding focus, calm, attention, and learning in 
persons on the Autism spectrum. An eight‐week pilot study investigated whether (and how) the role of tempo in discreet 
activity‐based music therapy treatment could influence habituation (entrainment) to regulated systemic inner rhythms, 
coordinating pacing, reducing stress, anxiety, and repetitive behaviors and yielding eye‐contact, attention, motor‐planning, 
and memory. Six subjects on the Spectrum, ages 8‐12 with minimal expressive language, received eight 45‐minute individual 
therapy sessions treated with four different rhythm interventions addressing breath control, regulation of arm movements, 
upper‐lower body coordination, and drumming. Each event was repeated four times within the sessions, to a rhythmic tempo 
pattern at 60‐beats per minute. A Lifeshirt heart monitor vest with embedded wireless sensors was worn by each subject in 
the first, fifth and eight session, to monitor and provide visible accounting of heart‐rate activity during those three sessions. 
Results appear to indicate various levels of pulse entrainment, and excellent progress and regulation in task undertaking and 
sequence retention by each of the six subjects, increases in motor planning abilities, visual contact, attention and reduction of 
repetitive behaviors were also indicated. Heart Rate data over the three sessions in which the vest was worn, display that a 
level of entrainment and regulation was taking place. Results tend to support the hypothesis that highly structured, tempo‐
specific rhythmic tasks at a slow tempo can bring about systemic pacing to redirect or reduce anxiety behaviors and yield 
functional adaptation. 
                                                                                           SMPC 2011 Program and abstracts, Page:       
                                                                                                                                   68 
 
A. 3 Effects of musical context on the perception of pitch differences between tones 
of same and different timbre. 
 
Elizabeth M. O. Borchert *, Andrew J. Oxenham  
 
University of Minnesota, Minneapolis USA  
* = Corresponding author,  olsen064@umn.edu 
 
Pitch plays an important role in complex auditory tasks such as listening to music and understanding speech, yet listeners’ 
ability to compare the pitch of two tones can be quite poor when the tones also differ in timbre. This study investigated the 
extent to which the difficulties encountered when judging isolated tones are mitigated in the presence of a musical context. We 
used measures of sensitivity (d’) and response times to determine the effects of various tone contexts on listeners’ ability to 
detect small changes in pitch between two tones that had either the same or different timbre (based on spectral shape). A 
series of three experiments were conducted, each of which included at least 18 subjects (musical training from 0 to 15 years). 
We found that a descending diatonic scale generally produced a small improvement in both sensitivity and response time 
when listeners compared tones of different timbre, but not tones of the same timbre. The improvement appeared to depend on 
the familiar tonality of the context, as a descending whole‐tone scale failed to yield similar improvements. Although the effect 
of a tonal context was significant in the first two experiments, it failed to reach significance in the third experiment. As the 
third experiment was also the longest, it may be that thresholds in the no‐context condition improve with sufficient training to 
the point that a tonal context no longer provides any additional benefit. Finally, we found that performance was not affected by 
whether the diatonic or whole‐tone context notes were always presented in the same (descending) order or whether they 
were presented in a variable and random order, suggesting that overlearned tonal hierarchies may play a more important role 
than the short‐term predictability of the context sequence. [Supported by NIH grant R01DC05216.] 
 
 
 
 
 
 
 
A.4 The Effect of Voice Numerosity on Perceived Musical Loneliness 
 
Yuri Broze (1)*, Kate Guarna (1), Brandon Paul (2) 
 
(1) Ohio State University School of Music, Columbus OH, USA, (2) Ohio State University Department of Speech and Hearing Science, Columbus OH, USA. 
* = Corresponding author, broze.3@osu.edu 
 
Several musical surface features—such as mode, tempo, and dynamic level—have been reliably linked to subjective reports of 
different perceived musical emotions. In the present study, we investigate possible emotional effects of voice numerosity—the 
instantaneous number of concurrently sounding musical voices in polyphonic music.  It is plausible that voice numerosity 
could be an important factor in the perception of those musical emotions which imply social contexts.  In particular, one might 
expect the perception of musical loneliness to be strongest for musical excerpts with fewer concurrent musical voices.  To test 
this hypothesis, we asked listeners to rate brief (~5s) musical excerpts for perceived happiness, sadness, pride, and loneliness 
using a slider interface.  Stimuli representing conditions of one, two, three, or four concurrent voices were drawn from J.S. 
Bach's The Well­Tempered Clavier, performed by a professional harpsichordist. Since fugal exposition is characterized by the 
stepwise accumulation of musical voices, it was possible to obtain sets of stimuli matched for mode, tempo, and motivic 
content.  Limitations of the harpsichord's plucking mechanism guaranteed consistent dynamic level and timbre.  While full 
results were unavailable at the time of submission, preliminary data suggest that voice numerosity can indeed impact the 
perception of certain musical emotions. 
                                                                          SMPC 2011 Program and abstracts, Page:       
                                                                                                                  69 
 
A.5 The Right Place at the Right Time: An Analysis of Spectral and Temporal Pitch 
Mechanisms Using Event­Related Potentials and Source Localization 
 
Blake Butler (1)*, Laurel Trainor (1) 
 
(1) McMaster University, Hamilton, Canada 
* = Corresponding author, BUTLERBE@MCMASTER.CA 
 
Sounds with pitch typically have energy at a fundamental frequency and integer multiples of that frequency.  Pitch is derived 
by the auditory system through complex spectrotemporal processing in which harmonics are combined into a single pitch 
percept.  Recent studies suggest pitch is represented beyond primary auditory cortex, in a region near its anterolateral edge.  
Although pitch research has focused on the spatial code originating in the cochlea, pitch information is also represented in a 
temporal rate code.  Here we compare event‐related responses (ERPs) derived from EEG to a pitch change using stimuli that 
target one or both of these mechanisms.  Each of twelve subjects participated in three listening conditions with pure tones, 
complex tones, or iterated rippled noise (IRN contains temporal pitch cues in the absence of spectral cues; Yost, 1996).  In each 
condition, a standard stimulus (perceived pitch of 167 Hz) was presented on 85% of trials, and an oddball stimulus (200 Hz) 
was presented on remaining trials.  Reponses were averaged for each participant and difference waves were created by 
subtracting the response to the control stimulus from the response to the oddball.  Repeated measures ANOVAs revealed a 
significant difference in MMN peak amplitude (p<0.001).  T‐tests revealed that peak amplitude was significantly greater in the 
IRN condition than the complex condition (p=0.007), which was in turn significantly greater than the pure tone condition 
(p=0.002).  Source localization using BESA is ongoing; however preliminary results show small but significant differences in 
the source location of the MMN response to changes in the different conditions.  These results suggest that MMN responses 
elicited by pitch changes are highly dependent on the type of cues present.  Complex tones, containing both spectral and 
temporal cues, appear to be analyzed in somewhat different cortical regions than IRN stimuli containing primarily temporal 
cues.  
 
 
A.6 Eyeblinks as an Index of Attentional Chunking in Music Listening 
 
Matthew Campbell*, Niall Klyn 
 
The Ohio State University, Columbus, Ohio, USA 
* = Corresponding Author, campbell.1120@osu.edu 

Physiological research measuring gross autonomic responses to music listening have focused almost exclusively on induced 
emotional states, as opposed to cognitive processing, as indicated by variations in heart rate, GSR, respiration, and GSM.  
Following the isolation of the endogenous (as opposed to reflexive or voluntary) eyeblink, blinking rate has been shown to 
index a variety of internal states and processes including arousal, emotion, cognitive load , and deception.   In addition to 
visual tasks, purely auditory conditions have also been shown to produce predictable variations in blink rate and fixation 
duration.  Most recently, in Nakano et al.’s 2009, “Synchronization of spontaneous eyeblinks while viewing video stories,” 
within and between subject blinking synchrony was found to strongly correlate with “low information” points when viewing 
visual narratives, indicating a shared mechanism for the implicit detection of narrative “lulls” and/or collective chunking to 
manage cognitive load.   The present study explores the possibility of endogenous blinking as a cross‐modal indication of 
attentional chunking in music listening, using Nakano et al.’s 2010 study “Eyeblink entrainment at breakpoints of speech” as a 
model.  In it, researchers presented each subject with video clips containing a single speaker delivering a monologue in three 
conditions: audio + video, video only, and audio only, finding significant subject/speaker entrainment and between subject 
synchrony in the AV condition but little entrainment (and no mention of synchrony) in the audio‐only condition.    This study 
prepares three similar clips depicting a clearly visible and audible singer performing unaccompanied and uninterrupted.  Due 
to music’s more inherently intermodal character and differing load requirements, we hypothesize strong between subject 
blink synchrony at similar time and prosodic intervals in both the AV and AO conditions. 
                                                                                              SMPC 2011 Program and abstracts, Page:       
                                                                                                                                      70 
 
A.7 The neurochemical basis for musical regulation of emotion, cognition, and 
health: A review 
 
Mona Lisa Chanda (1)*, Daniel J. Levitin (1, 2) 
 
(1) Laboratory for Music Perception, Cognition and Expertise, Department of Psychology, McGill University, (2) Center of Interdisciplinary Research in Music 
Media and Technology (CIRMMT) 
* = Corresponding author, mona.chanda@mail.mcgill.ca 
 
Music has strong emotional and social significance for many people however, the physiological basis for its powerful impact is 
not well understood. Many people use music as a means of regulating mood and arousal, much as they use caffeine or alcohol. 
Music is used to evoke a wide range of emotions (i.e. joy, excitement, relaxation, etc.), enhance concentration and cognition, to 
improve attention and vigilance, and to motivate. There is also an emerging body of evidence that musical intervention in 
clinical settings has beneficial effects on psychological and physical health and well‐being – lending credence to the adage that 
“music is medicine”.  These diverse functions of music are generally believed to reflect its widespread effects on 
neurotransmitter systems regulating motivation, emotion, social affiliation, attention and cognition. However, direct scientific 
inquiry into the neurochemical processes underlying music perception warrant further investigation. The present work 
examines the current state of knowledge regarding the neurochemical effects of music in the hope of stimulating further 
interest in this promising field. We review studies investigating the contribution of different neurotransmitter systems (e.g. 
dopamine, adrenaline, norepinephrine, serotonin, oxytocin) to musical perception, and the neurochemical basis for the 
beneficial effects of music on health‐related outcomes, including endocrine markers of stress and immune system functioning. 
We develop a conceptual framework and make recommendations for future scientific inquiry. We hope to: (a) advance our 
understanding of the physiological mechanisms underlying the strong emotional and social importance of music for healthy 
individuals (b) provide an empirical basis for the beneficial effects of music therapy and (c) make informed recommendations 
for clinical practice using music for a range of therapeutic outcomes, both physical and psychological. 
 
 
 
A.8 Semantic Priming Effects in the Cross­communication of Emotional Concepts by 
Music and Language 
 
Jacob Morgan *, Tsee Leng Choy, & John Connolly 
 
McMaster University, Hamilton, Ontario, Canada 
* = Corresponding author, morgaj5@mcmaster.ca 
 
This study investigated the comprehension of emotional concepts as conveyed by both music and language, respectively, in 
tandem. Previous work has demonstrated that music and visual language can cross‐communicate semantic meaning, such that 
musical excerpts determined to convey a given concept can prime a word which denotes that concept, and vice versa. There is 
a strong indication, without a complete understanding as to why or how, that music enjoys a privilege to human emotion. The 
current study has found – for the first time – that excerpts of music influence how the meaning of subsequent auditory words 
that denoted emotional concepts was processed. A brief musical excerpt was followed in quick succession by a spoken word, 
either “happy” or “sad”. Participants were asked to indicate with a button press whether the emotion conveyed by the music 
and the emotion described by the word were a match or a mismatch. Faster response times were observed in the match 
condition as compared to the mismatch condition, which indicate a strong priming effect, meaning that the emotion conveyed 
by the music facilitated the processing of the word. Event‐related brain potential (ERP) indices are especially well‐suited to 
measure semantic priming effects because they reveal neurocognitive processes online with exquisite temporal resolution. It 
is therefore possible to track aspects of cognition, such as comprehension, as they unfold in time. The ERP profile corroborated 
the behavioural results, showing disruptions of recognition and comprehension in the mismatch condition that were not 
exhibited in the match condition. These results show that the cognition of emotional concepts can be similarly communicated 
by both music and speech. Both music and language are parallel if not overlapping conduits for the cognition and meta‐
cognition of emotional concepts. These findings may be extrapolated to clinical applications, particularly in appraising 
comprehension and awareness in non‐communicative patients. 
                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                    71 
 
A.9 Explicit and Implicit Knowledge of Rock Harmony in Nonmusicians 
 
*Lincoln G. Craton, Wyatt Donnelly‐Landolt, Laura Domanico, Erik Muhlenhaupt, and Christopher R. Poirier 
 
Stonehill College, Easton, MA, USA 
* = Corresponding author, lcraton@stonehill.edu 
 
The way a piece of music plays with our expectations is at the heart of our aesthetic response to it (Huron, 2006; Meyer, 1956).  
To our knowledge, the present study is the first to explore listeners’ explicit (Experiments 1 and 2) and implicit (Experiments 
3 and 4) expectancies for Rock harmony.  In Experiment 1, 20 untrained listeners provided liking and surprise ratings for 31 
(major, minor, or dominant 7) target chords played after a brief musical context (major scale or major pentatonic scale + tonic 
major triad) or a non‐musical context (white noise + silence).  Recent music‐theoretical accounts (e.g., Stephenson, 2002) and 
a statistical corpus analysis (Temperley & de Clercq, 2010) indicate that Rock music (construed broadly to mean most recent 
popular music) utilizes all the chords of Common Practice, plus many that violate Common Practice.  Thus, the manipulation of 
greatest interest was harmonic system: 1) traditional diatonic chords, expected in both Common Practice and Rock music; 2) 
rock­only diatonic chords, unexpected in Common Practice music but expected in Rock; and 3) nondiatonic chords lying 
outside either harmonic system.For major chords only, liking ratings in both the rock­only diatonic condition (M = 6.20, SD = 
1.45) and in the traditional diatonic condition (M = 6.18, SD = 1.66) were higher than those in the nondiatonic condition (M = 
4.91, SD = 1.08), t (19) = 5.32, p < .001, and t (19) = 4.19, p = .001, respectively.  Untrained listeners thus possess some explicit 
harmonic knowledge of both Common Practice and Rock harmony. Experiment 2 was identical except for musical context 
(minor pentatonic or mixolydian scale + tonic major triad).  Experiments 3 and 4 used the harmonic priming paradigm 
(Bigand & Poulin‐Charronat, 2006) to measure implicit harmonic knowledge.  
 
 
 
 
 
 
A.10 Applying Principles of Music Scene Analysis to Simultaneous Auditory Warning 
Signals 
 
Matthew Davis (1)* 
 
(1) The Ohio State University, Columbus USA 
* = Corresponding author, davis.3131@osu.edu 
 
In emergency situations aircraft pilots are often presented with the difficult task of distinguishing between 
simultaneous auditory warning signals. An inability to effectively discriminate between synchronous warnings can lead the 
pilot to ignore certain signals, to misinterpret them, or to be simply unaware of their presence. The creation of signals easily 
distinguishable from other simultaneous signals would not only be desirable but would contribute to increased situational 
awareness and better judgment during emergencies. While much research has been conducted examining the appropriate 
properties of warning signals, it would be prudent to examine the contributions of auditory streaming research in relation to 
music. The ability of listeners to correctly identify synchronous melodic streams has been well documented and has been 
taken advantage of by composers for hundreds of years. For instance, many pieces of music contain over five melodically 
independent synchronous streams all while being harmonically related and integrated into the overall character of the piece. 
This can be accomplished by manipulating the onset synchrony, timbre, location, rhythmic identities, amplitude 
modulation, temporal continuity, and pitch proximity, to name a few. By applying these principles to warning signals, 
this study has sought to create a system of auditory warnings that contain more efficient differentiating properties in addition 
to conforming to a more unified stylistic entity. 
 
 
                                                                                SMPC 2011 Program and abstracts, Page:       
                                                                                                                        72 
 
 
A.11 A new model of perceived information content in melodic and non­melodic 
lines 
 
Ben Duane (1)* 
 
(1) Northwestern University, Evanston, IL, USA, ben­duane@u.northwestern.edu 
* = Corresponding author, benjaminduane2012@u.northwestern.edu 
 
Little is known about how listeners distinguish between melodies, counter‐melodies, accompaniments, and other musical 
lines. While a few researchers (Madsen & Widmer 2006, Duane 2010) have found evidence that information content is a 
factor—that melodies sound like melodies, for instance, because they contain relatively high information—this work is in its 
infancy and falls short on several counts. First, it concerns itself only with single notes (or pairs of adjacent notes), even 
though listeners can likely process information in a more global manner. Second, it accounts only for one’s knowledge of the 
piece being heard, not any larger knowledge of musical style. Third, these researchers fail to advance a plausible hypothesis 
about how musical information is perceived, processed, and remembered. This paper attempts to remedy these shortcomings 
by proposing and evaluating a new computational model of musical information tracking. For each note in a piece, the model 
computes various types of information content—some based on the current piece, others based on a corpus designed to reflect 
stylistic knowledge. The former are based only on the music preceding the note under analysis, simulating the listener’s 
evolving knowledge of the piece as it unfolds. The model can also compute information at several probabilistic orders, 
allowing notes to be analyzed in the context of one or more preceding notes. A running sum, weighted by an inverse 
exponential function, simulates the accrual and decay of information in a listener’s memory. This model was tested on several 
string quartets, the lines of which were labeled as either principal (e.g. melodic), secondary (e.g. counter‐melodic), or 
accompanying. A discriminant analysis demonstrated that the model’s output could be used to distinguish between principal, 
secondary, and accompanying lines with above‐chance accuracy. These results are consistent with the hypothesis that listeners 
base such distinctions on musical information content. 
 
 
 
A.12 Neural processing of dissonance distance in melodies as revealed by 
magnetoencephalography (MEG) 
 
Roger Dumas* 1,2,3, Arthur Leuthold 1,2, Apostolos Georgopoulos 1,2,3 
 
[1] Brain Sciences Center, VAMC, Minneapolis, MN, USA 
[2] Neuroscience, U of Minnesota, Minneapolis, MN, USA 
[3] Center for Cognitive Sciences, U of Minnesota, Minneapolis, MN, USA*  
* = Corresponding author, dumas@umn.edu 
 
We used magnetoencephalography (MEG) to investigate the brain mechanisms underlying the processing of dissonance in 
tone sequences. Ten human subjects listened to an audio file of 10 tone sequences of 60‐s duration each. Each sequence 
comprised a random permutation of 240 pure tones (250 ms/tone) from a set of pitches in the key of C major (2‐octave range: 
freq. 261.6 Hz‐987.86 Hz). These sequences were designed to have serial correlations from 0.0 to 0.9 over 5 lags to produce a 
wide variation in differences between successive tones. MEG activity was recorded using 246 sensors (axial gradiometers, 
Magnes 3600WH, 4‐D Neuroimaging) at 1017 Hz.  We evaluated the brain representation of dissonance calculated in 5 tonal 
spaces : a) the absolute value of the log of the  sound frequency/ratio (SF), b) Longuet‐Higgins (LH) space, c) a 4‐dimensional 
composite space developed by Roger Dumas (P4 helix, P4H), d) Krumhansl and Kessler's probe tone profile (KK) and e) 
Parncutt's weighted measure of chroma salience (P88). With respect to the neural data, successive absolute differences in the 
MEG signal (Nd) were calculated between note means.  With respect to the notes, Euclidean distances (Ed) were calculated 
between successive notes. We wanted to assess the relations between changes in neural activity to tone changes in each one of 
the five different tonal spaces.  For that purpose, we carried out an autoregressive analysis (to account for possibly correlated 
errors), where the dependent variable was Nd and the independent variable was Ed in 5 separate regressions. The results 
yielded five brain maps, corresponding to the independent variables. We found dense clusters of statistically significant 
relations for all five variables. Clusters in bilateral superior temporal gyrus were positive, whereas clusters in posterior 
parietal areas showed a positive/negative asymmetry. 
                                                                                              SMPC 2011 Program and abstracts, Page:       
                                                                                                                                      73 
 
A.13 Does Music Induce or Just Represent Emotions? The Role of Episodic Memories 
in Emotional Responses to Music 
 
Laura Edelman*,  Patricia Helm,  Alan Bass,  Laura Brehm,  Melissa Katz, & Melissa Wolpow  
 
Muhlenberg College, Allentown, PA., USA 
* = Corresponding author, ledelman@muhlenberg.edu 
 
Juslin (2009) distinguishes between music’s representation and induction of emotions.  Hunter, Schellenberg, and Schimmack 
(2010) found representation ratings were higher than induction ratings. Our four studies show that having a memory 
associated with a song increased both representation and induction of emotion in music.The first two studies used popular 
music that varied in emotional tone and familiarity.  Participants rated the music on several scales including the degree to 
which it represented the emotion and the degree to which the participant actually felt the emotion (on a seven point scale with 
higher number meaning more representative or more induction).  Participants also indicated whether they had a memory 
associated with the piece and whether that memory was positive, negative, or neutral.  Associated memories increased both 
the representation and induction ratings. For example, a negative familiar song was rated 4.5 for representation and 4.4 for 
induction when no memory was associated with it,  having a positive or negative memory associated with the song raised the 
ratings to 6.1 and 6.3 or 6.3 and 5.7 for representation and induction, F(18, 147) = 1.76, p < .01. In the third  and fourth study 
the researchers used classical pieces instead of popular music to eliminate the lyrics. The results were very similar and there 
was also generally a decrease in the difference between the two factors when a memory was present. For example, “Funeral 
March” by Chopin had a 0.7 difference between representation and induction ratings when there was no memory association 
with the piece; M = 5.5, M = 4.8. When there was a positive memory (M = 7.0, M = 7.0) or negative memory association (M = 
6.4, M = 6.3), the range between the two scores decreased. The current studies demonstrate a complex relationship between 
representation and induction of emotion in music.   
 
 
A.14  Musical Fan’ Stereotypes Activation and Mental Performance 
 
Marek Franek (1)*, Roman Mlejnek (2), Jan Petruzalek (1) 
 
(1) Faculty of Informatics and Management, University of Hradec Králové, Czech Republic, (2) Institute of Musicology, Faculty of Arts, Charles University, Prague, 
Czech Republic  
* = Corresopnding author, marek.franek@uhk.cz 
 
Large body of studies revealed that activation of a certain social stereotype may influence consequent behavior or 
performance. The presented study investigated an effect of stereotypes of musical fans. There are evidences that the social 
stereotype of classical music listener is associated with higher intelligence level compared to stereotypes of fans of certain 
genres of popular music. The goal of our study was to investigate an effect of activation of musical fans stereotypes on a mental 
performance – learning words in a foreign language. A preliminary investigation was conducted, which showed that the 
highest scores of perceived intelligence were associated with listeners of classical music and jazz, while the lowest score with 
fans of techno. Next, an experiment studying an effect of activation of classical music or techno fan’ stereotypes on a mental 
task was conducted. 88 subjects (56 females) aged 19‐22 years took part in the experiment. First, slides with typical forms of 
behaviors of classical music or techno musicians and listeners in a course of a concert/performance were presented. Further, 
the participants were asked to write down a short report about typical forms of behavior of classical music/techno fans. Next, 
during six minutes period the participants were asked to learn twenty Latin words. Finally, they were tested from their 
knowledge. ANOVA revealed the significant effect of the type of stereotype activation on performance in the mental task. 
Neither gender nor interactions had significant effects. Results indicated that the activation of stereotype of classical music 
listeners prior to mental task resulted in a slightly better score in the test of learning foreign words compared to activation of 
the stereotype of techno fans. The differences between various forms of stereotype priming and their effects on performance 
were discussed. 
                                                                                                SMPC 2011 Program and abstracts, Page:       
                                                                                                                                        74 
A.15 When Push Comes to Shove: Rhythmic Behavior in Improvised Walking Bass  
 
Ofer Gazit (1)*, Eric Battenberg (2), David Wessel (3) 
 
(1) Department of Music, University of California, Berkeley, U.S.A (2) Parallel Computing Laboratory, University of California, Berkeley, U.S.A (3) The Center for 
New Music and Audio Technologies, University of California, Berkeley, U.S.A 
* = Corresponding author, ofergazit@berkeley.edu 
 
The aim of this study is to investigate whether improvisation affects synchronization between bass and drums in performance 
of straight‐ahead swing. Previous studies of commercial jazz recordings have shown that bassists demonstrate consistent 
asynchronies of onsets in relationship to the pulse set by the drummer when improvising a “walking bass line”. However, the 
specific effect of improvisation on bassists' rhythmic behavior and synchronization strategies is largely unknown. Twelve 
professional bassists were asked to perform four synchronization tasks, divided into two pitch‐confined paradigms: A “scale 
paradigm” and a “jazz paradigm”. Each paradigm includes a memorized control task and an improvised task. Each task was 
performed in three modes: 1) self‐synchronized performance of the task, 2) a performance of the task synchronized with a 
ride cymbal, and 3) a “double‐time” performance of the task synchronized with a cymbal. This experimental design combines a 
sensorimotor synchronization research model with common stylistic procedures in jazz to allow analyses in different modes 
and tempi, both on global and local levels. It is hypothesized that the allocation of sufficient cognitive resources for the 
production of improvised walking bass lines will affect bassists’ synchronization behavior. More broadly, it suggests that 
behavioral analysis of rhythmically constrained circumstances can illuminate some of the cognitive mechanisms that govern 
jazz improvisation.  
 
 
 
 
 
 
A.16 Directional Asymmetry in Tonal Space and the Dramatic Use of bII: Theoretical 
Underpinnings, Empirical Evidence, and Musical Applications 
 
Bruno Gingras(1)* 
 
(1) Department of Cognitive Biology, University of Vienna, Vienna, Austria  
* = Corresponding author, brunogingras@gmail.com 
 
The endings of numerous musical works from the common‐practice era exhibit a pronounced emphasis on the Neapolitan 
chord, and, more generally, the bII tonal area, often as part of an expanded cadential progression leading to the concluding 
cadence. Such tonicizations of the bII tonal area are often set off through the use of dramatic pauses, virtuoso flourishes, or 
extended chromaticism, and tend to coincide with a breakdown of both voice‐leading and rhythmic continuity. The combined 
effect of these textural changes is a striking instance of concinnity (La Rue, 2001), in which all elements of the musical 
language combine to reinforce the appearance of the Neapolitan chord as a “show‐stopper” which then inexorably leads to the 
final cadence. Here, I suggest that composers’ strategic use of the Neapolitan sixth may owe its effectiveness, at least in part, to 
the perceptual and directional asymmetry between modulations to the “flat side” and modulations to the “sharp side” of the 
cycle of fifths (Rosen, 1971). Werts’ (1983, 1997) classification of modulating harmonic progressions introduced a theoretical 
framework that accounted for this asymmetry. Empirical validation for Werts’ model was provided by Cuddy and Thompson 
(1992), who reported that modulations to the “flat side” effect a clearer perceptual shift in tonal organization than 
modulations to the “sharp side”. Woolhouse’s (2009) model of tonal attraction also predicts a tonal asymmetry similar to 
Werts’ model. I present theoretical support for the hypothesis that an abrupt tonicization of the bII tonal area maximizes the 
perceptual impact of this directional asymmetry, and suggest that this is exploited musically to convey an impression of tonal 
remoteness which, by contrast, highlights the sense of closure brought about by the final cadence. Finally, I submit an 
experimental model to test this hypothesis and discuss relevant musical examples from J.S. Bach and Beethoven.  
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    75 
 
A.17 The Effect of Orchestration Changes on Continuous Responses of Emotional 
Intensity 
 
Meghan Goodchild (1)*, Jonathan Crellin (1), Jonathan Wild (1), Stephen McAdams (1) 
 
(1) Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), Schulich School of Music, McGill University, Montreal, Canada 
* = Corresponding author, meghan.goodchild@mail.mcgill.ca 
 
This project seeks to model one aspect of the temporal dynamics of music listening by investigating the concept of peak 
experience, which consists of two or more coordinated affective responses, such as chills, tears, emotions, awe, and other 
reactions. Peak experiences relating to music have been studied through extensive retrospective self‐reports; however, 
subjective memories may change over time, participants have difficulty verbalizing their ineffable nature, and there is limited 
temporal information. In this study, we explore emotional responses as experienced in real time, and ascertain the role of 
musical context. Previous research suggests that timbral contrasts (e.g., sudden shifts in orchestration) induce emotional 
responses in listeners, but timbre has not been theorized in music research to the same extent as other parameters. Musical 
stimuli were chosen to fit within four categories defined by the researchers based on instrumentation changes: gradual or 
sudden addition, or gradual or sudden reduction in instruments. Forty‐five participants (22 musicians and 23 nonmusicians) 
listened to the orchestral excerpts and completed an explicit behavioural task, which involved continuously moving a slider to 
indicate the buildup and decay of the intensity of their emotional responses. They also completed questionnaires outlining 
their specific subjective experiences (chills, tears, awe, action tendencies, and other reactions) after each excerpt. Musical 
features (tempo, event onset frequency, loudness, instrumentation, texture, roughness and various spectral properties of the 
acoustic signal) were coded as time series and used as predictors of the behavioural time series in a linear regression model to 
explore the relationships between perceptual and musical/acoustical dimensions and to quantify elements of the temporality 
of these experiences. We will discuss response patterns specific to the various musical parameters under investigation, as well 
as consider individual differences caused by factors including musical training and familiarity with the stimuli. 
 
 
 
A.18 What musical distinctiveness says about the organization of compositional 
memory 
 
Eliot Handelman * 
 
CIRMMT, McGill University, Montréal, Québec, Candada 
* = Corresponding author, eliot@colba.net 
 
Composers must produce new music that neither repeats existing music, nor may not be construed by connoisseurs as a close 
variation of existing music, according to cultural standards of originality. Despite the enormous burden thus placed on 
memory and invention, non‐derivative composers (here, J.S. Bach) never seem to inadvertently replicate existing music in part 
or whole: appropriation, probably conscious, implies improvement.  The problem is how the composer's memory may be 
organized and managed in order to to accomplish this. Memory operation can be witnessed in the creative problem of 
attaining distinctiveness in sets of works. Distinctiveness implies differential treatment of especially salient shapes that occur 
in all music. We argue that the most salient shapes are necessarily the simplest, and that an emergent structure of simplicity, in 
the sense of Elizabeth Bates, is available through intuition rather than cultural transmission. This is modeled as a recursive 
shape grammar whose base elements are the enumerable set of simplest things that can be done with musical material. The 
simplest class of shapes so obtained are easily demonstrated to exist in all musical cultures. Nevertheless, no historical rubric 
exists for this category: the creative management of simplicity may well be intuitive. This strengthens the implication that 
there is a parallel between compositional memory and the organization of patterns promoting difference. Applied to the solo 
violin and cello music of J.S. Bach, our model reveals robust _avoidance_ of similarly‐patterned simple structures in a 
combinatorial matrix governing bodies of works that are heard as unique and distinct. This result offers an argument that 
compositional memory is procedural rather than semantic: rather than remembering music as a succession of individual 
notes, music is remembered as structures which implicitly offer dynamic potential for difference and variation, much as chess 
players view configurations of the chessboard. 
                                                                                           SMPC 2011 Program and abstracts, Page:       
                                                                                                                                   76 
 
A.19 An Acoustic Model for Chord Voicings in Post­Tonal Music 
 
Robert Hasegawa * 
 
Eastman School of Music, Rochester, NY, USA 
* = Corresponding author, rhasegawa@esm.rochester.edu 
 
Analysts of post‐tonal music frequently treat pitches as pitch classes, abstracted from any specific register. The generalizing 
power of this theoretical choice has led to remarkable advances, but at the cost of overlooking the effect of register on our 
perception of complex harmonies. Drawing on psychoacoustic research, this paper presents a model for the analysis of post‐
tonal chord voicings that is sensitive to both pitch‐class and register. As recognized by theorists since Rameau, there are 
parallels between the perception of chords and the perception of sounds with complex harmonic spectra. In each case, 
listeners tend to understand higher frequencies as overtones of lower ones, and to group frequencies matching the same 
overtone series into a single Gestalt. According to Alfred Bregman, listeners apply “a scene‐analysis mechanism that is trying 
to group the partials into families of harmonics that are each based on a common fundamental.” An analogous mechanism can 
be developed for the analysis of post‐tonal chord voicings. In relationship to a given root, each of the twelve pitch classes can 
be understood as a tempered approximation of a harmonic partial. Any pitch‐class set can be voiced to fit the overtone series 
of each of the twelve equal‐tempered pitch‐class roots. However, not all of these voicings are equally convincing: the sense of 
harmonic rootedness is conveyed most strongly when (a) pitches are identified with harmonics in the lower part of the 
overtone series, and (b) low harmonics that reinforce the root are present in the chord. By elucidating the perceptual effects of 
different voicings of the same pitch classes, this analytical model offers a fuller appreciation of the vital role of register in 
music by composers including Boulez, Messiaen, and Knussen. The conclusion of the paper considers ways that the model 
could be refined through experimental research and testing. 

 
 
A.20 Brain correlates of happy and sad musical emotions using ragas of Hindustani 
classical music: An electrophysiological study 
 
Shantala Hegde (1)*, Shobini L Rao (1) 
 
(1) Cognitive Psychology & Neuroscience Laboratory, Cognitive Psychology Unit, Center for Cognition and Human Excellence, Department of Clinical Psychology, 
National Institute of Mental Health and Neurosciences, Bangalore, India  
* = Corresponding author, shantalah@nimhans.kar.nic.in 
 
The present study was carried out to examine the electrophysiological correlates of happy and sad musical emotion in 
musically untrained adults. Six ragas of Hindustani Classical Music, three to evoke happy emotion and three to evoke sad 
emotion. Two excerpts from each raga formed the stimulus of duration (mean‐duration = 129.00, SD =6.00 seconds). Ratings 
by (n=30) musically untrained adults on a 5‐point Likert scale showed that stimulus were distinguished as conveying happy 
and sad emotions above level of chance. Sample for the present study included twenty right handed musically untrained adults 
(M: F=10:10, mean age in years: 28.00, SD=4.00). EEG was recorded using the Neuroscan (Syn Amps), sampling rate of 256 Hz, 
with 30 electrodes placed according the 10/20 electrode placement system. Artifact free data was analyzed using fast Fourier 
transformation (FFT) with a Hanning window with 50% overlap and interval of 1023ms with a resolution 0.978 HZ and range 
of 500Hz. Alpha asymmetry was calculated using the formula (ln Right power‐ ln Left power). Preliminary analysis has shown 
that absolute alpha power was significantly higher across all channels while listening to ragas evoking happy emotion in 
comparison to sad ragas and eyes closed rest. The frontal EEG asymmetry did not differ between happy and sad emotion. This 
is the first study examining electrophysiological correlates of musical emotion using ragas of HCM in musically untrained 
listeners. Detailed analysis of musical properties of ragas may contribute for better understanding of present results. The 
results are important in understanding brain correlates of musical emotion. Brain basis of musical emotion is imperative with 
growing popularity of music as evidenced based therapeutic methods in wide range of clinical conditions. (Funded by the 
Department of Science and Technology under the fast track scheme for young scientist, Government of India, SR/FT/LS‐058/ 
2008, PI Hegde).  
 
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    77 
 
A.21 Influence of practiced instruments on the automatic encoding of polyphonic 
melodies.  
 
C. Marie, L. Herrington, & L. J. Trainor  
 
Auditory Development Lab, McMaster University, Hamilton, Ontario 
* = Corresponding author, mariec@mcmaster.ca 
   
Western polyphonic music is typically composed of multiple simultaneous melodic lines of equal importance, referred to as 
“voices”. Previous studies have shown that non‐musicians are able to pre‐attentively encode each voice in parallel sensory 
memory traces as reflected by Mismatch Negativity (MMN) derived from MEG recordings (Fujioka et al., 2004). MMN is seen in 
response to occasional changes in an ongoing stream of repetitive sounds. When presented with sequences composed of two 
simultaneous voices (melodies) with 25% changes in the higher voice and 25% changes in the lower voice, listeners show 
MMN to both changes, even though only 50% of trials are unchanged. Interestingly, the MMN for the higher voice is larger than 
for the lower voice, suggesting a more robust memory trace for the higher of two simultaneous voices. Using a similar 
protocol, the present study tests whether the advantage for the higher voice is malleable by experience. Specifically, we are 
testing whether the pitch register of the instrument played by musicians (higher/lower voice) modifies the dominance of the 
higher‐voice memory trace. Our initial results show that MMN, recorded with EEG, is larger for changes to the lower than to 
the higher voice for cellists and bassists but not for violinists and flutists. These results will inform us about the role of 
experience in encoding melodies in polyphonic contexts, and particularly whether experience playing the lower voice can 
overcome an inherent bias for better encoding of the higher voice.  
 
 
 
 
 
A.22 The Influence of Absolute Pitch to Three­Dimension Mental Rotation And 
Related Processing Characteristics  
 
Jian‐cheng Hou (1,2)*,  Qi Dong (1), Qing‐hua He (1), Chun‐hui Chen (1),  He Li (1), Chuan‐sheng Chen (3),  Gui Xue (4)   
 
(1) State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing 100875, PR China  
(2) BRAMS, Department of Psychology, University of Montreal, Montreal, Quebec, H3C 3J7, Canada 
(3) Department of Psychology and Social Behavior, University of California, Irvine, Irvine, CA 92697­7085, USA 
(4) FPR­UCLA Center for Culture, Brain and Development, University of California, Los Angeles, CA 90095­1563, USA 
* = Corresponding author, bonjovi_hou@163.com 
 
This article discussed about the processing characteristics of the populations with AP ability when they processed three‐
dimension mental rotation (3DMR) task. The subjects were divided into AP subgroup (with AP ability, experimental subgroup) 
and Non‐AP subgroup (without AP ability, control subgroup) and both of them participated in the 3DMR tasks. The results 
showed that: (1) The AP subjects had better 3DMR performance which could attribute to the better ability of visuospatial 
processing; AP subjects had multi‐processing strategies and the visual and spatial imagery, as an important strategy, improved 
the 3DMR performance. (2) Lingual ability could influence the non‐linguistic cognition, the visualization memory and the 
similarity between two objects could influence the spatial cognition and related characteristics, then influenced the 
performance of 3DMR. 
 
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    78 
 
A.23 The Influence of Absolute Pitch to Tone Language Working Memory and 
Related Processing Characteristics 
 
Jian‐cheng Hou (1,2)*,  Qi Dong (1), Qing‐hua He (1), Chun‐hui Chen (1),  He Li (1), Chuan‐sheng Chen (3),  Gui Xue (4)   
 
(1) State Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing 100875, PR China  
(2) BRAMS, Department of Psychology, University of Montreal, Montreal, Quebec, H3C 3J7, Canada 
(3) Department of Psychology and Social Behavior, University of California, Irvine, Irvine, CA 92697­7085, USA 
(4) FPR­UCLA Center for Culture, Brain and Development, University of California, Los Angeles, CA 90095­1563, USA 
* = Corresponding author, bonjovi_hou@163.com 
 
Absolute pitch (AP) has a tight relationship with tone languages and this article discussed about the processing characteristics 
of the populations speaking Mandarin with AP ability when they processed Chinesephonological, semantic and unfamiliar 
Tibetan word form working memory (WM) tasks. The subjects were divided into AP subgroup (with AP ability, experimental 
subgroup) and Non‐AP subgroup (without AP ability, control subgroup) and both of them participated in the three visual WM 
tasks. The results showed that the AP subgroup had significantly better performances on phonological and semantic WM tasks 
than that of Non‐AP subgroup, but no significance on word form WM task. In the limited capacity of WM, the AP subgroup 
could process phonology through ‘lexical tone’ and process semantics through multi‐cognitive strategies, but should need 
more resource to process unfamiliar Tibetan word form and this led to increasing load of WM and no advantage in cognitive 
strategy. These results reflect that the advantage of AP subgroup’s WM decreases with the increasing difficulty of tasks. 
 
 
 
 
 
A.24 Vocal Range Normalization and its Role in the Perception of Emotion in 
Different Voice Types 
 
Randolph Johnson (1)*, Elizabeth Lagerstrom (2) 
 
(1) Ohio State University, Columbus, USA, (2) Ohio State University, Columbus, USA 
* = Corresponding author, randolph.johnson@gmail.com 
 
Listeners perceive melodies as more or less aggressive or appeasing depending on the overall pitch height of the melody. In 
addition to the transposition of all of a melody’s pitches, selective raising or lowering of specific scale degrees also has effects 
on perceived emotions such as sadness and aggression. Whether specific notes are inflected lower with relation to a scale, or 
entire melodies are shifted lower in an instrumental or vocal range, there is cross‐cultural evidence consistent with the notion 
that “lower than normal” is associated with negatively‐valenced emotions. Since pitch highness and lowness are relative to 
specific voice types, judgments of the tone of voice are likely normalized to the range of a given voice. The present study 
investigates listeners’ ability, when presented with a single sung pitch, to determine the pitch’s location within the range of an 
anonymous singer. First, we recorded eleven trained and untrained vocalists as they sang their entire chromatic range on two 
vowel types: “ah” and “ee.” Then, a separate group of participants listened to individual pitches and estimated each note’s 
relative position in each singer’s range. The results are consistent with the notion that listeners can use timbre cues to infer 
the vocal range of a singer and normalize the produced pitch to the inferred range. Participants’ note‐position estimates 
accounted for approximately 70% of the variance of the sung notes’ actual positions within vocal ranges. Vocal range estimates 
of “ee” vowels exhibited significantly smaller range‐estimate discrepancies than “ah” vowels. We observed no significant 
difference of range‐estimate discrepancies between male and female listeners; and no significant difference between estimates 
by vocal and instrumental music majors. For future studies, we explore the potential for spectral analysis techniques to 
uncover specific timbre cues related to tone of voice. 
 
                                                                                       SMPC 2011 Program and abstracts, Page:       
                                                                                                                               79 
 
A.25 Investigating listeners’ preference and brain responses of multichannel­
reproduced piano music 
 
Sungyoung Kim (1)*, Tomasz M. Rutkowski (2) 
 
(1) Yamaha Corporation, Hamamatsu, Japan, (2) University of Tsukuba, Tsukuba, Japan 
* = Corresponding author, sungyoung@beat.yamaha.co.jp 
 
Understanding the complex characteristics of multichannel reproduced music is a major challenge for a recording engineer 
who wants to create a satisfying sound quality and deliver it to listeners. To date, various studies on the topic have 
investigated physical, perceptual, and contextual factors that affect listeners’ preference of multichannel reproduced music. 
This study extended previous findings by investigating the relationship between variation in listeners’ preference and 
physiological factors including variation in the listeners’ brain waves. For the experiment, we recorded various pieces of solo 
piano music using three multichannel microphone arrays, which controlled the stimuli to be constant in terms of musical 
performance but distinctly different from one another, mainly in how to capture the acoustical features and render them for a 
multichannel reproduction system. Six listeners, trained as musicians and recording engineers, participated in the experiment 
that collected listeners’ preference orders among three multichannel piano sounds and measured their 
electroencephalographic (EEG) responses with a 128 channels high‐resolution system. Additionally, electrooculographic 
(EOG) data and electromyographic (EMG) data (for facial muscle activity) were recorded, along with heart rate, breathing rate, 
and front head functional near‐infrared (fNIRS) blood oxygenation. The results showed, through three‐dimensional EEG 
source reconstruction, that the strength of theta, alpha, and beta waves in occipital and parietal cortices correlated with the 
variation in quantified listeners’ preference of multichannel‐reproduced piano music. The parietal cortices were the areas that 
corresponded to auditory spatial information. In contrast, the variation in occipital alpha wave patterns suggested that the 
level of relaxation was related to auditory driven visualizations considering the fact that the listeners kept their eyes open and 
focused on a fixed position in front of them. This pilot study’s results implicated that both auditory spatial features of each 
stimulus and its corresponding visualization procedure have affected listeners’ preference of multichannel‐reproduced piano 
music. 
 
 
 
A.26 The Perception of Non­chord Tones vs. Unexpected Chord Tones in Tonal 
Melodies: Influence of Melodic Context on Implied Harmony Perception 
 
Jung Nyo Kim * 
 
Northwestern University, Evanston, USA 
* = Corresponding author, jung‐kim@northwestern.edu 
 
A previous study on the perception of implied harmony in tonal melodies (Kim, 2009) showed that sing‐back reaction times 
for expected chord tones were faster than those for unexpected chord tones and RTs became faster as implied harmony 
became clearer. However, stimuli in this study consisted of only chord tones. The present study investigates how non‐chord 
tones are interpreted and integrated with chord tones. As in the previous study, 18‐tone melodies were constructed whose 
first 15 tones implied I‐V‐I‐ii‐V. However, each melody had an appoggiatura and a passing tone in the 15‐tone context. Three 
target tones, following the context, consisted of only the expected tonic chord tones or included a non‐chord tone (an 
appoggiatura or a passing tone). These non‐chord tones were either resolved or not. The melodies were presented using a 
gating paradigm: Musicians heard the first 16 tones in the first block, 17 tones in the second, and the whole melodies in the 
third. They sang the last tone of every sequence as quickly as possible and their sing‐back RTs were measured. The 
preliminary analyses showed that the presence of non‐chord tones in the context influenced the interpretation of target tones 
which were not expected chord tones. On the 16th, RT differences between ‘la’ and the tonic chord tones were smaller than 
those in the previous study. Also, when ‘fa’ followed ‘la’, RTs became slower, contrary to the result of the previous study. These 
differences suggest that ‘la’ was interpreted as an appoggiatura in the present study, not as an unexpected chord tone in the 
previous study. In the previous study, RTs for ‘fa’ were faster because implied harmony became clearer. However, in the preset 
study, RTs for ‘fa’ were slower because the appoggiatura was not resolved to the tonic chord tone. The RT trends for the 
passing tone on the 17th were similar to the results for the appoggiatura. 
 
                                                                                              SMPC 2011 Program and abstracts, Page:       
                                                                                                                                      80 
 
A.27 A Critical Examination of the Theory of Tonal Hierarchy and Arguments for a 
New Theoretical Framework for Explaining Tonality Perception 
 
Ji Chul Kim 
 
Northwestern University, Evanston, USA 
* = Corresponding author, jc‐kim@northwestern.edu 
 
Limitations of Carol Krumhansl’s theory of tonal hierarchy are discussed and an alternative theoretical framework is proposed 
to better explain tonality perception. First, Krumhansl’s idea of tonal hierarchy as a nontemporal tonal schema is based on an 
implicit assumption that individual pitch events are the basic psychological units of tonality perception, between which 
similarity and psychological distance can be measured. This view cannot adequately explain the temporal‐order effects 
induced by an established key or the role of temporal structure in establishing a key. I propose that the schematic knowledge 
of tonal organizations should be about perceptually stable and (partially) closed tonal‐temporal patterns. Second, the role of 
bottom‐up processing in the perception of relative tonal stability and its interaction with top‐down processing are not fully 
accounted for in Krumhansl’s theory. I propose that the pattern of tonal stability established by stimulus structure (bottom‐up 
tonal stability) should be distinguished from the pattern of tonal stability inferred from the knowledge of typical tonal 
organizations (top‐down tonal stability). These two types of tonal stability interact in the acquisition and activation of tonal 
schemata and the encoding of particular tonal structures. Third, pitch salience resulting from frequent occurrence, long 
duration, and other surface emphasis does not always lead to the perception of tonal stability. Tonal stability, which is the 
property of pitch events functioning within organized musical structures, is perceived only when individual tones are 
organized into coherent perceptual patterns. From these arguments, I propose that tonality is perceived when individual pitch 
events are perceptually organized into tonal‐temporal patterns with internal reference points (structural pitches and rhythmic 
accents), whose internal relational structures are determined jointly by the bottom‐up cues in stimulus structure and by the 
top‐down inferences based on activated knowledge structure. 
 
 
 
 
 
 
A.28 Pitch and Eyebrow Height; a Transcultural Phenomenon? 
 
Niall Klyn (1)*, Matthew Campbell (1) 
 
(1) Cognitive Ethnomusicology Laboratory at The Ohio State University, Columbus, Ohio, USA 
* = Corresponding author, klyn.1@osu.edu 
 
Any behavior that functions as a signal should be strongly evident cross‐culturally, but most studies on F0 and facial 
expression have focused solely on relatively small cultural groups. Furthermore, the methodology employed in most 
experimental studies by nature often allows for demand characteristics to creep in to the data acquisition process. The present 
study explored the possibility of transcultural F0‐eyebrow height correlation while attempting to eliminate possible demand 
characteristics. Using an existing body of multicultural video with audio ‐ The JVC/Smithsonian Folkways Video Anthology of 
Music and Dance ‐ this study used a three‐pass system to code the data and preserve independence of the coding groups. In the 
first pass, portions of the anthology wherein a singer's face is clearly visible and the singer is individually audible are 
identified. Two independent groups of graders were then recruited. The second pass had the first group of coders mark the 
lowest and highest pitches sung in the passages identified in pass one, but listened to the anthology with no video. The second 
group graded the relative eyebrow height of the singer at the time points identified by the first group while watching the video 
with no audio. The study then explored the correlation between these data. 
 
 
 
                                                                           SMPC 2011 Program and abstracts, Page:       
                                                                                                                   81 
 
A.29 Functional neuroimaging of musical emotions: a review and meta­analysis 
 
Thomas Kraynak* 
 
Case Western Reserve University, Cleveland, Ohio, USA 
* = Corresponding author, tek11@case.edu 
 
In the past ten years researchers have carried out dozens of neuroimaging studies using musical stimuli that induce emotional 
responses.  Curiously, the results of these studies have rarely been pooled together and compared in the fashion of a meta‐
analysis.  Considerable variance in experimental design and power may dissuade such combinative research, however shared 
activation across particular structures and systems in varying paradigms would indicate significant attributions in emotional 
and musical processing.  Main questions include the effect of crossmodal stimulus integration on emotion perception, the 
experimental effect of computerized vs. live recorded music, and the interplay between emotional valence and their respective 
neural networks. Published fMRI and PET studies involving music and emotion were acquired, and Talairach coordinates of 
significant activations were pulled.  Studies combining music and other emotional stimuli, such as film and pictures, were 
included.  Because of the small number of collected studies (~30 at this point, dwarfed by typical meta‐analyses) I am applying 
a multi‐level kernel density analysis (MKDA; Wager et al., 2007) in order to control for false positives.  Studies are still being 
added to analyses, however initial analyses show significant bilateral amygdalar activation in both positive and negative 
emotional contrasts.  Furthermore, some crossmodal combined conditions seem to exhibit musical accompaniment continuing 
to activate sensory integration areas (MTG) in the absence of visual stimuli. This meta‐analysis of the current neuromusical 
literature will prove a valuable resource for future directions in imaging studies, as well as field discussion on the importance 
of uniformity in music neuroimaging studies. 
 
 
A.30 Effects of Contour Change on Memory Encoding for Minuets: An ERP Study  
 
Shannon L. Layman (1)*, Ramiro R. Lopez (1), W. Jay Dowling (1) 
 
(1) The University of Texas at Dallas, Richardson, TX, USA 
* = Corresponding author, shlayman@utdallas.edu 
 
How does one encode the auditory information of music into memory? Previous studies hypothesized that features of music, 
such as contour, are important in memory encoding. It has also been demonstrated that the time interval between hearing a 
sample of music and the listener’s response to it determines the degree of integral formation of memory. The present study 
explores memory encoding for melodic contours over variable segments of time.  We presented listeners with classical 
minuets in which recollection of an initial phrase was tested after short (4 s) or long (12 s) delays. There were two test 
conditions: no change; and a change in contour at either the 3rd or 4th note from the start of the test phrase. Participants 
identified test items as the same as or different from the initial phrases.  Responses differed between long and short delays, 
suggesting that participants, given time, were able to convert the initial phrases into usable memory stores. Additionally, we 
measured EEG responses to explore how the brain responds when a contour change occurs in a just‐heard musical phrase. 
Results suggest that in non‐musicians, when the target is farther in temporal proximity to the test, the ERP component 
occurring in the latency range from 300 to 700 ms (the N5 component) shows a greater negativity in the central fronto‐lateral 
region than when the target is closer to the test. In musicians, however, the opposite pattern occurs. When the target is close in 
temporal proximity to the test, the N5 shows a greater negativity than when the target is farther from the test. These results 
shed light upon the previous behavioral results demonstrating that, at least in musicians, when the target and test are within 
close temporal proximity, there is a possible hindrance to memory.  
          
          
          
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    82 
 
A.31 Relationship between basic auditory abilities and performance on the MBEA 
 
J. Devin McAuley (1)*, Elizabeth Wieland (2) 
 
(1) Department of Psychology, Michigan State University, East Lansing, USA, (2) Department of Psychology, Michigan State University, East Lansing, USA 
* = Corresponding author, dmcauley@msu.edu  
 
Congenital amusia is a characterized as a music‐specific disorder that is unrelated to general auditory functioning, cognitive 
ability, or exposure to music (Ayotte, Peretz,& Hyde, 2003).  Impairments in congenital amusia include deficits in pitch 
processing and difficulty recognizing melodies without the aid of lyrics. Rhythm impairments in amusia are less consistently 
observed.  The most widely used method to diagnose amusia involves the Montreal Battery of Evaluation of Amusia (MBEA),  
which consists of six subtests that separately assess melodic organization, temporal organization, and memory. In a related 
literature on individual differences in auditory abilities, previous research has shown that speech perception abilities are 
surprisingly independent of a number of basic auditory abilities that tap into spectral and temporal processing.  The present 
study considered the question of whether basic auditory abilities predict aspect of music perception, and more specifically 
performance on the MBEA. The test we used to assess auditory processing was the test of basic auditory capabilities (TBAC) 
(Watson et al., 1982; Kidd, Watson, & Gygi, 2007). The TBAC consists of three single‐tone discrimination tests varying 
frequency, duration, and intensity, three temporal pattern processing tests, and two speech tests. Participants completed the 
TBAC and MBEA on different days with the order of the two tests counterbalanced. Overall, performance on the TBAC and 
MBEA was highly correlated. Regression analyses revealed that approximately 50% of the variance on the MBEA is accounted 
for by single‐tone frequency discrimination and the ability to detect the presence or absence of a single tone embedded in the 
middle of a 9‐tone sequence. 
 
 
 
A.32 Neural mimicry during perception of emotional song 
 
Lucy McGarry (1)*, Lisa Chan (1), and Frank Russo (1) 
 
(1)Ryerson University, Toronto, Canada 
* = Corresponding author, lmcgarry@psych.ryerson.ca 
 
Recent research in our laboratory using facial EMG has demonstrated that individuals mimic emotional movements during 
perception of song (Chan & Russo, in prep). In the current study, we wished to determine whether perception of an emotional 
song generates enhanced neural mimicry in addition to peripheral mimicry.  The mu wave, an electroencephalographic (EEG) 
oscillation with peaks at 8‐12 and 20 Hz, whose suppression occurs during perception and execution of action, is thought to 
reflect mirror neuron system (MNS) activity.  The mirror neuron system is purported to be involved in the perception and 
analysis of emotional intention.  We predicted that lyrics sung emotionally would generate greater mu suppression than the 
same lyrics sung neutrally. Data was collected using a within‐subjects design.  EEG data was collected while participants 
viewed randomized audiovisual representations of the same singer performing the same song lyric in three different ways: 
happy, neutral, or sad.  Timing was equated across conditions. Preliminary results suggest that happy and sad song lyrics 
generate greater mu suppression than neutral lyrics. Our results suggest that emotional song performance leads to greater 
neural simulation of movement than neutral song performance: When the same stimuli are performed in an emotional as 
opposed to neutral way, greater suppression of the mu wave is elicited, suggestive of greater MNS activity.  It is difficult to 
discern whether this enhanced MNS activation causes peripheral mimicry, or whether peripheral mimicry elicits greater MNS 
activity as a result of moving.  Future studies utilizing muscle paralysis techniques should examine whether neural mimicry 
occurs independently of movement. 
 
 
 
                                                                                         SMPC 2011 Program and abstracts, Page:       
                                                                                                                                 83 
 
A.33 Investigating the Role of Musical Expertise in Phonetic Analogies of Guitar 
Timbre 
  
Audrey Morin (1)*, Nathalie Gosselin (2), Caroline Traube (3) 
 
(1) BRAMS/OICRM, Université de Montréal, Montréal, Canada, (2) Adjunct Professor, Faculté de musique, Research Associate, BRAMS/OICRM, Université de 
Montréal, Montréal, Canada, (3) Associate Professor, Faculté de musique, BRAMS/CIRMMT/OICRM, Université de Montréal, Montréal, Canada 
* = Corresponding author, audrey.morin.3@umontreal.ca 
  
Expert guitar players create a great variety of timbres by varying the plucking position, angle and strength on the string, and 
these different timbres can be characterized by their similarity to particular vowels by guitarists (Traube et al., 2004). The 
main goal of this study is to investigate the origin of these phonetic analogies as used for timbre description (by means of 
onomatopoeia for example) and the role of musical expertise in their perception. To this aim, we tested expert guitar players 
(with an average of 14 years of musical education) and non‐musicians (no musical education outside regular Quebec 
curriculum) on a priming task. The stimuli of this task were produced by a professional guitarist who played two contrasting 
guitar sounds that corresponded to two French vowels ([ɛ̃] as in "vin" and [u] as in "vous"), and then sung the corresponding 
vowels, always on the same pitch. With these recorded stimuli, we ran a priming experiment in which participants performed 
in two conditions: guitar‐primed or vowel‐primed trials. Each trial consisted in the vowel target [ɛ̃] or [u] preceded by a prime 
that was congruent or incongruent with the target. Participants were instructed to identify the auditory vowel targets while 
ignoring the first sound of the pair. Preliminary results tend to show a similar reaction time profile for guitarists and for non‐
musicians. Both groups were faster to name the vowel when the prime was congruent with the target. Moreover, this 
congruency effect is observed on both guitar‐primed and voice‐primed trials. These results tend towards the hypothesis of a 
perceptual similarity between guitar sounds and vowels instead of an effect of musical training and/or learning by association. 
This could be explained by the acoustical similarities between particular guitar sounds and vowels (similar overall shape of 
spectral envelope). 
 
 

A.34 Harmonic Function from Voice­Leading: A Corpus Study 
 
Ian Quinn (1)*, Panayotis Mavromatis (2) 
 
 
(1) Yale University, New Haven, CT, USA, (2) New York University, New York, NY, USA 
* = Corresponding author, ian.quinn@yale.edu 
 
We describe a data representation for voice leading between two sonorities in a chorale texture, and a similarity measure for 
these voice leadings.  These tools are used in an empirical study of the relationship between voice leading and harmonic 
function in a corpus of Bach chorales and a corpus of Lutheran chorales from a hundred years earlier. Common voice‐leading 
types in the corpora are subjected to a cluster analysis that is readily interpreted in terms of harmonic functional syntax. We 
are thus able not only to read a theory of harmony directly out of a corpus, but to do so without building in a priori notions of 
chord structure, rootedness, or even key. The cluster analysis also clarifies important syntactic differences between the pre‐
tonal (modal) corpus and the Bach (tonal) corpus. 
 
 
 
                                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                                     84 
 
A.35 Composing by Selection: Can Nonmusicians Create Emotional Music?   
 
Lena Quinto (1)*, William Forde Thompson (1), Alex Chilvers (2) 
 
(1) Department of Psychology, Macquarie University, Sydney, Australia, (2) Sydney Conservatorium of Music, University of Sydney, Sydney, Australia.  
* = Corresponding author, lena.quinto@mq.edu.au 
 
In Western classical music, the communication of emotion typically proceeds from a process of emotional coding by 
composers and performers to emotional decoding by listeners. It is well established that listeners are sensitive to the 
emotional cues used by composers and performers, but because their role is passive, it is unclear whether they are competent 
at manipulating these same cues in order to create an emotional composition. The purpose of this experiment was to 
determine whether nonmusicians are capable of communicating emotion in music by manipulating relevant acoustic 
attributes. Twenty‐three participants (1.52 years of average training) were presented with pairs of quasi‐random pitch 
sequences that differed in intensity (low vs. high), articulation (staccato vs. legato), pitch height (high vs. low), tempo (fast vs. 
slow) and modality (major vs. minor). They then chose the sequence that most strongly conveyed one of the emotions of 
anger, fear, happiness, sadness and tenderness. Participants’ choices were retained in each successive trial, such that the 
selected attributes were cumulative. Once all of the choices had been made for a given emotional intention, a new random tone 
sequence was generated that contained all of the selected attributes. The choices made by participants were significantly 
different across emotional intentions. This was especially pronounced for the expression of happiness and sadness. The 
former was conveyed with a high intensity, fast tempo, staccato articulation, high pitch and major mode, and the latter with 
the opposite cues. The choices for the other emotions were mixed. Many decisions mirror those made by expert musicians 
although discrepancies did arise. These findings broaden our understanding of musical communication by demonstrating the 
presence and limitations of explicit knowledge for emotional communication in nonmusicians.  
 
 
 
 
A.36 Deconstructing Evolution's Sexual Selection Shows Music Could Arise Without 
Becoming Sex Dimorphic: Music is Not a Fitness Indicator 
 
Mark S. Riggle * 
 
Causal Aspects, LLC, Charlottesville VA, USA    
* = Corresponding author, markriggle@alumni.rice.edu 
 
A fundamental question concerning music: was music selected for by evolution or did music emerge as a side‐effect of other 
traits? Many researchers assume the latter because of the lack of viable scenarios describing evolutionary selection for music.  
Among the non‐viable sexual selection scenarios is music working as a fitness indicator controlling female choice; this is 
because music ability is not sex dimorphic, and, if functioning as a fitness indicator, then dimorphic it must be.  However, 
although sexual selection usually leads to a sex dimorphism of the selected trait, we show that under certain conditions, sexual 
selection will fail to produce a sex dimorphism. Sexual selection rapidly develops a trait because males possessing stronger 
expression of the trait gain a reproductive advantage. While the trait will initially express in both sexes, the trait only becomes 
dimorphic if the trait produces a fitness reduction in females. That is, without the reduction in female fitness, there is no 
selective pressure to sex link the trait.  Thus a trait can be under rapid development, and, if it merely remains fitness neutral 
for the female, then the trait will not become dimorphic.  Since all traits have a fitness cost, then for the trait to remain fitness 
neutral, it must offer some offsetting benefit. We show that if a particular sensory pleasure for rhythmic sounds exists, then for 
those individuals who can create rhythmic sounds, both sexes gain benefit. While the males gain a direct reproductive 
advantage, the females' advantage may lay in group bonding of females. A bonded group provides protection to her offspring 
aganist conspecifics (a female reproductive advantage). Although music ability may thus be sex monomorphic, other music 
related traits, such as pleasure and motivations, may be dimorphic. We show music may have arisen by sexual selection 
without being a fitness indicator. 
 
 
 
                                                                               SMPC 2011 Program and abstracts, Page:       
                                                                                                                       85 
A.37 Musical Training, Working Memory, and Foreign Language Learning 
 
Matthew Schulkind (1)* and Laura Hyman (2) 
 
(1) Amherst College, Amherst, MA, USA (2) Amherst College, Amherst, MA, USA 
* = Corresponding author, mdschulkind@amherst.edu 
 
Recent empirical work has suggested that the cognitive processing of music and language are closely aligned.  The brain areas 
involved in processing musical and linguistic syntax appear to overlap and musical training enhances pitch processing in both 
musical and linguistic contexts.  Foreign language and musical training also have been show to facilitate performance on 
executive control tasks (e.g., the Simon task).  The current study was designed to examine whether musical training and/or 
musical aptitude are associated with improved performance on a foreign language‐learning task.  The subjects in the 
experiment were college students with varying amounts of musical experience.  The subjects participated in two experimental 
sessions.  In the first session, the subjects completed tests of working memory span and musical aptitude; a musical training 
questionnaire and a questionnaire assessing experience with foreign language learning (both in the home and in academic 
settings) were also completed. In the second session, the subjects were taught 40 words and 12 common phrases in an 
unfamiliar foreign language (Amharic, spoken in Ethiopia). The learning phase was followed by a test phase that assessed 
old/new recognition for words and recall of definitions.  Performance on the foreign language recall and recognition tasks was 
directly correlated with measures of musical training, but not with measures of musical aptitude.  Musical aptitude scores 
were correlated with simple digit span and with measures of foreign language experience outside of the laboratory (e.g., 
number of languages studied; languages spoken in the home during childhood).  Finally, working memory span was correlated 
with both years of foreign language study and some measures of musical training.  These data suggest that both foreign 
language and musical training may enhance general working memory capacity and that some aspects of musical and language 
behavior draw upon a common pool of cognitive resources.  
 
 
 
 
 
A.38 Surveying the Temporal Structure of Sounds Used in Music Perception 
Research 
 
Michael Schutz (1)*, Jonathan Vaisberg (1) 
  
(1) McMaster Institute for Music and the Mind.  Hamilton, Ontario CANADA 
* = Corresponding author, schutz@mcmaster.ca 
 
A sound’s temporal structure or “amplitude envelope” is both perceptually salient and informative – it allows a listener to 
discern crucial information about the event producing the sound. Recent work has shown that differences in envelope lead to 
categorically different patterns of sensory integration.  Namely, sounds with “percussive” envelopes indicative of impact 
events (i.e. fast onset followed by an exponential decay – such as the sound produced by a piano) integrate more readily with 
concurrent visual information than sounds with the “flat” amplitude envelopes (i.e. fast onset, flat sustain, fast offset sounds 
such as a telephone dial tone; Schutz, 2009).  Additionally, melodies with percussive envelopes are easier to associate with 
target objects (Schutz & Stefanucci, 2010) than their flat counterparts.  Given these differences, we were curious to explore the 
degree to which each is used in auditory research.   To this end, we examined the temporal structure of sounds used in articles 
published in the journal Music Perception (this project is based on a larger survey by Tirovolas & Levitin that will also be 
presented at SMPC 2011).  Our analysis indicates that of the empirical articles using either single synthesized tones or a 
sequence of synthesized tones (over 100 papers), roughly 30% exclusively used flat envelope shapes, and roughly 20% used at 
least one percussive sound (i.e. piano or percussive tones). Curiously, 35% of the articles did not offer sufficient information 
about the temporal structure to allow for classification.  Given the admirable attention to methodological detail displayed in 
these articles, we found this omission intriguing.  In conjunction with our recent empirical work on the perception of 
percussive vs. flat tones, we believe this suggests a rich area of future perceptual research that will be of interest to the field. 
 
 
                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                     86 
A.39 Context Dependent Pitch Perception in Consonant and Dissonant Harmonic 
Intervals 
 
George Seror III *, Jeremy Gold, W. Trammell Neill  
 
University at Albany, State University of New York, Albany New York U.S.A 
* = Corresponding author, gs433931@albany.edu 
 
We conducted 2 experiments to examine if the perception of component tones in a harmonic interval is affected by the 
consonance or dissonance of the interval.  A second aim of this study was to determine if the effect of consonance or 
dissonance differed for upper vs. lower voices.  Each experiment used a single tone (B or C) followed by a harmonic interval in 
which the single note was repeated (e.g., C followed by C‐G or C‐F#) or one in which the single note was not repeated (e.g., C 
followed by B‐F#, or B‐G).  In experiment 1, the context tone in the interval (F# or G) was always above the target tone (B or 
C).  Participants’ reaction time and accuracy were measured for Yes‐No key‐press responses.  We hypothesized that 
participants would be slower and less accurate when responding to tones in a dissonant interval (e.g. TT) vs. a consonant 
interval (e.g. P5).  Specifically, our hypothesis suggests phase mismatch in the dissonant condition and harmonic 
reinforcement in the consonant condition as the reason for this effect.  For reaction time and accuracy, there was a main effect 
of a main effect of consonance, with tone detection in consonant intervals being faster and more accurate than dissonant 
intervals.  Experiment 2 used the same procedure except that the context tone in the interval was always below the target 
tone. To eliminate the potential effects of varying target pitch height, we used the same target tones as experiment 1.  No main 
effect of consonance was found.  The results from experiment 2 indicate that the consonance or dissonance produced by the 
lower context tone did not affect judgments of the upper target tone.  We conclude that the perception of lower but not upper 
tones in a harmonic interval is affected by the interval’s consonance or dissonance.   
 
 
 
 
 
A.40 Primitive Hierarchical Processes and the Structure of a Single Note 
 
David Smey * 
 
CUNY Graduate Center, New York, USA 
* = Corresponding author, davesmey@gmail.com 
 
Twenty‐five years after its publication Lerdahl and Jackendoff's Generative Theory of Tonal Music remains unique in positing a 
strictly segmented and rigorously ordered framework for hierarchically‐organized musical perception.  Such a framework is 
valuable in that it provides a comprehensive working model that can satisfy the interests of music theorists and place more 
focused investigations in a wider context.  However, as an actual model of cognitive organization the theory is somewhat 
problematic  ‐‐ it is not geared to account for tonal perceptions in real time and seems insufficiently grounded in processes of 
everyday hearing. My current project aims to dismantle and reconfigure the Lerdahl and Jackendoff  framework into a model 
that is more dynamic, efficient, and authentic.  I begin by considering the perception of single notes (and, more generally, 
discrete sonic events), a phenomenon that the GTTM authors took as given.  When one focuses on individual sounds a new 
sequence of events becomes clear, as what I call "primitive" event‐hierarchical processes are engaged before meter rather than 
after.   I recharacterize grouping as mere edge‐detection, assert that the perception of continuity is intrinsically event‐
hierarchical, and describe a model of meter that is less self‐contained and internally consistent. 
 
 
 
 
                                                                                   SMPC 2011 Program and abstracts, Page:       
                                                                                                                           87 
A.41 The Ineffability of Modern Art Music 
 
Cecilia Taher (1)* 
 
(1) University of Arkansas, Fayetteville, USA 
* = Corresponding author, ctaher@yahoo.es 
 
This poster explores the relationship between text description and non‐musicians’ perception of modern art music, aiming to 
illuminate our comprehension of that kind of music. The practice of providing verbal descriptions of the music to the audience 
is  widespread.  Nevertheless,  the  effects  of  verbal  descriptions  and  theoretical  comprehension  of  music  on  the  way  we 
experience music are still unclear. Contrary to what we would expect according to the practice of program notes, experimental 
studies have shown that “prefacing a [tonal] excerpt with a text description reduces enjoyment of the music” (Margulis, 2010). 
The complexity of tonal and atonal music seems to lie in different aspects of the musical discourse. In the present experiment 
with a repeated‐measures design, 65 college non‐music majors heard 24 50‐second long excerpts taken from compositions by 
Carter,  Berio,  Ligeti,  and  Stockhausen,  and  rated  their  level  of  enjoyment  along  a  7‐point  Likert‐like  scale.  The  participants 
read a four‐line text before listening to each excerpt. Three kinds of texts were randomly presented: (1) structural, technical 
text  about  the  music  that  follows;  (2)  dramatic,  affective  text  about  the  music;  and  (3)  non‐related  text.  The  results  suggest 
that  the  mental  processes  and  emotional  responses  that  underlie  non‐musicians’  enjoyment  of  modern  art  music  are  not 
guided by the structural or dramatic information that a short descriptive text can convey. Some contemporary pieces appear 
to be more enjoyable than others, independently of the previous information given to the listener. Despite the complexity of 
modern art music, increased knowledge in the form of short descriptions does not prove to have an effect on enjoyment. 
 
 
 
 
 
A.42 Music and the Phonological Loop 
 
Lindsey Thompson (1)*, Margie Yankeelov (1) 
 
(1) Belmont University, Nashville TN, USA 
* = Corresponding author, lindseymarie.thompson@gmail.com 
 
Research on the phonological loop and music processing is currently inconclusive, due both to conflicting data and differing 
definitions of “musician” or target “musical” information. The goal of the current study is to help unify the literature on musical 
working memory by modifying certain design elements and definitions. I used previous methods of measuring musical and 
linguistic working memory abilities with interference, but modified them to account for musical syntax and intervallic 
relationships. Across two experiments with similar interference structures, 31 musically proficient and 31 musically non‐
proficient Belmont University undergraduates listened to six practice tracks and 30 experimental tracks that contained five 
seconds of interference. In Experiment 1, interference consisted of white noise, a complete sentence, or a monophonic melodic 
phrase. In Experiment 2, interference consisted of white noise, a list of 3‐syllable English words, or a list of 3‐note musical 
patterns selected from the taxonomy of tonal patterns (Gordon, 1976). Participants were instructed to remember a 3‐syllable 
word or 3‐note musical pattern and to determine whether or not the remembered word/pattern was the same or different 
from a second word/pattern presented after the interference. Results of two‐way balanced ANOVAs yielded significant 
differences between musical participants and non‐musical participants, as well as between interference types for musical 
stimuli, implying a potential revision of the phonological loop model to include a temporary storage subcomponent devoted to 
music processing. 
 
 
 
                                                                                SMPC 2011 Program and abstracts, Page:       
                                                                                                                        88 
A.43 The Effect of Training on Melody Recognition 
 
Naresh N. Vempala (1)*, Frank A. Russo (1), Lucy McGarry (1) 
 
(1) SMART Lab, Department of Psychology, Ryerson University, Toronto, Canada 
*  = Corresponding author, nvempala@psych.ryerson.ca 
                                                                    
We extend a study by Dalla Bella et al. (2003) that compared melody recognition in musicians and nonmusicians. They 
identified three landmarks: familiarity emergence point (FEP), isolation point (IP) and recognition point (RP) and showed that 
musicians took longer in reaching IP, but had an earlier FEP and RP. Using cohort theory, Dalla Bella et al. interpreted these 
results as a consequence of a larger stored corpus in musicians’ long‐term memory (LTM). Vempala and Maida (2009) later 
simulated these findings computationally using corpus sizes that might be expected in musicians vs. nonmusicians. In the 
current study, we attempt to experimentally test the corpus size hypothesis, by inducing differential corpus sizes in two 
groups of participants. Participants with similar levels of music training were divided into two groups: large corpus (LC) 
and small corpus (SC). Eight diatonic melodies were composed in each of four different modes: Dorian, Phrygian, Lydian, 
Mixolydian with Dorian likely being the most familiar mode. The LC group learned all 32 melodies while the SC group learned 
only eight (two from each mode). Both groups were tested on familiarity and recognition of these melodies using a gating 
paradigm. Our results revealed no difference between groups with regard to FEP, IP, or RP. Based on the null findings, we 
believe that we were unsuccessful in our attempt to induce differential corpus sizes, likely due to the number of melodies 
already in listeners’ LTM prior to the experiment. Nonetheless, an important new finding emerging from this work is that LC 
participants demonstrated confidence in recognition (RP) of Dorian melodies significantly earlier than SC participants. This 
latter finding suggests that in addition to overall corpus size, confidence through short‐term acquisition in a familiar domain 
plays a significant role in the time‐course of melody recognition. 
 
                                                                                              SMPC 2011 Program and abstracts, Page:       
                                                                                                                                      89 
 
 
                                Titles and abstracts for poster session 2 (Saturday) 
                                                Arranged alphabetically by last name of first author 
 
 
B.1 Does the change of a melody’s meter affect tonal pattern perception? 
 
Stefanie Acevedo (1)*, David Temperley (2), & Peter Q. Pfordresher (1) 
 
(1) University at Buffalo, State University of New York, UAS, (2) Eastman School of Music, Rochester, New York, USA 
* = Corresponding author, sacevedo@buffalo.edu 
  
The interplay between motivic structure (repeated melodic patterns) and metrical structure is thought to be a critical 
component of music perception (cf. Lerdahl & Jackendoff, 1983). It has been anecdotally observed that the recognition of 
repeated melodic patterns is faciliateted when the repetitions are aligned with the meter (Povel & Essens, 1985; Sloboda, 
1983), but this has never been systematically demonstrated. We report an experiment that explored whether matched 
metrical and motivic structure facilitates the recognition of alterations to pitch patterns that have been stored in short‐term 
memory. Eight tonal melodies were composed with binary (four‐note) or ternary (three‐note) repeated patterns. Each melody 
was preceded by a harmonic progression that suggested either a simple meter (aligned with the binary patterns) or a 
compound meter (aligned with the ternary patterns), and a regular metronome click occurred throughout the melody that 
maintained the implied meter. Melodies, thus, consisted of motivic structures and metrical structures that were crossed 
factorially and could match or mismatch. On each trial, participants heard a single combination of meter and melody twice; in 
half the trials, one pitch in the second presentation could be displaced. Results showed differences between musically trained 
and untrained subjects (mean percentage correct: trained subjects = 84%, untrained subjects: 57%). The results for untrained 
subjects showed no matching effects. However, trained subject data showed a marginal interaction between pattern and meter 
effects: within the context of the same motivic pattern structure, matching metrical structure resulted in increased accuracy 
compared to mismatching metrical structure. Trained subjects also showed greater accuracy on stimuli with ternary motivic 
structures. The current results show possible influences of higher‐order aspects of pattern structure on the perception of 
local properties of events (in this case, pitch class), as well as possible differences in perception of motivic structures of 
differing lengths. 
 
 
 
B.2 The Melody of Emotions 
 
Michel Belyk (1)*, Steven Brown (1) 
 
(1) Department of Psychology, Neuroscience and Behaviour, McMaster University, Hamilton, Ontario, Canada 
* = Corresponding author, belykm@mcmaster.ca 
 
Melody in speech conveys linguistic information, such as syllabic stress and sentence focus. But it conveys emotional meaning 
as well. The objective of the present study was to examine the connection between music and emotion through melody. Most 
previous studies linking music with emotion have focused on comparisons of register, pitch range, tempo, and timbre, to the 
neglect of melody. The current study examined the melodic shapes of elicited emotional expressions. Subjects were presented 
with  neutral  text‐scenarios  followed  by  pictures  indicating  the  outcome  of  the  scenario;  the  scenario‐picture  pairs  were 
designed  to  elicit  a  wide  range  of  emotions.  Subjects  were  then  cued  to  vocalize  an  appropriate  exclamation  (e.g.,  “Yay!”  in 
response  to  joy).  Pitch  contours  were  extracted  from  the  recorded  vocalizations  using  Praat,  and  then  melodic  shapes  were 
analyzed using Functional Data Analysis, a cutting‐edge statistical tool for analyzing the structure of time‐varying data. Most 
exclamations  –  much  like  musical  phrases  –  were  characterized  by  a  sudden  rise  followed  by  a  gradual  fall  (as  in  “Yay!”). 
However,  a  subset  of  emotions  had  distinctive  melodic  features:  sensual  pleasure,  disgust,  gratitude,  appreciation,  awe,  and 
terror  were  expressed  with  a  more  symmetrical  rising‐and‐falling  contour  (as  in  the  “Mmmm”  of  pleasure).  These 
observations suggest an important melodic parallelism between emotional and musical expression. 
                                                                                                SMPC 2011 Program and abstracts, Page:       
                                                                                                                                        90 
 
B.3 Expression in romantic piano music: Critera for choice of score events for 
emphasis  
 
Erica Bisesi (1)*, Richard Parncutt (2) 
 
(1) Centre for Systematic Musicology, University of Graz, Graz, Austria, (2) Centre for Systematic Musicology , University of Graz, Graz, Austria 
* = Corresponding author, erica.bisesi@uni‐graz.at  
 
Motivation. Musical accents may be immanent (grouping, metrical, melodic, harmonic) or performed (variations in timing, 
dynamics and articulation). Performers use performed accents to “bring out” immanent accents. How they do that depends on 
musical/personal style, temporal/cultural context, and intended emotion/meaning. We are investigating pianists’ intuitive 
artistic criteria for selecting score events for emphasis and for deciding what kind of emphasis to apply. We are independently 
listening to diverse commercially available recordings of Chopin Preludes op. 28 and intuitively marking salient features of 
each pianist’s timing, dynamics and articulation on the scores, focusing on striking differences among performances. On the 
basis of this data we are formulating intuitive individual principles for selecting and emphasizing score events. Generally, 
melodic accents are more important than is generally assumed in the psychological literature, which focuses first on phrasing 
and second on harmonic accents. Pianists who play more virtuoso tend to shape big phrase arches by mean of wide changes in 
timing and dynamics, while emphasis on melodic and harmonic accents is specific of slower and more meditative 
performances. Regarding the latter, we find out two main groups of performances: those where accents are locally emphasized 
contrasting the stability of the phrases, and those where they are pillars supporting the agogics of phrases and sub‐phrases.  
Previous research on expression has searched for underlying principles but neglected qualitative accounts of the rich detail in 
individual performances. We are balancing sciences (psychology, computing), humanities (theory/analysis, history) and 
performance (intuitive knowledge of pianists). Our intuitive listening method has advantages and disadvantages over 
quantitative analysis of MIDI files, so the two approaches should be combined. [This research is supported by Lise Meitner 
Project M 1186‐N23 “Measuring and modelling expression in piano performance” of the Austrian Research Fund (FWF, Fonds 
zur Förderung der wissenschaftlichen Forschung).] 
 
 
 
 
B.4 Melodies and Lyrics: Interference Due to Automatic Activation 
 
Jack Birchfield (1)* 
 
(1) University of Texas at Dallas, Dallas Texas USA 
* = Corresponding author, jack.birchfield@utdallas.edu 
 
Melodies and their associated lyrics are uniquely bound in memory, but can merely hearing a melody automatically activate 
retrieval of its lyrics? Previous research into this question has been inconclusive, and I propose that relative familiarity of the 
musical stimuli can account for the different outcomes. In effect, highly familiar songs are more likely to produce automatic 
activation of the associated lyrics than less familiar songs. To explore this premise, I conducted two experiments which 
controlled both for the familiarity of the musical stimuli and for the presence or absence of lyrics. In Experiment 1, 
participants heard a random 9‐digit sequence followed by one of five auditory distractors (highly familiar or less familiar vocal 
songs presented without their lyrics [HVNL and LVNL], familiar instrumental music, or white noise), then were asked to recall 
the digits in correct order. The hypothesis was that, if the lyrics were triggered, they should interfere with rehearsal and 
retention of the digit sequence, thus producing poorer recall performance compared to instrumental music or noise. However, 
results showed no significant difference between HVNL songs and instrumental music. As lyric interference is dependent on 
rehearsal of the digits, it was possible that some participants were not rehearsing. Thus, in Experiment 2 participants were 
required to silently articulate the digits to ensure rehearsal. Overall results were similar to Experiment 1, but it was found that 
participants with higher musical expertise demonstrated significantly poorer recall when hearing HVNL than from hearing 
instrumental music. While this finding seems consistent with the concept of automatic activation, the difference was due to 
better performance while hearing instrumental music, rather than a decrease in HVNL performance. This suggests that the 
musically trained are better at disregarding familiar instrumental music than those with less expertise, yet are equally effected 
by unsung lyrics.       
 
 
                                                                                               SMPC 2011 Program and abstracts, Page:       
                                                                                                                                       91 
 
B.5 Musical Expertise and the Planning of Expression During Performance 
 
Laura Bishop*, Freya Bailes, Roger T. Dean 
 
MARCS Auditory Laboratories, University of Western Sydney, Sydney, Australia 
* = Corresponding author, l.bishop@uws.edu.au 
 
Musicians often say that the ability to imagine a desired sound is integral to expressive performance.  Our previous research 
suggests that expressive loudness can be imagined and that experts imagine it more vividly than non‐experts.  Research also 
suggests that online imagery may guide expressive performance when sensory feedback is disrupted.  However, both the 
effects of sensory feedback disruption on online imagery and the relationship between online imagery ability and musical 
expertise remain unclear.  This study aims to investigate the relationship between musical expertise and the ability to imagine 
expressive dynamics and articulation during performance.  It is hypothesized that imagery can occur concurrently with 
normal performance, that imagery ability improves with increasing musical expertise and that imagery is most vivid when 
auditory feedback is absent but motor feedback present.  Pianists performed two melodies expressively from the score under 
three feedback conditions: (1) with auditory feedback, (2) without auditory feedback and (3) without auditory or motor 
feedback (imagined performance).  Dynamic and articulation markings (e.g. crescendo, staccato) were periodically introduced 
into the score and pianists indicated verbally whether the marking matched their expressive intentions while continuing to 
play their own interpretation.  MIDI pitch, duration and key velocity data were collected for comparison against baseline 
performances, given under normal feedback conditions using scores devoid of expressive notation.  Preliminary analyses 
suggest that, as expected, expressive profiles are most accurately replicated under normal feedback conditions, but that 
imagery is most vivid in the absence of auditory feedback.  The improvements to online imagery ability expected to co‐occur 
with increasing musical expertise, if observed, will support the idea that enhanced imagery abilities contribute to expert 
musicians’ extraordinary control over expression.  If no relationship between imagery ability and musical expertise is found, 
imagery ability may not be as integral to expert music performance as traditionally presumed. 
 
 
 
B.6 Perceptual grouping: The influence of auditory experience 
 
Keturah Bixby (1)*, Joyce McDonough (2), Betsy Marvin (3) 
 
(1) University of Rochester, Rochester, NY, (2) University of Rochester, Rochester, NY, (3) Eastman School of Music, Rochester, NY 
* = Corresponding author, kbixby@bcs.rochester.edu 
 
Write Perceptual grouping has classically been viewed as a fixed property of the auditory system. This study provides 
additional evidence that it can also be influenced by auditory experience. Native English speakers tend to group tones of 
alternating amplitude as trochees, and tones of alternating duration as iambs, but native speakers of other languages 
sometimes perceive the tone sequences differently. Musicians are a group whose intensive auditory learning experiences have 
likely affected their perceptual groupings, but this iambic‐trochaic grouping principle has not yet been studied in musicians. 
This experiment extends Iversen, Patel & Ohgushi (2008), comparing musicians and non‐musicians on perceptual grouping 
tasks. Non‐musicians listening to tone sequences of alternating durations tend to group tones as iambs, while musicians seem 
to group the sequences more flexibly, switching between iambic and trochaic interpretations. This suggests that perceptual 
grouping can be influenced by even late auditory experience. 
 
 
                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                    92 
 
B.7 Song Style and the Acoustic Vowel Space of Singing 
 
Evan D. Bradley * 
 
University of Delaware, USA 
* = Corresponding author, yevb@udel.edu 
 
Speaking and singing use the same vocal apparatus, but during singing, the articulation of lyrics interacts with musical 
concerns for resonance, expression, and style, which may have consequences for the intelligibility of their linguistic content. 
 A previous study (Bradley, 2010) reported that the acoustic vowel space of female singers undergoes systematic changes 
during singing, consistent with known techniques such as larynx lowering. Specifically, the first and second formant 
frequencies of vowels are lowered for low and front vowels, respectively, resulting in compression of the vowel space and 
overlap of vowel categories. The present study investigated how these changes in the vowel space are moderated by song 
style. Semi‐professional female singers spoke and sung the lyrics of (i) a folk ballad, and (ii) a modern art song setting of 
poetry, which had a faster tempo and dynamic melody, and vowel formants were measured. Differences were found between 
the vowel formants of the two songs, including: (a) for low vowels (/a, ae/), f1 was lower for both songs compared to speech, 
but f1 lowering for the art song was less than for the ballad; (b) for front vowels (/i, e, ae/), f2 was lower for both songs versus 
speech, but f2 lowering for the art song was less than for the ballad. The gradient effects on low and front vowels (a, b) are 
consistent with previously described changes to the vowel space, moderated by song style, and are attributed to 
characteristics of the art song, including style, tempo, and range). 
 
 
 
 
 
 
 
B.8 Orff­Schulwerk approach and flow indicators in Music Education context: A 
preliminary study in Portugal 
  
João Cristiano * & R. Cunha  
 
 
Instituto Politécnico de Bragança, Portugal 
* Corresopnding author, jcrcunha@hotmail.com 
 
This paper presents preliminary results from an ongoing research on Music / Music Pedagogy area, which aims to discuss the 
relation between the Orff‐Schulwerk approach and the development of Musical Thougth / Musical Cognition, based on 
emotional and creative processes. Attempting to verify, analyze and understand this relationship, the empirical process is 
based on the Flow Theory ‐ Optimal Experience developed by Csikszentmihalyi (1988, 1990). On the basis of an experimental 
study, developed in the context of Music Education teaching in general public schools, we verify the existence of different 
“optimal experiences / flow states” boosted by several activities / teaching music strategies. With this purpose, we have 
adopted and adapted the FIMA ‐ Flow Indicators in Musical Activity developed by Custodero (1998, 1999, 2005) and analyzed 
data (audio and video) collected “in loco” in two Music Education classes (5th and 6th grades respectively), during three 
months (first academic term 2010/2011). The analysis of preliminary data obtained using FIMA, enables the validation of the 
experimental study as well as to extract relevant conclusions regarding the variation of “optimal experiences / flow states” in 
the Music Education context. In particular, it is worth mentioning that during this experimental study, most of the "flow 
indicators" (FIMA) were observed in classes developed throughout the Orff‐Schulwerk approach. These observations can 
strengthen the relationship that we believe exists between the Orff‐Schulwerk approach and the development of Musical 
Thougth / Musical Cognition. Moreover, inherent “optimal experiences / flow states” lived in the classroom seem to be 
valuable indicators of emotions, that may be a basic foundation in the relationship between music and emotion (Sloboda and 
Juslin, 2001, 2010), as a pillar of the ongoing investigation. 
 
 
 
 
                                                                                          SMPC 2011 Program and abstracts, Page:       
                                                                                                                                  93 
 
B.9 Movement during Performance: A Hunt for Musical Structure in Postural Sway 
 
Alexander P. Demos*, Till Frank, Topher Logan 
 
Department of Psychology, University of Connecticut, Storrs, USA, Alexander.Demos@uconn.edu 
* = Corresponding author, alexander.demos@gmail.com 
 
The movements of musicians in performance have been difficult to investigate because of their complexity.  However, by using 
techniques from dynamical systems theory created for chaotic systems (i.e., phase‐space reconstruction [PSR] and recurrence 
quantification analysis [RQA]), postural sway movements can be related to the score, and reviewed for similarities within and 
between performances.  The movements of a professional trombonist (third author) and an amateur violinist (first author) 
performing selections from J.S. Bach’s cello suites, were recorded on a Nintendo wii BalanceBoard (35 Hz). To determine the 
dimensionally of the underlying system and reconstruct the phase‐space, the movements underwent two analyses (average 
mutual information index and false nearest neighbor). The resulting four‐dimensional PSR underwent a RQA to locate where 
in the music recurrent patterns of movement occurred. For each performance, the RQA analysis showed recurrence in 
important structural locations, for example, within and across both the beginning of the main phrase, when the pattern 
repeats several bars latter, and at the end of the piece when the musical pattern is reversed. Recurrence in movement patterns 
between the performances assessed by Cross‐RQA, showed recurrence patterns in locations different from those seen within 
the individual performances. The recurrent movement patterns within performances suggest that the musical structure is, in 
part, reflected by the complex swaying movements of the performer. However, the differences between performances suggest 
that the movement patterns were unique to each performance. As in other chaotic systems, the initial conditions of the 
performer affected the pattern of movement across the whole performance, making each one different.  The movements 
within individual performances, on the other hand, reflected the influence of the musical structure.  
 
 
 
 
 
B.10 Developing a Test of Young Children’s Rhythm and Metre Processing Skills 
 
Kathleen M. Einarson (1)*, Laurel J. Trainor (1) 
 
(1) McMaster Institute for Music and the Mind, McMaster University, Hamilton, Canada  
* = Corresponding author, einarsk@mcmaster.ca 
 
Research indicates that adults can perceptually extract the beat from rhythmic sequences and can move in synchrony with 
that beat.  At the same time, adult performance is affected by experience with the particularly hierarchical metrical structure 
of their culture’s music. We examine development in Western kindergarten children, asking (1) whether they show a 
perceptual bias for common Western metres, (2) whether production develops later than perception, and (3) whether 
perception and production abilities are correlated. On each trial of the perception task, 5‐year‐olds are presented with a 
rhythmic sequence in either a four‐beat, five‐beat, or six‐beat metre, where each beat contains one of three patterns: one 
quarter note, two eighth notes, or one quarter rest. The sequence is then repeated, with small alterations on half of the trials. 
In the metric alteration, the second sequence contains one additional beat. In the rhythmic alteration, the notes on one beat 
are replaced by a different pattern (i.e., one of quarter, two eighths or rest). In a computer game, children indicate whether the 
animal producing the second sequence is able to copy exactly the animal producing the first sequence. The production tasks 
consist of recording and analyzing the children’s ability to (1) tap to an auditory beat and (2) tap back simple beat sequences. 
Additionally, we measure motor skills, vocabulary, pre‐reading skills, and working memory in order to examine correlations 
between these abilities and rhythmic perception. Data collection is ongoing. We expect that five‐year‐old children will be most 
sensitive to alterations in sequences whose metre is widespread in their native culture’s music, namely, the four and six‐beat 
metres.  We expect that perception and production skills will be correlated and that general motor skills and working memory 
will account for some individual variation in perception and production abilities. 
 
   
                                                                                            SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    94 
 
B.11 Effects of musical training on speech understanding in noise 
 
Jeremy Federman (1)*, Todd Ricketts (2) 
 
(1, 2) Vanderbilt Bill Wilkerson Center for Otolaryngology and Communication Disorders, Nashville, TN  USA 
* = Corresponding author, jeremy.federman@vanderbilt.edu 
 
Professional musicians have performed better, demonstrated shorter reaction times, and/or exhibited larger cortical 
amplitude responses than non‐musicians on tasks of timbre perception, pitch perception and frequency discrimination, 
contour and interval processing, spatial ability and vocabulary and verbal sequencing.  However, it is currently less 
understood whether the effects of musical training generalize to other important, non‐musical scenarios such as 
understanding speech in noise.  For the current study, primary aims included investigating effects of musical training on 
attention, working memory, and auditory stream segregation as they relate to music perception and speech understanding in 
noise.  Specifically, two groups differentiated by musical training status (professional musicians and non‐musicians) were 
assessed using non‐speech schema‐based auditory stream segregation (Music‐Achievement‐Test [MAT], interleaved melody 
task), working memory capacity (operation span), attention (dichotic listening task), and speech‐based, schema‐based 
auditory stream segregation tasks (Hearing in Noise Test, Connected Speech Test).  Data collected to date were analyzed to 
assess the effects of musical training.  Results showed that musicians significantly outperformed non‐musicians on the non‐
speech schema‐based auditory stream segregation tasks.  Regarding the speech‐based schema based tasks, data showed that 
musicians understand speech in noise better than non‐musicians both when speech and noise were collocated and when 
spatial separation of the speech and noise is provided but signal‐to‐noise ratio is maintained at both ears.  When spatial 
separation and changing SNR localization cues were present, although there was an effect of noise location, there was no effect 
of musical training suggesting that the effect of changing SNR as speech and noise are spatially separated is a robust cue that 
may swamp any group performance differences.  Results are important because they may represent a new way to investigate 
solutions to the number one complaint of individuals with hearing loss even when provided with amplification, namely the 
inability to understand speech in noisy environments. 
 
 
B.12 Differentiating people by their voices: Infants’ perception of voices from their 
own culture and a foreign species  
 
Rayna H. Friendly (1)*, Drew Rendall (2), Laurel J. Trainor (1,3) 
                                                                  
(1) McMaster University, Hamilton, Canada, (2) University of Lethbridge, Lethbridge, Canada, (3) Rotman Research Institute, Toronto, Canada 
* = Corresponding author, friendr@mcmaster.ca  
 
The ability to discriminate and identify people by voice is important for social interaction in humans. It is also important in 
musical contexts, where we can identify singers by their voice qualities. In the present study, we are investigating the role that 
experience plays in the development of voice discrimination. Learning to discriminate a number of musically‐relevant stimuli 
has been shown to follow a common pattern of experientially‐driven perceptual narrowing. For example, 6‐month‐old North 
American (NA) infants can detect mistunings equally well in both native Western scales and foreign Javanese scales. However, 
NA adults are much better at detecting the mistunings in native than Javanese scales. The aim of the current study is to 
investigate whether a similar narrowing pattern occurs for the processing of different vocal timbres. We tested English‐
speaking adults', 6‐month‐olds' and 12‐month‐olds' abilities to discriminate either native‐species (human), or foreign‐species 
(primate) vocalizations. On each trial, adults heard two vocalizations produced by females of one voice category (English‐
speaking humans or Rhesus monkeys) and indicated whether they were produced by the same individual or by two different 
individuals. Infants were tested on the same discriminations using a Conditioned Head Turn procedure in which correct head 
turns to a change in individual were rewarded with animated toy and light displays. Findings show that six‐month‐olds 
discriminated human voices and primate voices equally well. In contrast, 12‐month‐olds and adults discriminated between 
human vocalizations more easily than primate vocalizations. Results suggest that the ability to discriminate individuals by 
voice becomes specialized for the vocal timbres in one's environment between 6 to 12 months of age.  
                                                                                    SMPC 2011 Program and abstracts, Page:       
                                                                                                                            95 
 
B.13 Signs of infants' participatory­ and musical behavior during infant­parent 
music classes 
 
Helga Rut Gudmundsdottir * 
 
University of Iceland, Reykjavík, Iceland 
* = Corresponding author, helgarut@hi.is 
 
Infants are discriminative listeners and learners of music. Remarkably early in life, infants process and internalize the tonal, 
melodic and rhythmic  information presented in music. Musical behavior in infants has been reported in home and nursery 
settings, interacting with parents and peers. Attention has to a lesser extent been directed towards infants’ musical or 
participatory behavior in parent‐infant music classes. Music classes for infants and parents together have become a wide 
spread practice in many countries. The phenomenon of parent‐infant music courses is a relatively new subject of study and 
little is known about the direct effects of such courses. There are some indications that parent‐infant music courses may affect 
the participating parents well‐being. However, the effects of music courses on infants behavior and learning remain to be 
explored. The present study investigated the behavior of 8‐9‐month‐old infants during ten music classes taught over a period 
of 5 weeks. Each class was video taped from different angles. The video recordings were categorized according to the ongoing 
activities. Selected activities were analyzed and coded according to the type of behavior elicited by the infants. Violations from 
the routine of chosen activities were systematically planted into one of the classes towards the end. Infant responses to 
violations were compared to responses during normal conditions. The validity of the categorization of infant responses was 
tested with a panel of independent judges. The study sheds light on the types of behavior found in 8‐9‐month‐old infants 
during parent‐infant music courses. The aim was to identify what constitutes musical behavior and participatory behavior by 
infants in such a setting. 
 
 
B.14 The Effect of Amplitude Envelope on an Audio­Visual Temporal Order 
Judgment Task 
 
Janet Kim*, Michael Schutz 
 
McMaster Institute for Music and the Mind, McMaster University, Hamilton, Canada 
* = Corresponding author, janetkk89@gmail.com 
 
Previous research on a “musical illusion” demonstrates that percussionists strategically use long and short gestures to alter 
the perceived duration of notes performed on the marimba, although these gestures have no effect on the notes’ acoustic 
duration (Schutz & Lipscomb, 2007). This documentation of a strong visual influence on auditory judgments of event duration 
contrasts with previous research demonstrating that vision generally has minimal (if any) influence on temporal tasks, such as 
judging event duration. This exception to previously observed patterns of audio‐visual integration appears to be largely a 
function of amplitude envelope (Schutz, 2009). The possibility that amplitude envelope plays a crucial role in audio‐visual 
integration raises several theoretical issues. Therefore, in order to explore this claim, we designed a temporal order judgment 
(TOJ) task offering an objective measure of the role of amplitude envelope in audio‐visual integration. In this experiment, 
participants were presented with a series of video clips consisting of a tone paired with an image of a single white dot 
(temporal alignment of tone onset and dot onset varied across trials) and were subsequently asked to judge the temporal 
order of the two stimuli (dot vs. tone). Two types of tones were used: “percussive” (exhibiting an exponential decay) and “flat” 
(an abrupt offset; similar to sounds used in auditory psychophysics research). Participants (1) found the task more difficult 
when the dot preceded the tone, and (2) when the tone preceded the dot, the task was significantly more difficult when 
hearing percussive tones compared to flat tones. The results suggest that the perceptual system is more inclined to integrate 
percussive sounds rather than flat sounds with temporally adjacent visual events.  
                                                                                                SMPC 2011 Program and abstracts, Page:       
                                                                                                                                        96 
 
B.15 Motion Capture Study of Gestural­Sonic Objects 
 
Mariusz Kozak (1)*, Kristian Nymoen (2), Rolf Inge Godøy (3) 
 
(1) University of Chicago, Chicago, USA, (2) University of Oslo, Oslo, Norway, (3) University of Oslo, Oslo, Norway 
* = Corresponding author, mkozak@uchicago.edu 
 
In this poster we will present the results of two experiments in which we investigated how different spectral features of sound 
affect listeners’ movements. Current theories argue for the existence of gestural­sonic objects, which are small perceptual sonic 
entities extracted from a continuous stream of sound by our perceptual and cognitive systems, bound up with particular 
embodied responses (Godøy, 2006). These have been studied in tasks involving sound tracing, or drawing what subjects 
perceived to be the “shape” of the sound on a tablet. In the present context we extended this notion to free movement in three‐
dimensional space and employed motion capture technology to observe participants’ gestures in response to sounds. Whereas 
previous work used only single sounds, here we designed sequences according to variable rhythmic complexity. Thus, we 
came up with four different rhythms, ranging from metrically simple to non‐metrical and without isochronous pulses. We 
paired these with three perceptual features of sound (loudness, pitch, and brightness) and two kinds of attack envelopes 
(smooth and sharp), resulting in 24 separate stimuli. We asked participants to move their right arm in synchrony with each 
rhythm, once using jerky/mechanical motions, and once with smooth/continuous movements. Our results indicate that sound 
features had an effect on gesture continuity and the overall quantity of motion, but not on the accuracy of synchrony itself. In 
the second experiment we once again used motion capture to study participants’ gestures, but this time to excerpts taken from 
20th and 21st century repertoire. The purpose was to extend our findings to more ecological settings, and observe gesture 
types employed by listeners in contexts that are timbrally and rhythmically complex. On the basis of previous work on feature 
extraction we observed how sonic and gestural features correlated with one another. 
 
 
 
 
B.16 Interactive Computer Simulation and Perceptual Training for Unconventional 
Emergent Form­bearing Qualities in Music by Ligeti, Carter, and Others 
 
Joshua B. Mailman* 
 
Columbia University, New York, USA 
* = Corresponding author, jmailman@alumni.uchicago.edu 
 
Embracing the notion that metaphors influence reasoning about music (Zbikowski 2002, Spitzer 2004), this study explores a 
computational‐phenomenological approach to perception of musical form driven by dynamic metaphors. Specifically, rather 
than static metaphors (structure, architecture, design, boundary, section) instead, dynamic ones are emphasized (flow, 
process, growth, progression) as more appropriate for modeling musical form in some circumstances. Such models are called 
dynamic form (or more precisely: temporal dynamic form) and arise in a substantial variety of ways, as shown by Mailman 
(2009, 2010). Adopting an interdisciplinary approach, this presentation shows some computational models of qualities that 
convey such dynamic form in unconventional repertoire. Since such models are quantitative, it is plausible that listeners who 
do not spontaneously attend to these by default could be trained to do so, and then subsequently tested on their perception 
and cognition of such form‐bearing flux. Using simulation algorithms developed for this purpose, the presentation offers a 
Max/MSP computer software patch that enables real‐time user manipulation of the intensity of such qualities. Such hands‐on 
control is intended to cultivate sharper perception, cognition, attention, and interest of listeners confronting unconventional 
music. The presentation also offers computer animations of some theorized unconventional emergent qualities, which indeed 
constitute vessels of musical form. 
                                                                                   SMPC 2011 Program and abstracts, Page:       
                                                                                                                           97 
 
B.17 Automatic Imitation of Pitch in Speech but not Song 
 
James Mantell*, Peter Pfordresher, Brian Schafheimer 
 
University at Buffalo, The State University of New York, Buffalo, NY, USA 
* = Corresponding author, jtm29@buffalo.edu 
 
Previous research suggests that pitch may be automatically imitated during speech shadowing tasks, yet little work has 
examined automatic imitation of pitch across speech and song contexts. The current research examined two questions using a 
shadowing/repetition paradigm: (1) do individuals imitate pitch when they are not explicitly instructed to do so and (2) does 
the magnitude of pitch imitation differ when individuals begin their repetitions immediately vs. after a delay? In two studies, 
participants were instructed to clearly repeat the word contents of 48, 3‐5 syllable, word and pitch contour matched speech 
and song target sequences. Participants were instructed to repeat the word contents of the target sequence as soon as possible 
(Study 1, n = 11), or after a three second delay (Study 2, n = 15). We assessed absolute and relative pitch accuracy by 
quantifying mean absolute error and calculating target‐production pitch correlations. We also compared the current data with 
a previously collected dataset that utilized an intentional pitch imitation task (Mantell & Pfordresher, in review). Results 
clearly showed that intentional imitation leads to more accurate pitch productions than shadowing. However, the current 
shadow results revealed greater relative pitch imitation for sentences than melodies, perhaps because participants ignored the 
musical pitch contours, but not the prosodic speech contours. Comparisons across Study 1 and Study 2 suggested that shadow 
delay does not strongly influence pitch production for these word repetition tasks. Finally, although group level analyses 
indicated that the task instructions (repeat words; no mention of pitch) reduced pitch imitation, an analysis of individual 
differences revealed that several participants aligned their pitch productions with the targets as accurately as average 
performance from the intentional pitch imitation task, but only for sentences.  
 
 
 
B.18 Sequence Context Affects Memory Retrieval in Music Performance 
 
Brian Mathias (1)*, Maxwell F. Anderson (1), Caroline Palmer (1), Peter Q. Pfordresher (2) 
 
(1) McGill University, Montreal, Canada (2) University at Buffalo, Buffalo, USA 
* = Corresponding author, brian.mathias@mail.mcgill.ca 
 
Some models of memory retrieval during sequence production propose that sequence elements are prepared prior to 
production in an incremental fashion: Performers’ access to sequence events in short‐term memory is constrained to a subset 
of events.  Serial ordering errors, in which correct sequence events are recalled in incorrect positions, have been cited as 
evidence for simultaneously accessible elements and incrementality of planning in production tasks.  We test predictions from 
a formal model of planning for effects of sequential context on memory retrieval.  Twenty‐six skilled pianists practiced novel 
short musical excerpts that were embedded in small and large melodic contexts until they achieved an error‐free performance, 
and subsequently performed the sequences at fast and moderate tempi, chosen to elicit errors.  Pitch error rates decreased 
across practice, demonstrating learning, and increased with production rate, consistent with speed‐accuracy tradeoffs.  An 
interaction between context and tempo was observed: Long contexts enhanced the effect of tempo on error rates in the 
excerpt, relative to short contexts.  Serial‐ordering errors tended to arise from greater distances in excerpts placed in longer 
contexts than in shorter contexs.  Fits of the data with a contextual model of sequence planning (Palmer & Pfordresher, 2003) 
showed that errors tended to arise from metrically similar events more often in long contexts than in short contexts.  These 
findings provide evidence that longer contexts facilitate the planning of events that are enveloped by the context, by 
strengthening associations among proximal and metrically similar sequential elements.   
 
 
                                                                                              SMPC 2011 Program and abstracts, Page:       
                                                                                                                                      98 
 
B.19 Developing a window on infants’ structure extraction 
 
Jennifer K. Mendoza (1)*, LouAnn Gerken (2), Dare Baldwin (3) 
 
(1) University of Oregon, Eugene, OR, USA (2) University of Arizona, Tucson, AZ, USA (3) University of Oregon, Eugene, OR, USA 
* = Corresponding author, jmendoz4@uoregon.edu 
 
Young infants detect abstract rules in domains as diverse as language, visual displays, and music, expediting their acquisition 
of sophisticated knowledge systems. Recently, Gerken, Balcomb, & Minton (in press) discovered that 17‐month‐olds 
discriminate linguistic stimuli containing a learnable abstract rule versus those lacking a learnable rule: when input 
instantiated a learnable rule, infants listened longer and listening times displayed a detectable peak. We are curious whether 
these listening‐time (L‐time) effects will emerge in infants’ music processing, and whether it is possible to use L‐time to 
diagnose the specific point during learning when infants discover an abstract pattern. In a first study, infants (6‐8 months) 
hear strings of the same set of 3 pure‐tone sequences either containing an abstract rule (ABB) or lacking a rule. If infants listen 
longer and display a listening peak only when the abstract pattern is present, this would provide initial confirmation that L‐
time indexes infants’ rule‐detection. A second study aims to determine whether a listening‐time peak represents the specific 
point at which infants discover abstract structure during their ongoing exposure to the stimulus set. We manipulate the 
frequency range of the stimuli, affecting infants’ ability to distinguish between tones, while keeping the abstract pattern across 
tones constant. Abstract rules should be more challenging to discover when tones are difficult to discriminate. Therefore, 
infants should show a later peak in listening time for low‐ versus high‐frequency sequences. In a third study, infants hear high‐ 
versus low‐frequency stimuli in which no abstract structure is present. If L‐time peaks are specifically related to abstract 
pattern discovery, stimulus differences without implications for patterns should not affect them. Together, these studies have 
the potential to validate a methodology providing an altogether new window on infants’ rule learning as it unfolds during real 
time, capturing infants’ online rule‐discovery for all to see. 
 
 
B.20 The Effect of Visual Stimuli on Music Perception 
 
Jordan Moore (1)*, Christopher Bartlette (2) 
 
(1) Dallas Baptist University, Dallas, TX, USA, (2) Baylor University, Waco, TX, USA 
* = Corresponding author, moore.jordan@me.com 
 
We investigated whether a visual stimulus affects the perception of a musical excerpt's valence.  The hypothesis was that the 
visual stimulus would affect the perception of the music, such that the perceived valence of the music would shift towards the 
visual.  One hundred fifty undergraduate students observed eight stimuli consisting of 1) four visual‐music pairs and 2) four 
music‐alone controls.  The visual and musical clips were categorized by valence (positive/negative) and arousal (high/low); 
these were determined through a pre‐test involving 12 musically trained graduate students, and confirmed through the 
control condition in the main study.  Participants were divided into eight groups, and each group observed a different ordering 
of the stimuli.  Following the presentation of each stimulus, participants selected an adjective from a group that best 
represented their perceived emotion of the music and also completed four 7‐point Likert‐type scales designed to assess 
cognitive perceptions of the musical structure.  In all conditions where the valence of the visual and music agreed, the mean 
valence ratings for the music did not significantly differ from the non‐visual control rating.  When the valences of the visual 
and music disagreed, the result was a significant, symmetrical trend that highlights the effect that arousal has on the 
perception of valence.  Low‐arousal musical stimuli were strongly affected by visual conditions (p<.0001), with mean valence 
ratings shifted towards the visual.  High‐arousal musical stimuli were also affected by visual conditions (p<.05); however, the 
only significant shift in valence ratings occurred with high‐arousal, opposite‐valence visuals. 
                                                                                              SMPC 2011 Program and abstracts, Page:       
                                                                                                                                      99 
 
B.21 An Experiment on Music Tempo Change in Duple and Triple Meter 
 
    Yue Ouyang (1, 2) *  
 
(1) Beijing Technology and Business University, Beijing, China, (2) Communication University of China, Beijing, China  
* = Corresponding author, oyaya713@yahoo.com.cn 
 
In the studies of time interval, researchers have found an interesting phenomenon like it happened in time duration called 
Time Order Error. When the time interval is less than 500ms, people will estimate accelerated tempo changes more accurately, 
when the time interval is more than 700ms, people will estimate slowdown tempo changes more accurately. When the time 
interval is between 500ms and 700ms, various investigations indicated different results. This work was aimed to investigate 
whether people have perceptual bias when the time interval is 600ms, and also attempted to explore the impact of tempo 
changes by certain factors such as change variety, change direction, meter type as well as different music learning experiences. 
This experiment chose two drum beats as downbeat and upbeat, to perform duple and triple meter sequences respectively. In 
each trail there were three standard bars before the variable beat, the variable beat will put ahead or delay for 15%, 10%, 8%, 
5 %, 2% or arrive on time. Participants were divided into two groups according to whether he or she had more than 10 years 
learning of music. The two groups were asked to listen to each trail carefully and make the choice whether the last beat came 
early, delayed, or on time.  The experiment results have shown that people are more sensitive when they feel the beat ahead 
than behind, and this superiority showed especially obvious in triple meter. Furthermore, music students performed better 
than non‐music students in both accuracy and reaction time, especially obvious in triple meter.  It can be inferred that 
professional music training experiences may help on improving people’s ability of judging tempo changes, and this effect acts 
more obviously in triple meter. 
 
 
 
B.22 Listener­defined Rhythmic Timing Deviations in Drum Set Patterns 
 
Brandon Paul (1, 2)*, Yuri Broze (2), Joe Plazak (2) 
 
(1) Department of Speech and Hearing Science, The Ohio State University, Columbus OH USA (2) School of Music, The Ohio State University, Columbus OH USA 
* = Corresponding author, paul.674@osu.edu 
 
Rhythmic Timing Deviations (RTDs) are timing displacements of performed rhythms in relation to a presumptive downbeat or 
tactus. Drummers are thought to employ RTDs in order to create a sense of “feel” or “groove” in certain musical styles. We aim 
to further describe listener preferences by employing a ‘method of adjustment’ paradigm in which participants actively 
manipulate snare drum RTDs to their liking. Stimuli were synthesized from digitally recorded drum samples according to 
notated patterns randomly chosen from a drum set methods book. Each pattern consisted of a four‐beat measure continuously 
looped with an inter‐beat interval 600ms (100bpm). Each of 10 different stimuli appeared three times in randomized order for 
a total of 30 trials. 31 participants were asked to adjust the snare drum onset timing to their preference. Snare drum onsets 
occurred only on metric beats 2 and 4 and could not be independently adjusted. Post‐experiment interviews collected 
information on overall timing preferences, strategies, and adjustment confidence. Results indicate that the majority of 
participants preferred timings that were as synchronous as possible. The mean distance from the synchronous point was 1ms 
ahead of the beat (SD=14ms).  Frequency distribution of raw timing preferences was skewed toward timings ahead of the beat 
(‐.165) and was leptokurtic (.521).  Post hoc analysis revealed no significant difference between subjects grouped by timing 
strategy, preference, or confidence (p > .05). Intra‐ and inter‐rater reliability coefficients of timing preferences to drum set 
patterns were found to be low. We suggest that timing preferences are stable across employed timing strategies and rhythmic 
content in common drum set patterns.  
 
 
                                                                                               SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    100 
 
B.23 The Effects of Altered Auditory Feedback on Speech and Music Production. 
 
Tim A. Pruitt (1)*, Peter Pfordresher (1),  
 
(1) University at Buffalo, The State University of New York, Buffalo New York, United States of America  
* = Corresponding author, tapruit@buffalo.edu 
 
Fluent productions of musical sequences depend on the match between planned actions and auditory feedback. Past research has
revealed distinct effects of different altered auditory feedback (AAF) manipulations. When AAF leads to asynchronies between
perception and action, timing of production is disrupted but accuracy of sequencing is not. Conversely, AAF manipulations of
contents (pitch) disrupt sequencing but not timing. Previous research by Pfordresher and Mantell (submitted) has demonstrated that
such distinct disruptive effects of AAF in manual production (keyboard) are qualitatively similar in vocal production (singing) of
musical sequences. The current research further examines whether similar effects are found for the production of speech, for which
syllables rather than pitches constitute event categories. On different trials, participants either sung melodies or spoke sequences of
nonsense syllables at a prescribed production rate of 600 millisecond inter onset intervals (IOI) while experiencing AAF
manipulations of feedback synchrony or feedback contents (alterations of pitch or of syllabic content). We constructed novel speech
sequences that were structurally isomorphic to previously used melodies, by matching each pitch class in a melody to a unique
consonant-vowel (CV) nonsense word. Preliminary results suggest that the dissociation in sequencing and timing generalize across
both the speech and musical production domains. The trend in mean IOIs across feedback conditions demonstrate the largest slowing
of production during the asynchronous feedback condition. Likewise, the expected trend emerged with respect to error rates, as
participants were least accurate under the content shifted feedback condition. These qualitatively similar effects suggest that action
and perception associations are guided by abstract representations that may be similarly organized across music and language
domains.
 
 
B.24 Does Note Spacing Play Any Role in Music Reading? 
Bruno H. Repp (1)*, Keturah Bixby (2), Evan Zhao (3) 
 
(1) Haskins Laboratories, New Haven, CT, (2) University of Rochester, NY, (3) Yale University, New Haven, CT 
* = Corresponding author, repp@haskins.yale.edu 
 
In  standard  notation,  long  notes  occupy  more  space  than  do  short  notes.  Violations  of  this  rule  look  odd  and  may  cause 
performance errors. Musically illiterate persons believe note spacing conveys tempo (Tan et al., 2009). Can note spacing affect 
musical  behavior?  In  Experiment  1,  we  investigated  whether  global  note  spacing  influences  tempo  choice.  Skilled  pianists 
were  asked  to  sight‐read  20  unfamiliar  excerpts  (with  tempo  and  expression  marks  deleted)  and  find  the  most  appropriate 
tempo for each. The excerpts were printed with either wide or narrow note spacing, in a counterbalanced design. The pianists 
tended to play slower when the notation was widely spaced, but this effect was only marginally significant. In Experiment 2, 
we  tested  whether  a  local  change  in  note  spacing  can  influence  perceptual  judgments.  Musicians  judged  whether  a  rhythm 
they heard matched notation they saw. The notation either did or did not contain a leftward shifted note, and the preceding 
note in the rhythm either was or was not shortened. Our hypothesis was that the mismatch due to the shortened note might be 
harder to detect in the presence of a shifted note. However, no significant effect emerged. In Experiment 3, musicians again 
judged  whether  an  auditory  rhythm  matched  notation  but  were  required  to  base  their  judgment  on  note  spacing  while 
ignoring note symbols. Symbols were either congruent with spacing, incongruent (reversed in order), or absent (stems only). 
The rhythms either did or did not contain a reversed pair of notes. Participants were able to use note spacing when required to 
do so but unable to ignore note symbols entirely: Both match and mismatch judgments were more accurate with congruent 
than  with  incongruent  symbols.  In  conclusion,  while  incidental  effects  of  note  spacing  in  musical  tasks  remain  elusive,  it  is 
clear that the temporal information conveyed by note spacing is readily accessible. 
 
 
                                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                                  101 
 
B.25 Bayesian modelling of time interval perception 
 
Ken‐ichi Sawai (1)*, Yoshiyuki Sato (2), Kazuyuki Aihara (3,1) 
 
(1) Graduate School of Information Science and Technology, University of Tokyo, Tokyo, Japan, (2) Graduate School of Information Systems, University of Electro­
Communications, Tokyo, Japan, (3) Institute of Industrial Science, University of Tokyo, Tokyo, Japan 
* = Corresponding author, ken1@sat.t.u‐tokyo.ac.jp 
 
Perception of the time interval between sounds is important to music perception. Especially, hearing three successive sounds 
with short time intervals is one of the most fundamental situations of temporal pattern perception. As for hearing three rapid 
sounds, it has been known that our brain sometimes misestimates the latter interval depending on the relative length of the 
two intervals. Concretely, underestimation of the second interval occurs if the first interval is a little shorter or much longer 
than the second, and overestimation of the second occurs if the first is a little longer or much shorter. However, any model has 
not comprehensively succeeded to explain the misperception of auditory time intervals. We propose a model of auditory 
temporal pattern perception using the Bayesian inference framework. Our Bayesian model assumes that our neural system 
cannot observe true time intervals, but only intervals including noise. Bayesian inference enables the observer to effectively 
infer the true time intervals by combining an observation with a prior knowledge. We formulate the prior knowledge for 
temporal patterns as follows, assuming that the observer solves a source identification problem. First, our brain infers from 
the three successive sounds whether or not each pair of two neighboring sounds comes from the same source. Next, our brain 
assumes that the sounds from the same source are temporally close and isochronous. Then, we combine the prior knowledge 
into one prior distribution. We conducted a numerical simulation and showed that our model can qualitatively replicate the 
misperception of auditory time intervals. This result suggests that our brain makes a Bayesian inference to estimate time 
intervals, and that source identification plays an important role for the inference. 
 
 
 
B.26 Linguistic Influences on Rhythmic Preference in the Music of Bartok 
 
Andrew Snow, Heather Chan* 
 
Eastman School of Music, Rochester, New York, USA 
* = Corresponding author, heather.y.chan@gmail.com 
  
 This study examines the relationship of rhythms between Hungarian folksongs and compositions by Bela Bartok. Bartok made 
extensive tours of Hungary and surrounding regions during which he copiously transcribed the folksongs he encountered. His 
purpose was to collect these melodies into a bank that he could draw upon to write in a more authentic Hungarian style. The 
motivation of this study is the observation that folk melodies are likely to be intentionally based on rhythms existing in the 
associated language, since they were likely originally sung with texts, while the absence of direct quotation may remove that 
rhythmic bias from other Bartok compositions. By using the normalized pairwise variability index (nPVI) previously used by 
Ani Patel (2006), we have quantitatively compared rhythm in the primary themes of folksongs and Bartok's original 
compositions and found that no statistically significant difference exists between the two groups. Perhaps this could be taken 
as evidence that Bartok successfully integrated a Hungarian “sound” even in works that do not use original folk melodies. On 
the other hand, the large variability of nPVI among the themes in both categories suggests that nPVI may not be correlated 
with the native language of composers and may be too unstable for this analytical application, which could have implications 
for other studies using nPVI to study rhythm. Our repertoire includes the Romanian Folk Dances, the Violin Rhapsodies, the 
Forty‐Four Violin Duos, the String Quartets, Concerto for Orchestra, and Concerto for Viola. Themes were extracted with the 
aid of Vera Lampert's source catalog of folk music in Bartok's compositions (2008). 
  
                                                                                          SMPC 2011 Program and abstracts, Page:       
                                                                                                                               102 
 
 
B.27 Infants prefer singers of familiar songs 
 
Gaye Soley and Elizabeth Spelke 
 
Department of Psychology, Harvard University, Cambridge, Massachusetts, USA 
* = Corresponding author,  gayesoley@gmail.com 
 
Infants prefer familiar structures in many domains, including faces, language and music [1‐3]. Native language preferences 
lead to preferences for speakers of that language [4], suggesting that these preferences may influence infants’ social 
relationships. Given that after exposure young infants can recognize familiar music [5‐6], we investigated the role of song 
familiarity and melodic structure in guiding 5‐month‐olds’ visual preferences for singers, using methods that previously 
revealed preferences for native‐language speakers. Forty‐eight infants saw videos of two women, appearing in sequence and 
singing one of three songs with the same lyrics and rhythm.  One song was familiar to all children (according to parental 
report); the other two were unfamiliar, and either tonal or atonal. Infants first saw a silent baseline, where the two women 
appeared side‐by‐side, smiling to the infant. Then, they saw six familiarization trials, where the women appeared in 
alternation and sang for 6 trials the song version corresponding to the condition; this was followed by a silent test trial 
identical to the baseline. The lateral positions of the women, the order of the presentation and pairings of women to melodies 
were counterbalanced across infants. We compared the percentages of looking time to singers of familiar songs during 
baseline and test, respectively, using two‐tailed, paired t‐tests. Infants preferred singers of the familiar song to singers of both 
unfamiliar songs (atonal unfamiliar song: p<.01; tonal unfamiliar song: p=.06). In contrast, infants did not prefer singers of the 
tonal unfamiliar song to singers of the atonal one (p >.5). These results suggest that music modulates 5 month‐old infants’ 
visual preferences for the singers, and that familiarity with specific songs rather than musical conventions drive these 
preferences in early infancy. These findings indicate that music‐based social preferences observed later in later life might 
originate in infancy. 
 
 
B.28 Learning to sing a new song: Effects of native  English or  Chinese language on 
learning an unfamiliar tonal melody having  English or Chinese lyrics  
 
Leah C. Stevenson, Bing‐Yi Pan, Jonathan Lane, & Annabel J. Cohen* 
 
AIRS SSHRC MCRI, Department of  Psychology, University of Prince Edward Island, Charlottetown, PE  Canada C1A 4P3 
* = Corresponding author, Acohen@upei.ca 
 
What is the role of language familiarity in learning to sing a new song? In a previous study, the AIRS Short Battery of Tests of 
Singing Skills was administered to native Chinese and native English‐speakers attending a Canadian University [McIver, A. J., 
Lamarche, A. M‐J., & Cohen, A. J., “Non‐native acquisition of lyrics and melody of an unfamiliar song,” 2010 CSBBCS Annual 
Meeting, abstract, Can. J. of Expt. Psychology, 64, 296‐297 (2010)]. Effects of native language arose in the test component 
entailing learning a new song. The structure of the 23‐note tonal melody of the song paralleled the phrase and syllabic 
structure of the lyrics. Lyrics were in English. Native English speakers sang both melody and lyrics more accurately than native 
Chinese. The present new study examines performance of the two language groups as before, but, in the task requiring 
learning the new song, half of the participants of each language group receives Chinese lyrics instead of English. The aim is to 
show that the deficit to melody acquisition arising from non‐native lyrics, as found for native Chinese speakers, generalizes to 
native English speakers. However, because the native English speakers know less of Chinese language than native Chinese 
students  know of  English, a larger deficit for  native English speakers is expected. The pattern of the results may reveal the 
separate influences of general cognitive load  versus  demand on a syntactic  integrative resource shared by melody and lyrics 
(SSIRH) [Patel, A.,“Music, language and the brain”. NY: Oxford (2008)]. The design entails 32 participants, 16 native English 
speaking and 16 Chinese. Other components of the test battery (e.g., reproducing musical elements,  singing a familiar song, 
creating both song and story, completing a song) provide additional context. The results  may also apply to teaching songs in 
multicultural settings. [supported by SSHRC] 
 
                                                                                    SMPC 2011 Program and abstracts, Page:       
                                                                                                                         103 
 
B.29 Exploring Real­time Adjustments to Changes in Acoustic Conditions in Artistic 
Piano Performance 
  
Victoria Tzotzkova (1)* 
  
(1) Computer Music Center, Columbia University, USA 
* = Corresponding author, vdt3@columbia.edu 
  
In an essay titled “Coping with pianos”, Alfred Brendel assures us that “anyone who has ever traveled with a piano knows that 
the same instrument not only sounds different in different halls, it even seems to feel different in its mechanism…” Even more 
strikingly, this difference in the feel of the instrument manifests itself in the same space and on the same day between the 
afternoon rehearsal and the evening performance. On Brendel’s account, the acoustic difference the presence of an audience 
makes figures into the performance experience of the pianist in significant ways, impacting even the experience of an 
intimately familiar instrument. The present research focuses on the role of listening in acts of performance, aiming to open to 
investigation the ways that pianists may adjust their actions in performance in order to obtain a desired sort of sound under 
particular acoustic circumstances. It further aims to complicate the idea of timbre in piano performance, seeking to move 
towards a conception of timbre as a range of possibilities available to the pianist. The project focuses in turn on timbral 
variability of piano sonorities by comparing timbral profiles of corresponding chords in Morton Feldman’s Last Pieces 
performed under identical studio conditions, and on interview responses of participating performers asked to perform an 
excerpt from Last Pieces three times under subtly enhanced studio acoustic conditions. The project is part of a larger research 
program focusing on timbral (coloristic) aspects of classical music piano performance. Such aspects of performance are 
considered in line with Augoyard and Torgue’s definition of “sonic effects” as phenomena which incorporate both “physical 
and human dimensions of sound” (Augoyard and Torgue 2006). The coupled approach of this project aims to contribute to 
understanding the ways in which physical and human dimensions of sound interact in experiences of artistic music 
performance. 
  
 
B.30 The role of continuous motion in audio­visual integration 
 
Jonathan Vaisberg*, Michael Schutz 
 
McMaster Institute for Music and the Mind, McMaster University, Hamilton, Canada 
* = Corresponding author, vaisbejm@mcmaster.ca 
 
The present study explores the role of motion in audio‐visual integration, with the goal of better understanding the ways in 
which visual information plays a role in the musical experience. Much research on audiovisual interactions relies on the use of 
visual stimuli that are either static, or exhibit apparent motion (rather than continuous motion). However, when watching a 
musical  performance,  we  are  generally  seeing  continuous  motion.  Therefore,  we  are  investigating  the  degree  to  which 
movement  in  visual  stimuli  affects  the  ways  in  which  it  alters  our  perception  of  concurrent  auditory  information.  Previous 
work  has  shown  that  a  single  moving  dot  mimicking  an  impact  gesture is  capable  of  altering  the  perceived  duration  of 
percussive sounds (Schutz & Kubovy, 2009).  In the context of this paradigm, longer gestures induced participants to perceive 
longer tone durations. This study builds on that work by directly comparing the visual influence of two classes of kinds of dots: 
dynamic (based on the movements of a percussionist striking an instrument) and static (single dots turning on and off without 
moving). In this experiment, participants are asked to indicate perceived duration ratings for a series of sounds paired with 
long and short version of each of these two classes of visual stimuli to explore their relative influence on the perception of tone 
duration. We are currently collecting data, and anticipate that the results will shed light on the degree to which motion plays a 
role in audio‐visual integration.  We believe the outcome of this study will be useful in applying the vast literature on audio‐
visual integration to questions regarding the role of visual information in music perception.  
 
 
                                                                                             SMPC 2011 Program and abstracts, Page:       
                                                                                                                                  104 
 
B.31 The Effect of Rhythmic Distortion on Melody Recognition 
 
David Weigl (1)*, Catherine Guastavino (1), Daniel J. Levitin (2, 3) 
 
(1) School of Information Studies, McGill University, Montréal, Québec, Canada (2) Department of Psychology, McGill University, Montréal, Québec, Canada, (3) 
Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT), McGill University, Montréal, Québec, Canada 
* = Corresponding author, david.weigl@mail.mcgill.ca 
 
The role of rhythm remains underexplored in research on melody recognition. This study investigated the cue validity of 
rhythmic information for melody recognition by asking participants to identify melodies with distorted rhythm while leaving 
pitch information intact. The stimuli consisted of excerpts from 46 melodies selected in previous research: 422 undergraduate 
psychology students were asked to list melodies they would find immediately recognizable; melodies with the highest inter‐
subject agreement were selected. The set included nursery rhymes and popular songs. In this study, 50 psychology 
undergraduates listened to MIDI files representing melodies in 3 rhythmic conditions: shuffled, where all note durations were 
randomly reassigned among the notes of the melody; randomized, where all notes were assigned a random duration in midi 
ticks, constrained by the shortest and longest durations present in each melody; and undistorted, the control case. A similarity 
metric based on chronotonic distance was used to quantify the degree of rhythmic distortion in the manipulated conditions. 
This research is currently underway; we anticipate completing data collection and analysis well before the conference date. 
We hypothesize a highly significant drop in recognition under rhythmic distortion; furthermore, we expect the drop in 
recognition to correlate with the chronotonic distance between original and distorted versions. This study contributes to the 
ongoing discussion on the role of individual musical facets in melody recognition. Results inform a larger project exploring the 
effects of temporal, structural, pitch‐related and polyphonic musical facets and their interactions on the recognition of familiar 
tunes. By systematically manipulating these facets, we hope to discover minimal information requirements for melody 
recognition. 
 
 
 
B.32 Perception of entrainment in apes (pan paniscus) 
 
 Philip Wingfield (1)*, Patricia Gray (2) 
 
(1) University of North Carolina­Greensboro, Greensboro NC, USA, (2) Univeristy of North Carolina­Greensboro, Greensboro NC,  
* = Corresponding author, e: ptwingfi@uncg.edu 
 
 
We hypothesize that given that the genetic evidence indicates that apes and humans are closely‐related species (98.797% of 
human DNA); and given evidence of similar neural structures and patterns of neural asymmetry in both humans and non‐
human apes that suggest the organization of cortical areas critical to temporal organization for music‐making and language 
abilities in humans are physically available for the expression of these behaviors in non‐human apes; and that Bonobos and 
humans claim a common ancestor (~ 6 million years ago) before taking separate evolutionary paths; and that bonobos share 
many biological and cultural elements with humans; and that bonobos have complex cultural processes; and given the right 
context in which cultures develop, there can be basic elements of musicality manifest in bonobo communication.  This 
research explores integrative auditory and cognitive capacities of bonobo apes (Pan paniscus) and probes possible 
relationships to music evolution in humans.  Looking at data collected between highly enculturated bonobos and English‐
speaking humans, we analyzed timings of exchanges, rhythmicity of turn‐taking, and rhythmicity of conversational English 
with ape interjections (‘peeps’) as integrated sequences. Research results are in progress but suggest that temporal 
correlations of bonobo vocalized interjections with English speakers are statistically significant. The research project bears 
relevance to: 1) language and cognition in apes and humans; 2) auditory communication perception via speech prosody, non‐
speech vocalizations, and music; and 3) the evolution of temporally moderated interactive behaviors. The specific objectives of 
the research are to analyze data of bonobo/human interactions based on vocalizations/speech and to determine if bonobos 
make temporally correlated vocalizations to English‐speaking humans.  

 
                                                                                               SMPC 2011 Program and abstracts, Page:       
                                                                                                                                    105 
 
B.33 Transfer Effects in the Vocal Imitation of Speech and Song 
 
Matthew G. Wisniewski (1)*, James T. Mantell (1), Peter Q. Pfordresher (1) 
 
(1) University at Buffalo, The State University of New York, Buffalo, The United States of America 
* = Corresponding author, mgw@buffalo.edu 
 
The imitation of an auditory stimulus, whether it is speech or song, requires a participant to perceive the sound and translate 
it into vocal production.  This suggests that similar mechanisms are used to imitate stimuli from the speech and song domains. 
There is evidence, however, for independent imitation mechanisms. For instance, some people (amusics) have processing 
impairments specific to stimuli from the song domain (Peretz & Coltheart, 2003).  In this study we asked the question: How 
does practice imitating a stimulus from one domain impact the ability to imitate a stimulus from the other?  In addition, we 
investigated how changes to stimulus contour and word information affected performance on non‐practiced stimuli. 
 Participants practiced imitating speech or song stimuli and were then transferred to stimuli that were the same or different in 
domain, contour, or text.  Analyses of performance on transfer trials shows that the effects of switching contour are beneficial 
for imitation accuracy if domain is also switched, but not beneficial if domain in the transfer trials is the same as practice. 
 Results suggest that the mechanisms for imitating speech and song are not independent and that contour is not processed 
independently of stimulus domain. 
 
 
 
 
 
B.34 The single voice in the choral voice: How the singers in a choir cooperate 
musically 
                                                                                    
Sverker Zadig 
 
Örebro University, Örebro, Sweden  
* = Corresponding author, sverker.zadig@telia.com  
 
As a choral conductor and also as a choral singer I have the experience of formal and informal leaders in the choral voice, and I 
became interested to study what really goes on between the singers. This paper describes what can happen between the 
singers in a choral voice and how the individuals in a choir differ in taking initiative and acting in leading roles. I have done 
qualitative interview studies with conductors and singers, and also recording studies of the individuals in a choral voice. These 
recordings have been made in a Swedish gymnasium school. With close up microphones on headsets and by multi track 
recordings it have been possible through an analyzing program to watch graphically exactly how each individual sings and 
also to compare the singers with each other. The recording sessions have been done in following choir rehearsals, and with a 
simultaneous video recording to be able to also take notice on eventual visual signs between the singers. Analyze have been 
done with printouts of the same sequence of the music with the recordings of all voices in the same choral voice. It is possible 
to graphically view differences in attacks and intonation, and also to notice when someone is ahead and “pulling” others to 
follow. This leading role can be both positive and negative, a confident but not so good singer can unfortunately bring along 
other singers to take wrong steps in the music. My vision is to find, improve and develop a positive leadership from good 
singers to the rest of the choir, and also to find if and how the seating in the choir can affect the singing.
