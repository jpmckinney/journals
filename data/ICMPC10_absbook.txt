ICMPC 10                                                         Special Sessions


Keynotes

Organization Processes in Music Perception
Jun-ichi Abe; Hokkaido University, Japan
K-1
Some sequences of tones are perceived as musical events, i.e., as coherent melodies,
whereas others are not. Why is this? Needless to say, listeners’ minds are responsible
in some way. But how do our brain mechanisms make such distinctions? For listeners
— whether they may be music experts (musicians) or novices (non-musicians), and
regardless of their musical culture, — two organizational processes are essential
for the perception of a stimulus tone sequence as a melody: one process involves
metrical organization and the other is a process of tonal organization of a stimulus
tone sequence. If metrical organization is successfully carried out in accordance
with a listener’s metrical schema, the result is that a coherent temporal structure
is assigned to a stimulus tone sequence; psychological time units such as “beat”
and “meter” are perceived. Likewise, if a tonal organization is successfully executed
in accordance with a listener’s tonal schema, then a coherent tonal relationship is
assigned that relates constituent pitches within a sequence of tones. In turn, this
confers a sense of “key” or “tonality” upon the sequence. Conversely, if both of
these two kinds of organization are not successfully carried out, then tones would
be perceived as a random-like sequence or a jumbled sequence of notes. In this
talk, I discuss some general properties of these organization processes in music
perception.

Japanese Voices — A Video Archive of Singing Styles and
Techniques in the Japanese Language
Ichiro Nakayama; Osaka University of Arts, Japan
K-2
This is an introduction of Japanese Voices — A Video Archive of Singing Styles and
Techniques in the Japanese Language (Nihongo o Uta, Uta, Utau ᮹ᴀ䁲ȧ℠・૘・䃵Ǜ ),
a DVD set about Japanese vocal music that I recently published. The feature of this
archive is that the performers from both traditional, contemporary and western style
genres of Japanese song sang a set phrase in their own respective métier. I’d like to
give an overview on the characteristics of Japanese vocal music, referring to those
scenes from the DVD set.

The Origins of Rhythm in Movement
Laurel J. Trainor; McMaster University, Canada
K-3
We tend to think of music as based in the auditory modality, but we see the
movements of musicians, we play our instruments using our sense of touch and
proprioception, and we feel the rhythm in our bodies. In a series of studies, we have
shown that body movement shapes how we hear rhythm. In particular, moving on
every second versus on every third beat of a rhythm pattern with more than one
possible metrical interpretation disambiguates the perceived meter. The vestibular
system appears to be crucial for this interaction as diﬀerential galvanic stimulation
of the vestibular nerve alone inﬂuences metrical interpretation. Young infants are
not able to self-locomote, but they do experience considerable daily correlated
multisensory input as parents walk with them and rock them while singing and
talking. Movement aﬀects metrical interpretation in infants similarly as in adults,
suggesting that the movement and auditory systems wire together at an early age.
Furthermore, infants enrolled in music classes for babies and parents that emphasize
moving to musical rhythms show an earlier bias for the metrical structures of the
rhythms of their culture. This suggests that experience is central in the development
of auditory-movement pathways in the brain.

The ICMPC Series, and Some Current Research on Music
Perception and Cognition
Diana Deutsch; University of California at San Diego, USA
K-4
This paper is in three main parts. The ﬁrst part describes the circumstances sur-
rounding the founding of the ICMPC series in the late 1980s, and the ﬁrst two ICMPC


                                          5
Special Sessions                                                          ICMPC 10

conferences, which were held in Kyoto and in Los Angeles. The second part describes
some recent research by the author and collaborators. The ﬁrst body of research
concerns a new illusion, in which a spoken phrase is perceptually transformed to
be heard as song, just through simple repetition. The second body of research
concerns absolute pitch. It is argued, based both on published work and also a
newly completed study, that exposure to tone language during the critical period for
speech strongly inﬂuences the capacity to acquire absolute pitch for musical tones.
The third body of research concerns a new illusion of pitch circularity, in which this
is achieved with a bank of tones that each comprise a full harmonic series. This both
extends the theoretical scope of pitch circularity and also has implications for the
development of new music. Finally, developments in the ﬁeld of music perception
and cognition since the inception of the ICMPC series are considered.


3PM3-R01 : Young Researcher Award Winners’ Papers
Room 1, 17:30 – 19:00 Wednesday 27 August 2008, Oral session

Eﬀect of Expressive Intent, Performer Expertise, and Listener
Expertise on the Perception of Artistic Individuality in Organ
Performance
Bruno Gingras, Tamara Lagrandeur-Ponce, Bruno L. Giordano,
Stephen McAdams; McGill University, Canada
3PM3-R01-1
This study investigated the perception of artistic individuality in organ performance.
Six organists, three of whom were prize-winners at national competitions, each
recorded two “mechanical” and two expressive interpretations of a chorale setting
by Scheidt (1587–1654). In a subsequent perception experiment, twenty non-
musicians and twenty musicians listened to these recordings and grouped together
performances they thought had been played by the same organist. Twenty-eight
participants (70%) performed signiﬁcantly above chance level, demonstrating that
most listeners can recognize speciﬁc performers even on an instrument with a limited
range of expressive parameters such as the organ. There was no signiﬁcant diﬀerence
in the performance of musicians and non-musicians. Participants were less likely to
group together expressive performances from diﬀerent performers than mechanical
ones. Participants’ performance was signiﬁcantly higher for prize-winning organists
than for non-winners, suggesting that the performers’ level of expertise is linked to
the perception of artistic individuality.

Stimulating Music: Combining Singing with Brain Stimulation to
Help Stroke Victims Recover Speech
Bradley W. Vines 1 , Andrea C. Norton 2 , Gottfried Schlaug 2 ;
1
  University of British Columbia, Canada; 2 Harvard University, USA
3PM3-R01-2
It is strange to think that singing might help a stroke victim speak again, but this is
the secret of Melodic Intonation Therapy (MIT), a speech therapy that emphasizes
musical aspects of language. We investigated the potential for transcranial Direct
Current Stimulation (tDCS) to augment the beneﬁts of MIT for Broca’s aphasia
patients. tDCS is a non-invasive technique that modulates neural activity. We used
tDCS to stimulate the right inferior frontal gyrus, which is thought to contribute
to singing, and to recovery from aphasia. The stimulation coincided with an MIT
session, conducted by a trained therapist. Participants’ language ﬂuency showed
greater improvement with real tDCS + MIT, compared to sham tDCS + MIT. These
results provide evidence that combining tDCS with MIT may enhance activity in a
sensorimotor network for articulation in the right hemisphere, to compensate for
damaged left-hemisphere language centers.




                                          6
ICMPC 10                                                           Special Sessions


5AM1-S01 : APSCOM3 General Meeting & Symposium:
The Latest Research Trends in Asia-Paciﬁc Regions
Space 1, 9:15 – 10:45 Friday 29 August 2008, Oral session

The Latest Research Trends in Asia-Paciﬁc Region
Sun-Hee Chang 1 , Catherine Stevens 2 , Emery Schubert 3 , Shin-ichiro
Iwamiya 4 , Kyungil Kim 5 , Qian Zhang 6 , Shibin Zhou 7 , Eddy Chong 8 ;
1
  Seoul National University, Korea; 2 University of Western Sydney,
Australia; 3 University of New South Wales, Australia; 4 Kyushu
University, Japan; 5 Ajou University, Korea; 6 Central Conservatory of
Music, China; 7 Capital Normal University, China; 8 Nanyang
Technological University, Singapore
APSCOM3
The Symposium is organized as a plenary session for the 3rd triennial International
Conference of the Asia-Paciﬁc Society for the Cognitive Sciences of Music (APSCOM3),
which is held as a joint meeting with ICMPC10. The purpose of this symposium
is to discuss the research trends and progress made in APSCOM member societies
since ASPCOM2, which was held in 2005 in Seoul (Korea). In particular, the sym-
posium provides an opportunity for researchers in the Asia-Paciﬁc region to meet
one another especially those who are new to ICMPC and ASPCOM meetings. We
welcome representatives from the Society for Music Psychology of China (SMPC)
who will outline research trends and SMPC activities. SMPC will review the ﬁeld of
music psychology in China evolving in the context of music education curricular
reform and explaining its characteristic rapport with the ancient Chinese thoughts of
music. SMPC is exploring an international cooperation network to meet their needs
caused by the rapid expansion of the ﬁeld in China. A Singaporean delegate will
also join to share his research interests with the society members as an observer.
Representatives from the Australian Music & Psychology Society (AMPS), the Japanese
Society for Music Perception and Cognition (JSMPC), and the Korean Society for
Music Perception and Cognition (KSMPC), will report on current research activities
and research groups and future directions within their own society as well as in
international communities.

The Construction and Development of Music Psychology in China
Qian Zhang 1 , Shibin Zhou 2 ; 1 Central Conservatory of Music, China;
2
  Capital Normal University, China
5AM1-S01-01
The education of music psychology began along with the introduction of the concept
into China. According to the education of music psychology in China in the past 20
years, a system of the education has been founded, master and doctor degrees are
oﬀered in some universities. The research ﬁelds are enlarging and probing deeply.
On the solid foundation of Chinese traditional music-psychological ideas, new
technique and achievements are more and more combined to enhance the research
level of the subject.
The gap between the education home and aboard exists in the structure of the knowl-
edge, which needs the cooperation of the scholars from all the relative ﬁelds and
explores the issues in music psychology. There will be a bright future of music psy-
chology education.

Music Perception and Cognition Research in Australia
Catherine Stevens 1 , Emery Schubert 2 ; 1 University of Western Sydney,
Australia; 2 University of New South Wales, Australia
5AM1-S01-02
This paper sketches activities concerning music perception and cognition research in
Australia in the three years that have elapsed since the successful APSCOM 2 meeting
in Seoul in 2005. We outline activities at society, conference and research group level.
The recipients of the AMPS Graduate Student International Conference Travel Support
Scheme are noted. The competitive Endeavour International Postgraduate Research
Scholarship (EIPRS) scheme is described as a potential source of support for students
in the Asia-Paciﬁc region considering undertaking graduate research in Australia.



                                           7
Special Sessions                                                          ICMPC 10

Research, Activity, and People in Music Perception and Cognition
in Korea
Kyungil Kim; Ajou University, Korea
5AM1-S01-03
The purpose of this paper is to review the research related to music perception and
cognition in Korea. To this end, the past, current, and future of relevant Korean re-
search, activities, and researchers are outlined. Speciﬁcally, the research before the
creation of the Korean Society of Music Perception and Cognition (KSMPC), the orga-
nization of the KSMPC and its activities and research thus far, and, ﬁnally, the future
directions of the KSMPC are discussed. Introduction of the members to the KSMPC
(Tables 1 and 2) provides insight into the research domains and historically represen-
tative activities in Korea.

The Japanese Society for Music Perception and Cognition
Shin-ichiro Iwamiya; Kyushu University, Japan
5AM1-S01-04
The Japanese Society for Music Perception and Cognition was founded in 1988. Since
then, the SMPC has continued its eﬀorts for collaborative exploration and develop-
ment of this research ﬁeld. The JSMPC organized the ﬁrst ICMPC in Kyoto in1989.
This paper includes brief introduction of history, activities and future of the JSMPC.
As a recent topic of the JSMPC, a special issue of the Journal of Music Perception and
Cognition (the oﬃcial journal of the JSMPC) focused on Sound Design has been pub-
lished. Now, the JSMPC is to host the tenth ICMPC in Sapporo in 2008.

Music Psychology Research in Singapore: A Report
Eddy Chong; Nanyang Technological University, Singapore
5AM1-S01-05
This paper presents the state of research in music psychology in Singapore. A brief
history of the two tertiary institutions which could have embarked on this area of
research in the last four decades — the period spanning Singapore’s independence
as a nation — makes clear the reasons for the nascent state of development. The
handful of relevant research theses and journal publications are then listed against
this historical backdrop.




                                          8
ICMPC 10                                                                   Symposia


1PM1-R01 : Symposium 1 : Music and Health: Empirical
Investigations and Theoretical Constructs
Room 1, 13:00 – 15:00 Monday 25 August 2007, Oral session

Music and Health: Empirical Investigations and Theoretical
Constructs
Laura Mitchell 1 , Raymond MacDonald 1 , Gianna Cassidy 1 , Julie C.
De Simone 1 , Douglas Lonie 2 , Eugenia Costa-Giomi 3 ; 1 Glasgow
Caledonian University, UK; 2 MRC Social and Public Health Sciences
Unit, UK; 3 University of Texas at Austin, USA
SUMMARY-1
Aligned with the growing recognition of the potential beneﬁts of music interventions
on heath parameters are considerable advances in research that investigates the
beneﬁts of music on various health measures (Wosch and Wigram, 2007). This
heightened interest is driven by both developments in research methodology (quali-
tative and quantitative) and the ever-increasing availability of music in everyday life
(Pothoulaki, MacDonald and Flowers, 2006). However, despite this increasing interest
there remains much to learn, from a health context, about the processes involved
and the potential outcomes of music interventions (Mitchell, MacDonald, Serpell and
Knussen, 2007).
This symposium presents a series of empirical research studies relating to the
eﬀects of both musical participation and music listening on health parameters. A
number of important issues provide overarching themes for this symposium. For
example the importance of musical preferences and the complex relationship we
have with favourite music is developed in a number of preservations, while real-world
experiences of music as a part of everyday activities is also tackled. The symposium
also presents both quantitative and qualitative methodologies and mixed methods
paradigms in bringing this research forward.
Raymond MacDonald will begin the symposium with a discussion of the key themes
and recent literature. Laura Mitchell will discuss the growing literature on music
listening for pain relief, and in particular whether cognitive style may be an important
individual diﬀerence in the eﬃcacy of and reactions to this intervention. Gianna
Cassidy will then look at the beneﬁts of music listening on health parameters and
experience during interactive game play, comparing physiological measures relating
to health and wellbeing when gaming is accompanied by soundtrack music or
self-selected music. Julie De Simone will present recent ﬁndings in collaboration with
the music charity Polyphony on the eﬀects of musical activity on communication
and cognitive functioning of dementia suﬀerers. Douglas Lonie will then discuss
his work on the relationship between music listening and emotional health in
young adults, which encompasses both psychological and sociological theory and
methodology and looks at preference for music seen as ‘negative’ such as heavy rock.
Eugenia Costa-Giomi will act as discussant for the symposium in bringing together
the ﬁndings and themes.

The Inﬂuence of Cognitive Style on Pain Relief Using Preferred
Music
Laura Mitchell, John McDowall, Raymond MacDonald; Glasgow
Caledonian University, UK
1PM1-R01-01
A growing body of research has investigated music listening as a non-invasive
technique capable of aﬀecting pain, with consistent ﬁndings that the person’s own
choice of music is particularly eﬀective (Mitchell and MacDonald, 2006). Further
work is needed, however, to uncover potential individual diﬀerences which may
aid clinicians in identifying who will beneﬁt the most from this intervention. One
signiﬁcant recent research development is the empathising and systemising quo-
tient (Wakabayashi et al, 2006), two brain types which relate to diﬀerent cognitive
styles; high empathisers can identify and respond well to emotions and thoughts
in others, and high systemisers are able to deal easily with abstract, organisational
and mechanical systems. This study used experimentally induced pain in 72 healthy
volunteers to compare the eﬀects of preferred music and mental arithmetic to control
in individuals high in ‘empathising’ or ‘systemising’. A signiﬁcant interaction was



                                           9
Symposia                                                                  ICMPC 10

found between condition and cognitive style, with empathisers tolerating more pain
during the music condition than during both other conditions, but no diﬀerences
in systemisers. Systemisers were further found to rate pain lower in the arithmetic
condition than the control, and empathisers to rate their perceived control as higher
when listening to music. The ﬁndings here suggest that this may be an important
individual diﬀerence in eﬃcacy of the intervention that may also clarify previous
gender diﬀerences in the research.

Music and Videogame Play: The Eﬀects of Self-Selected and
Experimenter-Selected Music on Performance and Experience
Gianna Cassidy, Raymond MacDonald; Glasgow Caledonian
University, UK
1PM1-R01-02
The Wii platform presents not only a new form of interactive ‘edutainment’ for
health and wellbeing, but a new platform for musical experience. There is a need
to investigate both the nature of our music practices and preferences during
gameplay, and the eﬀects of exposure to music on gameplay performance and expe-
rience. The study aimed to investigate the eﬀects of exposure to self-selected and
experimenter-selected music on Wii game performance and experience. Employing a
repeated-measures design, participants completed Wii Star Wars in the presence of
experimenter-selected music, self-selected music or game soundtrack, in a counter-
balanced order. Two physiological measures (heart-rate and caloriﬁc expenditure),
three behavioural (pedometer rate, completion time, and performance), and three
experience measures (enjoyment and mood state), were obtained. Preliminary results
indicate that performance and experience measures were optimal when listening to
self-selected music: players scored highest; burned most calories; and experienced
greatest enjoyment and positive mood change. However, listening to music, whether
self-selected or experiment-selected, resulted in more positive performance and ex-
perience measures in comparison to playing without music. The results are discussed
in relation to theories of attention-distraction, arousal and aﬀect modiﬁcation, and
our subjective relationship with the musical stimuli. Directions for further research
and implications for serious-games design, practitioners, and educationalists are
also discussed.

‘It Just Makes Me Feel Better’: Music and Emotional Health Over
the Youth-Adult Transition
Douglas Lonie 1 , Patrick West 1 , Graeme Wilson 2 ; 1 MRC Social and
Public Health Sciences Unit, UK; 2 Newcastle University, UK
1PM1-R01-03
Much has been written about the salience of music in adolescence for identity
formation and socialisation. The aims of the paper are to investigate the emotional
health of fans of the three main ‘taste groups’ identiﬁed (Chart, Dance and Rock
music) and explore the diﬀerent uses of music for emotional regulation and wellbeing
within these groups. To do this, logistic regressions were carried out on a large scale
longitudinal dataset (The West of Scotland Twenty-07 Study). This was followed
by semi-structured interviews with 18 participants, designed and analysed using
Interpretative Phenomenological Analysis (IPA).
The quantitative analysis showed little variation between taste groups in measures of
emotional health suggesting that this does not vary according to music preference.
This was corroborated by the qualitative enquiry which highlighted how participants
across all taste groups perceived multiple means of using music to aid emotional
development and sustain emotional wellbeing throughout the youth-adult transition.
Participants used music for emotional regulation and ‘feeling better’. This was not
linked to any taste group in particular but instead was practised by participants who
indicated a stronger taste for music generally. These results indicate that the process
of using music for emotional wellbeing is closely linked to any music preference as
opposed to particular music genres or styles.

A Mixed Methods Investigation into the Eﬀects of Music on
Individuals with Dementia
Julie C. De Simone, Raymond MacDonald; Glasgow Caledonian
University, UK
1PM1-R01-04


                                         10
ICMPC 10                                                                  Symposia

This is a joint study between Glasgow Caledonian University and a music charity,
Polyphony, who provide access to musical activities in a large psychiatric hospital
in the west of Glasgow, Scotland. This paper investigates the use of music as an
intervention in the treatment of individuals with dementia. An experimental design
was utilised with 20 participants in an experimental group and 20 participants in a
control groups. Cognitive function and communication skills were assessed. Semi
structured interviews with the participants’ carers which were also used and analysed
using thematic analysis. Quantitative analysis utilising ANOVAs investigated the
eﬀects of music upon the dependant measures. The qualitative data was analysed
using thematic analysis. This analysis reported a number of key themes such as
enjoyment of sessions, improvement in social interaction and enhanced self esteem.
This research highlights the beneﬁts of music intervention which has the potential
to improve quality of life. Future research should seek to uncover in more detail the
process and outcomes of these types of intervention.

Researching the Relationship Between Music and Health: An
Overview of Published Research and Key Themes
Raymond MacDonald, Laura Mitchell; Glasgow Caledonian University,
UK
1PM1-R01-05
This paper will overview key issues for researchers interested in investigating the
relationship between music and heath. It will provide an overview and critical
evaluation of published research in the area highlighting the contrasting ways in
which music and health can be studied. Diﬀerent approaches will be presented
with an emphasis on comparing and contrasting the ways in which quantitative
and qualitative methodologies are utilised. For example, the paper will present
studies that utilise focus group research (eg music education and health) and semi
structured interview research (eg the eﬀects of musical participation by patients with
cancer). These will be compared with research that involves questionnaire designs
(eg patterns of music listening and health) and with studies that have an experimental
approach (e.g. the eﬀects of listening to music while undergoing kidney dialysis
treatment). Theories of musical identities and musical communication will be used
to suggest psychological mechanisms by which music may have health beneﬁts. The
role of musical structure, cultural variables and the inﬂuence of personal musical
preference will be discussed. The contrasting but related contributions of music
therapy, community music and music education to our understanding of music and
health will also be presented.


1PM2-R01 : Symposium 2 : Investigating Musical
Performance: Aspects of Self in Advanced Musical
Learning Across Diﬀerent Musical Genres
Room 1, 15:15 – 17:15 Monday 25 August 2007, Oral session

Investigating Musical Performance: Aspects of Self in Advanced
Musical Learning Across Diﬀerent Musical Genres
Graham Frederick Welch 1 , Andrea Creech 1 , Ioulia Papageorgi 1 ,
Margaret S. Barrett 2 , Raymond MacDonald 3 ; 1 IOE University of
London, UK; 2 University of Queensland, Australia; 3 Glasgow
Caledonian University, UK
SUMMARY-2
The purpose of this symposium is to present new evidence relating to teaching,
learning and performance in diverse musical genres within higher education contexts
that draws on data from a two-year research project funded by the UK’s Economic
and Social Research Council under their Teaching and Learning Research Programme
(TLRP) and with Professor Graham Welch as the Principal Investigator. The presen-
tation will also draw on an Australian Research Council Linkage grant ‘Mapping the
Novice-expert Continuum in Composing and Performing’ with the Universities of
Tasmania and Brisbane (led by Professor Margaret Barrett with Professor Graham
Welch as the co-researcher).
The Investigating Musical Performance [IMP]: Comparative Studies in Advanced Mu-
sical Learning research project (http://www.tlrp.org/proj/Welch.html) is a two-year
comparative study of advanced musical performance (2006–2008). The project has


                                         11
Symposia                                                                   ICMPC 10

been devised to investigate how classical, popular, jazz and Scottish traditional
musicians deepen and develop their learning about performance in undergraduate,
postgraduate and wider music community contexts. It is conceived as a multi-site,
multi-methods research project that draws equally on the strengths and expertise of
the four higher education partners (the Institute of Education, University of London;
University of York; Leeds College of Music; and the Royal Scottish Academy of Music
and Drama, Glasgow).
Included in the research methodology is a specially-devised web-based question-
naire, linked electronically to a 623-ﬁeld database, that provides a comprehensive,
short-term, longitudinal comparison of participants (N= 244) concerning their
backgrounds, attitudes and approaches to advanced performance learning over a
twelve month period. This set of data is complimented by interviews, individual case
studies, focus groups and digital video analyses of studio-based instrumental lessons.
Analyses of quantitative and qualitative data have produced new evidence relating
to the processes that underpin teaching and learning within higher education music
contexts. The motivation for hosting this two-hour symposium is that it provides a
unique opportunity to bring together evidence relating to the concept of self in the
teaching and learning of diﬀerent musical genres - an aspect of higher education
learning in music that remains relatively under-researched. It is anticipated the
discussion will explore ways in which higher education music curricula should take
account of how the aspects of self and genre interact and impact on learning, both
in the UK and in Australia.
Speciﬁc research foci to be presented and discussed include: (a) the ways in which
learning in music is mediated through gender and musical genre by self concept;
(b) a longitudinal comparison of the attitudes and perceptions of advanced musical
learners towards ideal and self-perceived musical skills; and (c) the relationship
between musical self-eﬃcacy and time, eﬀort, perceived relevance and pleasure in
musical activities amongst undergraduate music students. Professor Barrett will
report on the perceptions of twenty eminent composers who teach composition in
higher education settings concerning the learning and teaching strategies that they
employ and the environmental conditions and factors that support these strategies.

Investigating Musical Performance: A Longitudinal Comparison of
Advanced Musical Learners’ Attitudes and Perceptions Towards
Musical Skills
Ioulia Papageorgi, Andrea Creech, Graham Frederick Welch; IOE
University of London, UK
 1PM2-R01-01
This paper focuses on a twelve-month longitudinal comparison of 87 advanced mu-
sicians’ attitudes regarding musical skills. Participants (undergraduate and portfolio
career musicians) completed a specially-devised questionnaire investigating their
backgrounds, attitudes and approaches to performance learning on two occasions,
to capture potential changes in attitudes over time. Responses to questions focusing
on (a) perceptions regarding the importance of particular musical skills in becoming
a successful musician (‘ideal’ musical skills) and (b) self-assessment of musical skills
(‘perceived’ musical skills) were analysed in SPSS. Findings suggest that, over the
course of twelve months, musicians lowered their ratings of ‘ideal’ musical skills,
whilst self-perceived skill ratings did not signiﬁcantly change. A previously observed
gap between musicians’ ‘ideal’ and ‘perceived’ level of skill decreased, potentially
due to gaining more experience and perhaps more realism about what is possible
‘ideally’. Signiﬁcant diﬀerences in attitudes were observed in relation to gender, but
not musical performance genre or professional status. Female musicians appeared
more likely to place unreasonable expectations upon themselves. As undergraduate
musicians came towards the end of their studies, their attitudes seemed to converge
with those of established professional musicians. Gender, musical genre and profes-
sional status interacted to inﬂuence musicians’ attitudes in a variety of ways.

Subjective Values and Musical Self-Eﬃcacy: The Relationship
Between Musical Self-Eﬃcacy and Time, Eﬀort, Perceived
Relevance and Pleasure in Musical Activities Amongst
Undergraduate Music Students
Andrea Creech, Ioulia Papageorgi, Graham Frederick Welch; IOE
University of London, UK
1PM2-R01-02

                                          12
ICMPC 10                                                                  Symposia

Although musical self-eﬃcacy has been found to play an important role in the
development of higher education music students, an under-researched area is the
relationship between musical self-eﬃcacy and subjective values such as enjoyment
of musical activities, perceived importance or usefulness of musical activities and
the perceived time/eﬀort cost of engaging in musical activities. This paper ﬁrst
addresses the question of whether the perceived relevance, time and eﬀort expended
and pleasure experienced in musical activities changed over time for music under-
graduates. Secondly, this paper tests the hypothesis that these variables would
account for variability in musical self-eﬃcacy of the students. Fifty-nine music
undergraduates in the UK (encompassing the four musical genres noted above) were
surveyed at time 1 and again at time 2, 12 months later. Likert scales measured
musical self-eﬃcacy as well as perceived relevance, eﬀort required and pleasure
experienced in musical activities. A signiﬁcant decrease in the perceived relevance
and pleasure in practising alone was found, while an increase in pleasure experienced
in mental practice was found. Multiple regressions revealed that pleasure in musical
activities was the only predictor to account for signiﬁcant variability in musical
self-eﬃcacy. The evidence presented here points to future research that probes the
notion of pleasure in musical activities and how this may be capitalized on in order
to maximize opportunities for enhancing musical development.

Musical Self, Genre, and Gender as Factors in Higher Education
Learning in Music
Graham Frederick Welch, Ioulia Papageorgi, Andrea Creech; IOE
University of London, UK
1PM2-R01-03
Educational and psychological research suggests that gender and musical genre can
inﬂuence musical learning and the development of musical identities, particularly
during adolescence. However, there are few studies concerning the possible impact
of gender and musical genre on higher education (HE) musical learning. A two-year
comparative study, funded under the ESRC’s Teaching and Learning Research
Programme (TLRP), investigated the eﬀect of musicians’ gender and chosen musical
performance genre (Western classical, jazz, popular, or Scottish traditional music)
on undergraduate and postgraduate (career-based) learning. Data were gathered
through a web-based survey of participants (n=244) drawn from four HE institutions
(HEIs) in Glasgow, York, Leeds and London and the wider workplace, supplemented
by semi-structured case study interview data from a sub-set (n=27). Statistical
and qualitative analyses indicate that gender and genre can impact individually on
aspects of participants’ psychological and socio-psychological make-up and attitudes
to learning. However, there was no evidence any major interaction between genre
and gender in the data. Also, irrespective of musical genre, skilled musicians had
many aspects in common in terms of their core musical identities and behaviours,
implying that the requirements for highly skilled musical performance can transcend
particular group characteristics.

Eminence Lessons: Eminent Composers’ Perceptions of Learning
and Teaching Practices in Music Composition
Margaret S. Barrett; University of Queensland, Australia
1PM2-R01-04
The phenomenon of music composition learning and teaching has been the focus
of considerable inquiry in educational and psychological research. Much of this
inquiry has examined learning and teaching practices in school settings where music
educators work with novice student composers. Less research has examined the
learning and teaching practices of eminent composers working with experienced
student composers. Eminence studies provide insights into the beliefs, values,
processes and practices of eminent leaders in a domain of practice. This project
employed life-history interview to examine the perceptions of 20 eminent composers
who teach composition in higher education settings concerning the learning and
teaching strategies they employ and the environmental conditions and factors that
support these strategies. Findings suggest that composition teaching is individualis-
tic, genre-speciﬁc, and shaped by composers’ biographies and experience of learning
and teaching as student and educator. Whilst all composers valued their learning
and teaching experiences, their capacity to reﬂect on this, and develop individualised
strategies for students was related to their experience in teacher education. These
ﬁndings hold implications for the ways in which student composers in the higher
education setting are prepared for their professional lives, and for the professional
learning needs of higher education composition academics.

                                         13
Symposia                                                                  ICMPC 10


2AM1-R01 : Symposium 3 : Applying Music Psychology
Room 1, 8:30 – 10:30 Tuesday 26 August 2008, Oral session

Applying Music Psychology
David J. Hargreaves 1 , Raymond MacDonald 2 , Graeme Wilson 3 ,
Hiromichi Mito 4 , Adam Ockelford 1 , Scott D. Lipscomb 5 ;
1
  Roehampton University, UK; 2 Glasgow Caledonian University, UK;
3
  Newcastle University, UK; 4 Miyagi University of Education, Japan;
5
  University of Minnesota, USA
SUMMARY-3
The digital revolution gave rise to the widespread availability of relatively cheap and
portable music listening and recording equipment, which meant that a considerable
proportion of the world’s population now has access to virtually any music in many
listening situations. More recent developments have seen music download services
overtaking CD sales, and software packages which enable anyone to compose and
transform music relatively easily. This has changed the ways we create and use music
in everyday life, and to the study of its applications in areas including health care,
education, consumer behaviour, leisure and the media. This newly-emerging ﬁeld of
study has recently been surveyed in The Social and Applied Psychology of Music, by
Adrian North and David Hargreaves (Oxford UP, 2008).
The symposium aims to explain the historical context of applied music psychology
within the discipline as a whole, and to outline its scope and main features.
The opening position paper by David Hargreaves outlines the historical context and
scope of applied music psychology. It outlines the main areas of application, as
well as some of the theoretical questions which arise. Each of the next three papers
illustrates a diﬀerent aspect of applied music psychology. Raymond MacDonald and
Graham Wilson present some ﬁndings on musical identities in improvising musicians
which emerge from their interviews with 10 improvising musicians from varied
backgrounds, analysed using discursive psychological methods. Hiromichi Mito
discusses some empirical studies which focus on the process of musical learning
in informal settings, with an emphasis on how people can acquire various kinds of
musical skill through everyday experiences. His research shows that young Japanese
non-musicians can show very high levels of cognitive musical skill, and can outper-
form musicians with abundant experience in both formal musical training and daily
musical activities on certain tasks. Finally, Adam Ockelford discusses the ﬁndings of
research into the musical engagement of children with complex needs, and presents
a model of musical development that synthesises (a) classroom observations, (b)
the ﬁndings of psychological research into early music development in able-bodied
children, and (c) a theory of music cognition.
Music not only fulﬁls many important roles in society, but has equally important cog-
nitive, social and emotional functions for the individual. Applied music psychology
is beginning to describe and explain these functions in a widening variety of real life
contexts, such those illustrated in this symposium.

Learning Musical Skill Through Everyday Musical Activities
Hiromichi Mito; Miyagi University of Education, Japan
2AM1-R01-01
The purpose of the present study is to show the role of everyday musical activity in
the acquisition of musical skills, and is based on the premise that people without
formal musical training can acquire a high level of musical skills through exposure
to everyday musical activities. The present study begins with the discussion of
some studies from my own doctoral research which focus on the process of musical
learning in informal settings. From a detailed analysis of research in music education,
anthropology and ethnography, it became clear that people can acquire various kinds
of musical skill through everyday experiences, and the discussion then goes on to
examine some psychological studies which have been providing empirical evidence
to show that various kinds of cognitive musical skills can be acquired without
formal training. I describe some of my own experimental studies of the cognitive
musical skills of young Japanese people, in which non-musician participants showed
extremely high level of cognitive musical skill. It was especially surprising in these
studies that even the musician participants who had abundant experience in both
formal musical training and daily musical activities could not perform as well as


                                         14
ICMPC 10                                                                     Symposia

the non-musician participants. An important implication of this study is that young
participants’ daily musical leisure activities function, on a high level, as musical
training, even though the participants may have no conscious awareness of this
process.

Musical Identities in Improvising Musicians
Raymond MacDonald 1 , Graeme Wilson 2 ; 1 Glasgow Caledonian
University, UK; 2 Newcastle University, UK
2AM1-R01-02
There is signiﬁcant interest in improvisation and how this universal but under-
researched musical activity is conceptualised and realised in contemporary practice.
Our previous work has highlighted how professional jazz musicians talk about
improvisation and how these discourses shape musical identities, deﬁnitions of jazz
music and perceptions of lifestyle. In particular, a mastery repertoire prioritises
instrumental virtuosity as a key feature of improvisatory practices while the mystery
repertoire prioritises more inspirational, instinctual or ‘soulful’ understandings.
However, while improvisation is often seen as the preserve of elite jazz musicians
there is signiﬁcant improvisational activity around the world by musicians who
do not come from a jazz background and there is an urgent need to understand
how these musicians utilise and talk about improvisational practices. This paper
utilises discourse analysis to investigate how ten improvising musicians negotiate
and maintain identities. Results highlight how participants used psychological
constructs (eg ﬂow) to describe successful improvisation across the very wide range
of improvisational practices they describe. The ﬁndings are discussed in relation
to discursive psychology, applied psychological research and research on the views
of musicians and contemporary conceptualisations of improvisation. This paper
highlights how applied psychological research in the form of discourse analysis
can further our knowledge of musical identities and improvisational practices. The
fundamental relationship between talking about music and performing music, and
the importance of identity negotiations when improvising together, are highlighted.

Towards a Music Curriculum for Children with Complex Needs
Adam Ockelford; Roehampton University, UK
2AM1-R01-03
The aim of the research project that is reported here has been to provide an
evidence-based model of musical development in children with complex needs, with
the objective of providing teachers with an interactive software package for assessing
and recording their pupils’ achievements and progress, and to oﬀer a curriculum
framework and materials. Three sources of evidence have been used to build up the
model of musical development: observations of children with complex needs, made
by practitioners and members of the research team; the ﬁndings of the last three
decades of psychological research into the early musical development of able-bodied
children; and the ‘zygonic’ theory of music-structural cognition developed by Adam
Ockelford in the last ten years. The model has developed through an iterative
process, as systematic observations have led to hypothesis-building - theories that
have then been tested by practitioners in the classroom. The results indicate that
the musical development of children with complex needs can usefully be modelled
in six levels across three domains: ‘reactive’, ‘proactive’ and ‘interactive’. Each sector
is itself divided into four elements to provide the level of detail needed for teachers
to make practical use of the material. The research is ongoing, and is currently in the
phase of software development mentioned above.

Music Psychology: Developments and Applications
David J. Hargreaves; Roehampton University, UK
2AM1-R01-04
Music psychology has moved from a predominance of psychometrics in the 1960s, to
an emphasis on cognitive psychology and the study of classical music in the 1980s,
which accompanied the emergence of sub-disciplines including the developmental
and social psychologies of music. We are now seeing a new emphasis on the applica-
tions of psychology in areas including health care, education, consumer behaviour,
leisure and the media. This paper outlines the historical context of applied music
psychology within the discipline as a whole, and outlines its scope and main features.
The paper describes some of the main cognitive and learning functions that music
can fulﬁl for the individual, which include the promotion of artistic and performance
skills, of creativity and improvisation, memory, problem solving and reasoning, and


                                           15
Symposia                                                                  ICMPC 10

other potential scholastic gains. It also reviews some of music’s main social and
emotional functions, which include emotional communication, mood regulation,
aesthetic appreciation, entertainment, teamwork and co-operation, and moral and
spiritual development. It is argued that the understanding of these functions as they
operate in everyday life contexts will become a primary task of music psychology,
and that this will requite a much greater emphasis on practical implications, and an
increasing use of naturalistic methodologies.


2PM1-R01 : Symposium 4 (Invited) : Absolute Pitch and
its Implications for Music Perception and Cognition
Room 1, 13:30 – 15:30 Tuesday 26 August 2008, Oral session

Absolute Pitch and Its Implications for Music Perception and
Cognition
Ken’ichi Miyazaki 1 , Andrzej Rakowski 2 , Piotr Rogowski 2 , Sylwia
Makomaska 3 , Elizabeth W. Marvin 4 , Elissa L. Newport 4 , Sandra E.
Trehub 5 , David Huron 6 , Carol L. Krumhansl 7 ; 1 Niigata University,
Japan; 2 Fryderyk Chopin University of Music, Poland; 3 Warsaw
University, Poland; 4 University of Rochester, USA; 5 University of
Toronto at Mississauga, Canada; 6 Ohio State University, USA; 7 Cornell
University, USA
SUMMARY-4
Absolute pitch (AP), a faculty to identify musical pitch categories of individual tones
without any context, is one of remarkable auditory skills associated with music
and has drawn a great deal of interest of researchers. Research on AP has a long
history from the early period of scientiﬁc psychology, but previous research on
AP had rather limited perspectives, mainly focusing on aspects of AP as a musical
talent and marvelous performance of its possessors, partially because AP possessors
were believed to be extremely rare in the general population. During the recent
few decades, however, AP has been studied in a wider range of perspectives, and a
growing body of research has revealed perceptual and cognitive aspects of AP more
in detail, and not only AP per se but several related phenomena have drawn wider
interest of researchers in cognitive science, neuroscience, music education, and
behavioral genetics. Recent studies have extended the scope of AP and raised several
controversial issues including the possible association between AP and language —
particularly an assumed relationship between AP and tone languages, the existence
of the sensitive period for AP, learnability of AP, various types of implicit AP, and
genetic basis of AP. These lines of research on AP from multidisciplinary perspectives
are expected to converge on a consistent model for understanding human mind.
In this symposium, four distinguished presenters from diﬀerent perspectives talk
about their insightful ideas about the nature of AP and related phenomena as well as
ﬁndings of their experiments.

Absolute Pitch as a Measuring Device in Psychoacoustic
Experiments
Andrzej Rakowski 1 , Piotr Rogowski 1 , Sylwia Makomaska 2 ; 1 Fryderyk
Chopin University of Music, Poland; 2 Warsaw University, Poland
2PM1-R01-01
Absolute pitch (AP) is a unique feature of the musical pitch memory. The possessors
of full AP (several percent of the population of musicians) preserve in their long-term
auditory memory the pitch standards of 12 within-octave values of musical pitch
(chromas).
Two experiments concerning absolute pitch have been presented. In the ﬁrst one
pitch strength of short pure-tone pulses containing a limited number of vibration
periods, was estimated as a proportion of correct chroma identiﬁcations by expert
AP possessors. It was found that the duration time of a tone pulse, more than the
number of vibration periods contained in it, determine the tone pulse’s pitch strength.
Second experiment concerned estimations with the method of chroma identiﬁcation
of pitch strength in residual tones with missing fundamental and spectrum composed
of 11 harmonics located at various parts of the harmonic series. The higher number
of lowest harmonic of this band-limited spectrum, the more diﬃcult to perceive the


                                         16
ICMPC 10                                                                  Symposia

virtual pitch of the tone’s missing fundamental. Experimental results were compared
with those obtained with the method of musical interval identiﬁcation (Houtsma &
Smurzynski, 1990).
Additional experiment concerned ﬁnding the frequency range where AP is fully
eﬀective. This range appeared to spread from third through seventh octave.

Statistical Learning in Language and Music: Absolute Pitch
Without Labeling
Elizabeth W. Marvin, Elissa L. Newport; University of Rochester, USA
2PM1-R01-02
Absolute pitch (AP) is typically deﬁned as the ability to identify or produce a musical
pitch without access to an external reference tone. By this deﬁnition, pitches must
be named by some standard, and the incidence of AP in the general population
is estimated to be quite low (e.g., 1 in 10,000). Recent research has shown that
pitch memory is more wide-spread if the labeling requirement is removed. We have
developed a test of AP, based on statistical-learning research in language acquisition,
which tests pitch memory after a familiarization phase but does not require labeling.
Our results demonstrate a robust ability of listeners to learn and remember three-
note patterns across four participant groups (n=44): AP (professional) musicians,
non-AP musicians, amateur musicians, and nonmusicians. When listeners are asked
to distinguish between learned pitch patterns and their transpositions, performance
in all four groups is above chance (in accord with other recent studies). Further,
the test discriminates reliably between AP musicians and non-AP musicians, and
correlates well with traditional pitch naming tests for those who can name pitches.
This allows us to test those without musical training. We discuss results for each
group in light of current theories of pitch memory. Of particular interest are three
nonmusicians whose scores place them well within the range of AP musicians’
performance.

Developmental Perspectives on Pitch Memory
Sandra E. Trehub; University of Toronto at Mississauga, Canada
2PM1-R01-03
We examined pitch memory in children of diﬀerent ages, cultures, and language
backgrounds. Instead of using isolated pitches, as in typical AP studies, we used
familiar music. In Study 1, Canadian 5- to 10-year-olds and Japanese 5- to 6-year-olds
were required to distinguish the original versions of familiar TV theme songs from
foils that were pitch-shifted by one semitone. Older children performed better than
younger children, and Japanese children performed better than same-age Canadian
children. In Study 2, 9- to 12-year-old children of Asian (Chinese) or non-Asian
(European) heritage performed a similar task. Some Asian children spoke a tone
language in addition to English; others were monolingual English speakers. Age,
heritage (Asian, European), and tone-language use had no eﬀect on memory for pitch
level. These ﬁndings disconﬁrm the view that AP processing is superior in younger
than in older children. They also disconﬁrm the reported contributions of genetic
factors to cross-cultural diﬀerences in pitch memory. The absence of pitch memory
diﬀerences between tone-language users and non-users in Study 2 makes it unlikely
that language factors contributed to Japanese children’s enhanced performance in
Study 1. Instead, early note-naming and key-naming opportunities are likely to be
responsible for the observed diﬀerences.

On the Mental Representation of Pitch: Lessons from Absolute
Pitch
David Huron; Ohio State University, USA
2PM1-R01-04
How does the auditory system represent music-related percepts such as pitch?
The extant research suggests that multiple representations exist concurrently in
the auditory system, and that these representations are shaped by the auditory
environment during development. These observations are consistent with theories
of competitive representations, such as Edelman’s “neural Darwinist” approach.
Following Huron (2006), it is suggested that the diﬀerence in predictive accuracy
for diﬀerent representations provides the feedback mechanism by which competing
representations are selected. Representations that perform poorly in predicting
future sounds atrophy. Repercussions for cognitive modeling of music are brieﬂy
discussed.


                                         17
Symposia                                                                 ICMPC 10


2PM2-R01 : Symposium 5 : Musical Dynamics as
Adaptive, Flexible Behavior: The Emergence of Meaning
and Social Life
Room 1, 15:45 – 17:45 Tuesday 26 August 2008, Oral session

Musical Dynamics as Adaptive, Flexible Behavior: The Emergence
of Meaning and Social Life
Patricia M. Gray 1 , Edward W. Large 2 , Paul J. Thibault 3 ,
William Southworth Greaves 4 , James D. Benson 4 , Laurel J. Trainor 5 ,
Ian Cross 6 ; 1 University of North Carolina at Greensboro, USA;
2
  Florida Atlantic University, USA; 3 University of Agder, Norway;
4
  York University, Canada; 5 McMaster University, Canada; 6 University
of Cambridge, UK
SUMMARY-5
The contributors to this Panel will discuss the question of whether music-making
with its underlying concomitant sound and time dynamics is uniquely human.

Rhythmic Analysis of Musical Interactions Between Bonobo and
Human
Edward W. Large 1 , Marc J. Velasco 1 , Patricia M. Gray 2 ; 1 Florida
Atlantic University, USA; 2 University of North Carolina at Greensboro,
USA
2PM2-R01-01
Synchronization of rhythmic communication signals has been observed in insects
and amphibians. While such behavior is rare among higher animals, synchronous
chorusing may exist among bonobos. To our knowledge however, non-human
primates have never been reported to spontaneously entrain with rhythmic sounds,
and we are aware of no successful attempts to train them to do so. Thus, our goal was
to establish whether three language-capable bonobos were able to entrain to auditory
stimuli during musical interactions with a human. We analyzed MIDI recordings of
the musical interactions, which took place over a three day period. During these
interactions, a bonobo played “chords” on a MIDI keyboard while interacting with
the musician, whom s/he could see and hear playing on a separate MIDI keyboard.
Thirty-seven episodes of rhythmic interaction were identiﬁed. Within each, descrip-
tors of rhythmicity and entrainment were computed, including instantaneous tempo
and phase of bonobo events relative to human events. In about half of the rhythmic
interactions, attunement of tempo was observed, and in just under half of these
episodes, statistical evidence of phase entrainment was found. Our analysis provides
evidence of meaningful rhythmic interaction between a non-human primate and a
human in a musical context.

Musicking and Culture Creation: Sound and Time as Agents of
Social Cohesion
Patricia M. Gray, Ryan Daniels; University of North Carolina at
Greensboro, USA
2PM2-R01-02
The ubiquity of music-making throughout past and present human cultures, com-
bined with recent research revealing a suite of musical faculties present at birth in
normal humans suggest a deep evolutionary role for the perception and manipula-
tion of sound and time. Just as the primary focus for spoken language lies not in
its concepts or precepts but in the manipulations of sound and time that convey
meaning, so it is with music-making (musicking). Musicking is often spontaneous
and part of a natural and organic outcome of a social process. Participation in the
group’s manipulation of sound and time through musicking is a central element of
citizenship in the group and of the group’s cohesiveness. From this perspective, mu-
sicking is transformed into a process of social cohesion. Because musicking rides on
rhythmic synchrony, a communicative interaction where a coupling between brains
can aﬀect each other’s internal states, and because this interactional synchrony is
basic to musicking, spoken language, and to other collective dynamics, new research
with a group of bonobo apes (Pan paniscus) who demonstrate similar capacities


                                        18
ICMPC 10                                                                  Symposia

suggests these capacities may have existed in a common ancestor thereby oﬀering
increased resources for social cohesion earlier in the evolutionary record.

Norms, Co-Constructed Body Dynamics, and Interaction in
Bonobos and Humans
Paul J. Thibault; University of Agder, Norway
2PM2-R01-03
This presentation will examine the centrality of micro-temporal bodily dynamics
in bonobo-human interactions in a joint Pan-human cultural environment (Savage-
Rumbaugh et al 1998). How do the micro-temporal interactional dynamics of bodies
eﬀect coordination relations among agents (bonobo and/or human)? How are these
dynamical processes of bodies-in-interaction connected to norms and higher-scalar
cultural and historical constraints? Our brains and bodies tune into and exploit these
dynamics in ways that connect experience to normative cultural patterns and values,
which exercise their own constraints on these same dynamics. Body dynamics can
transform cognition, selfhood, and social reality. This view challenges the traditional
idea of language as a code, which treats the brain as a processor of symbolic inputs
and outputs. Instead, co-constructed body dynamics themselves have causal and
cognitive powers. Hominids are inﬂuenced by and respond to these dynamics in
micro-time. Hominids (e.g. bonobos and humans) exploit pitch, rhythm, cadence,
tempo, duration, loudness and other aspects of vocalizations as well as facial and
gestural activities. The term ‘micro-temporal body dynamics’ refers to the very fast,
small-scale bodily processes involved in vocalizing, eye movements, rhythm, tone,
gesture, voice quality, and much more.

Musical Representation by a Bonobo Resulting from the
Assimilation of Meaning Through Bonobo-Human Dialogic
Interaction
James D. Benson, William Southworth Greaves, Ashley Watkins; York
University, Canada
2PM2-R01-04
The data for this study consists of a 30 minute interaction between the bonobo
Panbanisha, the musician Peter Gabriel, and the primatologist Sue Savage-Rumbaugh:
a dialogue between the three participants, interspersed with interactive song im-
provisations on an electronic keyboard by Panbanisha. There are six iterations of a
speech genre, with the structure song-title negotiation ˆ acceptance ˆ song production
ˆ evaluation. Panbanisha improvises ﬁve diﬀerent songs in the six conversations:
the ‘orange and banana song’, the ‘lookout point song’, the ‘car song’, the ‘banana
song’, and the ‘grooming song’. ‘Groom’ was originally proposed by Panbanisha
in the second conversation, but no song was produced. During the course of the
conversations, Sue mediates Panbanisha’s song production by re-contextualizing
Panbanisha’s song-titles as activities and events in the past and future, such that
the ‘lookout point song’ becomes the ‘yesterday song’, the ‘car song’ the ‘tomorrow
song’, and the ‘banana song’ the ‘later song’. In the process, Sue re-contextualizes
the ‘grooming song’ as a ‘now’ song. The ‘grooming song’ with its focus on the
here and now is the culmination of the preceding discourse, and the result is an
expansion of Panbanisha’s musical assimilation in the most aesthetically pleasing of
the ﬁve songs, as indicated by the extended positive evaluation by Panbanisha, Sue
and Peter. A linguistic methodology for the analysis of casual conversation is used to
show how the assimilation of meaning through ﬂexible dialogic interaction with Sue
is the scaﬀolding for Panbanisha’s assimilation of musical meaning in her musical
interaction with Peter.




                                         19
Symposia                                                                  ICMPC 10


3AM1-R01 : Symposium 6 : Aesthetic Evaluation and
Cognitive Classiﬁcation of Music in Experts and
Laymen: Behavioral and Electrophysiological Data
Room 1, 8:30 – 10:30 Wednesday 27 August 2008, Oral session

Aesthetic Evaluation and Cognitive Classiﬁcation of Music in
Experts and Laymen — Behavioral and Electrophysiological Data
Elvira Brattico 1 , Sirke Nieminen 1 , Kjetil Falkenberg Hansen 2 , Mira
Müller 3 , Tuomas Eerola 4 , David J. Hargreaves 5 ; 1 University of
Helsinki, Finland; 2 KTH, Sweden; 3 University of Leipzig, Germany;
4
  University of Jyväskylä, Finland; 5 Roehampton University, UK
SUMMARY-6
Subjective processes aimed at aesthetic or cognitive evaluation of music are an
important, although relatively neglected, aspect of music listening. Those processes
occur without any instruction since, for instance, listeners involuntarily classify
music as belonging to a certain style or as mismatching stylistic expectations.
Expertise with music and musical concepts reﬁnes those cognitive and aesthetic
evaluative processes leading to the usage of ﬁne-grained or even novel categories
by music experts. In this symposium, the interaction between expertise and sub-
jective processes related to music classiﬁcation and evaluation is studied from an
original perspective. In the ﬁrst talk by S. Nieminen & E. Istok, behavioral data will
explore how aesthetic and emotional concepts of music develop from childhood to
adulthood. This talk provides conclusions on the similarity of musical concepts
applicable to Western tonal music in individuals with varying degrees of expertise
and at diﬀerent ages. The second talk by K. Hansen & R. Bresin presents contrasting
behavioral ﬁndings showing how expertise with a special musical genre, such as DJ
scratching, resulted in conceptual musical structures diﬀerent from the knowledge
repertoire of individuals exposed to the Western tonal music genre. The third and
fourth talks will then reports attempts at ﬁnding neural correlates of subjective
judgments and classiﬁcation of music. In particular, the third talk by E. Brattico
deals with brain oscillatory responses of orchestral musicians possessing a highly
reﬁned musical knowledge, enabling them to correctly classify the style of even
750-ms musical excerpts of the classical genre. The ﬁnal talk by Mueller, L. Hoefel,
E. Brattico & T. Jacobsen will illustrate how evaluative and descriptive judgments of
musical passages modulate the time-locked brain responses and the correspondent
role of formal musical instructions.

Verbal Description of DJ Recordings
Kjetil Falkenberg Hansen, Roberto Bresin; KTH, Sweden
3AM1-R01-01
In a recent pilot study, DJs were asked to perform the same composition using
diﬀerent intended emotional expression (happiness, sadness etc). In a successive
test, these intentions could not be matched by listeners’ judgement. One possible
explanation is that DJs have a diﬀerent vocabulary when describing expressivity
in their performances. We designed an experiment to understand how DJs and
listeners describe the music. The experiment was aimed at identifying a set of
descriptors used mainly with scratch music, but possibly also with other genres.
In a web questionnaire, subjects were presented with sound stimuli from scratch
music recordings. Each participant described the music with words, phrases and
terms in a free labelling task. The resulting list of responses was analyzed in
several steps and condensed to a set of about 10 labels. Important diﬀerences were
found between describing scratch music and other Western genres such as pop,
jazz or classical music. For instance, labels such as cocky, cool, amusement and
skilled were common. These speciﬁc labels seem mediated from the characteristic
hip-hop culture. The experiment oﬀered some explanation to the problem of verbally
describing expressive scratch music. The set of labels found can be used for further
experiments, for example when instructing DJs in performances.




                                         20
ICMPC 10                                                                  Symposia

The Development of Aesthetic Responses to Music and Their
Conceptual Basis
Sirke Nieminen, Eva Istók, Elvira Brattico, Mari Tervaniemi; University
of Helsinki, Finland
3AM1-R01-02
Aesthetic experiences are multidimensional in their nature and include both cognitive
and aﬀective responses. In our study, the developmental aspects of aesthetic and
emotional responses to music were investigated. While listening to musical pieces
(major, minor, atonal), our participants (six-to nine-year-old children) were asked to
rate the pieces according to the beautiful-ugly, the like-dislike and the happy-sad
dimensions by using a special, child-adapted method. Results revealed the major
piece to be mostly considered as beautiful, while none of the pieces was rated as
ugly. The children generally liked the major piece best, the atonal piece second
best, and the minor piece least. All age groups rated the major piece as happy, but
only nine-year-olds rated the pieces clearly either happy or sad. In addition to the
diﬀerent kinds of evaluative ratings, the children were asked to describe the pieces
by giving them appropriate titles. None of the titles contained the word ugly, while
beautiful occurred in several titles. These results conﬁrm the results of the study
investigating the conceptual basis of the aesthetic value of music in adults. Our
study suggests the presence of aesthetic responses to music to be associated with
the concept of beauty already in school-aged children.

The Electrophysiology of Aesthetic Music Processing: Comparing
Music Experts with Laymen
Mira Müller 1 , Lea Höfel 1 , Elvira Brattico 2 , Thomas Jacobsen 1 ;
1
  University of Leipzig, Germany; 2 University of Helsinki, Finland
3AM1-R01-03
This study augments prior ﬁndings of inﬂuence of music expertise on music pro-
cessing by investigating whether music experts and laymen diﬀer with regard to
evaluative aesthetic processing of musical sequences. 16 music experts and 16 music
laymen judged the aesthetic value (evaluative task) as well as the harmonic correct-
ness (descriptive task) of musical chord sequences consisting of ﬁve chords with
the ending chord sounding either incorrect, ambiguous or correct in the harmonic
context established by the preceding four chords. Electrophysiological recordings
were analysed. ERP data indicates diﬀerences in processing of the evaluative vs.
descriptive judgements, i.e., a late and widespread positivity was observed that
was signiﬁcantly larger for aesthetic compared to correctness judgements. This
diﬀerence, however, was equally pronounced for experts and laymen. Additionally,
established ERP eﬀects reﬂecting the processing of harmonic rule violation were
investigated. Here, group diﬀerences were observed in the processing of the mild
violation. Furthermore, experts and laymen diﬀered in their early brain responses
to the beginning of the whole chord sequence. Given the present results, a distinct
inﬂuence of expertise on aesthetic music processing could not be revealed even
though experts and laymen diﬀered in their early brain responses to musical stimuli.

Brain Oscillatory Responses of Musical Style Classiﬁcation in
Orchestral Musicians
Elvira Brattico; University of Helsinki, Finland
3AM1-R01-04
The classiﬁcation of music according to a particular style relies on abstraction of
the characteristic acoustic features, as well as on long-term memory of schemata.
The aim of the study was to determine the brain oscillatory signatures of musical
passages belonging to diﬀerent musical styles. Orchestral musicians classiﬁed
hundreds of brief musical excerpts as belonging to one out of four musical styles:
Baroque, Classical, Romantic, and Modern. Brain oscillatory responses at 2–25 Hz
were measured. Spectral centroid, spectral entropy, roughness and spectral ﬂux were
also computed for each musical excerpt. The acoustical analyses revealed higher
spectral centroid for Baroque and Modern styles as compared with the others and the
highest entropy and roughness for the Modern style contrasted with the others. The
behavioural responses showed that Classical style was the most diﬃcult to classify
whereas Modern style was the easiest. Early theta brain oscillations were increased
and late alpha oscillations decreased to the Modern style as compared with all the
others. Findings converged to suggest a sequence of style processing from initial
extraction of the characteristic acoustic parameters to the subsequent matching of
sensory codes with pre-existing style representations.

                                         21
Symposia                                                                  ICMPC 10


3PM1-R01 : Symposium 7 : Music and Health: a
Paradoxical Relationship
Room 1, 13:30 – 15:30 Wednesday 27 August 2008, Oral session

Music and Health: A Paradoxical Relationship
Gunter Kreutz 1 , Stephan Bongard 2 , Cynthia Quiroga-Murcia 2 ,
Stephen Clift 3 , Jane Ginsborg 4 , Laura Mitchell 5 ; 1 University of
Oldenburg, Germany; 2 University of Frankfurt, Germany;
3
  Canterbury Christ Church University, UK; 4 Royal Northern College
of Music, UK; 5 Glasgow Caledonian University, UK
SUMMARY-7
Musical practices including listening, singing, playing instruments and dancing are
subject of a growing body of research in the context of wellbeing and health. In
general, most musical practices seem to promote wellbeing by providing opportuni-
ties of psychophysical training in one way or another. Playing musical instruments,
singing or dancing, for example, implies a certain amount of mental and physical
exercise, social interaction and emotional reward, all of which may have profound
implications for wellbeing and health. Paradoxically, professional engagement in
musical activities appears not to raise the beneﬁcial eﬀects of musical practices to an
optimum, but rather seems to be associated with increasing levels of practice-related
health problems. These problems are both apparently intrinsic to the activities as
well extrinsic to them as they may relate to contextual and situational conditions
of practice. The papers presented in this symposium tap into this paradox by
demonstrating eﬀects of musical practices in amateurs and professionals. Three
papers are dedicated to amateur singing and dancing, drawing from wide-spread
practices in many countries. Partnered dance is a common activity particularly in
urban areas that is practiced by a growing community of individuals, who often have
otherwise limited musical backgrounds. Choral singing, which has attracted a larger
number of studies over the past years, by contrast, appears to imply greater musical
demands from the participants, but is similarly popular among non-professionals
as is dance. These studies highlight beneﬁcial aspects of amateur singing and
dancing, contrasting sharply to the health implications of professional musicianship.
The forth paper in this symposium draws from ﬁndings about occupational stress,
coping, performance anxiety and personality variables in a study of musicians who
work in high-ranked German orchestras. Thus the contributions they provide some
insight into the controversial facets and impacts of musical activities on health and
wellbeing on the basis of empirical research.

Does Partnered Dance Promote Health?
Gunter Kreutz; University of Oldenburg, Germany
3PM1-R01-01
The arts, music, and dance have recently become prominent topics of debate in
healthcare and health promotion. There is growing recognition of the beneﬁts of
artistic involvement to well-being and health. This research focuses on partnered
dance as a common practice in all human cultures. Its purpose was to investigate the
dancers of tango argentino, and to explore potential health beneﬁts in this group.
Participants (N=110) completed a self-developed inventory, which was, in part, based
on interviews and observations as a participant observer. The inventory addresses
educational, musical, and socio-economic background, motivation for and investment
in dancing tango, and other leisure activities. Dancers are characterized by high-level
education and socio-economic status. Motivation appears to be predominantly driven
by both hedonistic and social factors, accounting altogether for nearly 60% of the
variance. Physical investment in terms of time and money indicate tango dancing as
a highly important activity that seems to involve substantial opportunity of moderate
physical exercise, social interactions and emotional reward. Tango dancers are
dedicated to the practice in many respects, including physical and emotional. They
draw from similar motivations as compared to participants of other musical activities
such as singing and listening to music. Notably, aspects of physical ﬁtness are more
prominent in this group. Implications of partnered dance for social, emotional, and
physical well-being and health promotion demand further research.




                                         22
ICMPC 10                                                                  Symposia

Eﬀects of Tango Dancing on Testosterone, Cortisol and Emotional
State
Cynthia Quiroga-Murcia; University of Frankfurt, Germany
3PM1-R01-02
The present study examine the physiological and emotional eﬀects of one form of
partnered dance: tango argentino. In particular, the inﬂuences of the presence of
music and partner on testosterone, cortisol and emotional state were considered.
Twenty four healthy tango dancers were submitted to four conditions of dancing:
“with partner with music”, “with partner without music”, “without partner with
music” and “without partner without music”. The conditions were temporally sepa-
rated by at least one week. Before the experimental treatment and again 20 minutes
thereafter, the participants provided saliva samples for analysis of cortisol and
testosterone concentrations and ﬁlled out the Positive and Negative Aﬀect Schedule
(PANAS). The data suggest that motion with a partner to music has a more signiﬁcant
positive eﬀect on emotional state than motion without music, or motion without
physical contact have by their own. Moreover, signiﬁcant decreases of cortisol
concentrations were found with the presence of music, whereas signiﬁcant increases
of testosterone levels were found associated with the presence of a partner. To our
knowledge, this is the ﬁrst study showing aﬀect and hormonal changes in response
to partnered dance, in general, and the diﬀerential inﬂuences of music and partner
in particular.

Stress in the Orchestra: Interrelationships of Personality,
Performance Situation, Performance Anxiety and Coping Strategies
Stephan Bongard, Volker Hodapp, Franziska Langendoerfer;
University of Frankfurt, Germany
3PM1-R01-03
Professional musicians often suﬀer from performance anxiety. Some studies at-
tribute levels of anxiety to personality variables, while other studies have identiﬁed
inﬂuences of situation and context. Objective of the present study was to examine
the inﬂuences of personality and coping strategies on performance anxiety among
professional orchestra musicians. The sample consisted of 122 members of six
German symphony and opera orchestras. The musicians were asked to complete
questionnaires measuring various personality traits. In addition, shortly before a
normal rehearsal and a public performance, they also gave details about their coping
and their performance anxiety. The latter was measured by four aspects: Lack
of conﬁdence, worry, emotionality, and physical symptoms. The various aspects
of performance anxiety are highly correlated and yet are diﬀerentially associated
with other personality traits. In the rehearsal, there is greater concern about social
interactions, whereas in the performance situation, the music becomes the focus
of concern. Furthermore, performance anxiety is associated with personality traits
that are related to neuroticism and escapism. In addition, musicians suﬀering from
performance anxiety use a range of strategies before a rehearsal or performance to
cope with the situation, even if some of these strategies may be ineﬃcient to reduce
performance anxiety.

Health Problems, Health-Promoting Behaviours and Their Eﬀects
on Music Performance and Non-Music Performance Students in
Higher Education Institutions
Jane Ginsborg 1 , Gunter Kreutz 2 , Mike Thomas 3 , Aaron Williamon 4 ;
1
  Royal Northern College of Music, UK; 2 University of Oldenburg,
Germany; 3 University of Chester, UK; 4 Royal College of Music, UK
3PM1-R01-04
The interrelationships between lifestyle, physical health and psychological wellbeing
have been studied in a number of populations. The health-promoting behaviours
of music performance students are of particular interest given the physical and
emotional demands of expert music making. The present study aimed to compare
the health behaviours of music and non-music students in higher education. It also
sought to determine the extent to which self-rated health and self-reported symp-
toms were associated with health-promoting lifestyle. Music performance students
from two conservatoires and students of nursing and biomedical science from two
universities aged 19–25 years completed the Health-Promoting Lifestyle Inventory
and an inventory of musculo- and non-musculoskeletal health problems. Music
performance students scored lower than non-music performance students on health

                                         23
Symposia                                                                    ICMPC 10

responsibility, physical activity and spiritual growth. Music performance students
rated their health, generally, as better than non-music performance students, but
reported worse symptoms. Lifestyle, self-rated health and self-reported symptoms
were associated positively and negatively. Nursing and biomedical science students
may be atypical in that they are likely to gain a greater awareness of health issues
from their studies. Nevertheless, music performance students need to adopt healthy
lifestyles in order to reach their full potential as musicians, and the evidence suggests
an urgent need for better health promotion as part of their training.


3PM1-R06 : Symposium 8 : ICMPC-Rencon: Toward New
Research on Designing Expression in Musical
Performance
Room 5, 13:30 – 15:30 Wednesday 27 August 2008, Oral session

Rencon: Performance Rendering Contest for Automated Music
Systems
Mitsuyo Hashida 1 , Teresa M. Nakra 2 , Haruhiro Katayose 1 , Tadahiro
Murao 3 , Keiji Hirata 4 , Kenji Suzuki 5 , Tetsuro Kitahara 1 ; 1 Kwansei
Gakuin University, Japan; 2 The College of New Jersey, USA; 3 Aichi
University of Education, Japan; 4 NTT Communication Laboratories,
Japan; 5 Tsukuba University, Japan
SUMMARY-8
Rencon (Performance Rendering Contest) is an annual international competition at
which entrants present the computer systems they have developed for generating
expressive musical performances and audience members and organizers judge the
performances. Recent advances in performance rendering technology have brought
with them the need for a means for researchers in this area to obtain feedback about
the abilities of their systems in comparison so those of other researchers. At the
ICMPC10, the competition will consist of an autonomous section (for evaluating
the ability of the entered systems to generate performances autonomously) and a
system-supported section (for evaluating human performances done using computer
systems). The autonomous section aims to evaluate performances rendered by
autonomous computer systems using e.g. a rule-based or case-based approach, and
the system-supported section aims to build common ground for evaluating human
performances done using computer systems.

Musical Analysis of Conducting Gestures Using Methods from
Computer Vision
Teresa M. Nakra; The College of New Jersey, USA
3PM1-R06-01
In this paper we present musical analyses and interpretations of a noted conductor’s
gestures. The analyses were enabled by computer vision techniques that tracked the
position of the conductor’s right hand from a video sequence. The resulting output
of the hand tracking system, when combined with beat and tempo data from the
audio signal, provides numerous possibilities for analyzing the conductor’s gestures
and expressive techniques. We describe the stages of the video and audio processing,
and present our analyses of the conductor’s movements.


3PM2-R01 : Symposium 9 : Musical Emotions: Eﬀect of
Structural and Performance Cues
Room 1, 15:45 – 17:15 Wednesday 27 August 2008, Oral session

Musical Emotions: Eﬀect of Structural and Performance Cues
Petri Toiviainen 1 , Tuomas Eerola 1 , Eva Istók 2 , Roberto Bresin 3 ,
Isabelle Peretz 4 ; 1 University of Jyväskylä, Finland; 2 University of
Helsinki, Finland; 3 KTH, Sweden; 4 Université de Montréal, Canada
SUMMARY-9
Most of the research on human music information processing has until now focused
on cognitive aspects, such as perception of musical elements, learning, and produc-


                                          24
ICMPC 10                                                                 Symposia

tion. Yet, one major reason for listening to music is the emotional impact it has.
For the domain of aﬀective science, therefore, music is of central concern. While a
number of aspects contribute to music-induced emotions, the content of music is
one of the most important factors. The aim of this symposium is to discuss recent
advances in music emotion research, with the focus being on the relation between
musical content and emotions. In particular, the eﬀect of structural and performance
features on perceived emotions will be discussed. Speciﬁc questions addressed in
the three presentations of the symposium include the eﬀect of structural features of
music on diﬀerent levels of abstraction on perceived emotion; the eﬀect of expressive
timing variations on the facilitation of auditory grouping and on perceived pleasant-
ness; and the eﬀect of performance features on perceived emotional quality as well
as their interaction with structural features. These questions are investigated using
interdisciplinary approaches that combine methods of musicology, psychology, and
computer modeling.

Inﬂuence of Acoustic Cues on the Expressive Performance of
Music
Roberto Bresin, Anders Friberg; KTH, Sweden
3PM2-R01-01
The main aim of this study was to investigate the optimal combination of perfor-
mance and structural parameters for obtaining a certain emotional expression. To
explore the role of the performer with respect to musical expressivity, musicians
tested a number of acoustic and structural cues.
In a factorial design, twenty subjects adjusted the values of seven performance
parameters (tempo, sound level, articulation, phrasing, transposition, instrument,
attack speed) for communicating ﬁve diﬀerent emotional expressions (neutral, happy,
scary, peaceful, sad) for each of four diﬀerent scores. The scores were speciﬁcally
composed for communicating four diﬀerent emotions (happiness, sadness, anger,
calmness).
Main results showed a general agreement with previous research in the ﬁeld of
expressive music performance. There was a tendency to use faster tempi and
louder sound level for happy and angry performances, and slower tempi and softer
sound level for sad performances. More staccato articulation and faster attack time
were used for happy and angry performances. Phrasing was used with the typical
accelerando-ritardando/crescendo-decrescendo in happy performances while it was
used in the opposite way for peaceful and sad performances. Happy performances
were transposed to a higher pitch, while sad and scary performances to a lower one.

Eﬀects of Timing Cues in Music Performances on Auditory
Grouping and Pleasantness Judgments
Eva Istók 1 , Mari Tervaniemi 1 , Anders Friberg 2 , Uwe Seifert 3 ;
1
  University of Helsinki, Finland; 2 KTH, Sweden; 3 University of
Cologne, Germany
3PM2-R01-02
By means of varying timing, dynamics, pitch, and timbre music performers put
emphasis on important events of a musical piece and provide their listeners with
acoustic cues that facilitate the perceptual and cognitive analysis of the musical
structure. Evidence exists that the speed and the accuracy with which stimulus
features are being processed contribute to how a stimulus itself is evaluated. In
our study, we tested whether expressive timing facilitates auditory grouping and
whether these timing variations inﬂuence pleasantness judgments. To this aim,
participants listened to short atonal melodies containing one or two auditory groups
and performed both a cognitive and an evaluative task. The expressive phrasing
patterns of the excerpts were gradually modiﬁed ranging from inverted phrasing
through deadpan versions to exaggerated timing patterns. Reaction times decreased
and hit rates increased with a more pronounced grouping structure indicating that
subtle timing variations alone do facilitate the formation of auditory groups in a
musical context. Timing variations also modulated the direction of pleasantness
ratings. However, the results suggest that the threshold of an expressive musical
performance to become more pleasant than its deadpan counterpart presumably can
be exceeded only by the simultaneous covariance of more than one acoustic cue.




                                        25
Symposia                                                                  ICMPC 10

Mapping Musical Features to Perceived Emotions Using Partial
Least Squares Regression
Tuomas Eerola; University of Jyväskylä, Finland
3PM2-R01-03
The aim was to uncover such musical features from audio that are relevant for percep-
tion of emotions using a speciﬁc multivariate analysis method, Partial Least Squares
(PLS) regression. An experiment was conducted where non-musicians (N=116) rated
either basic emotions or emotional dimensions (3) of ﬁlm music excerpts (n=110).
The musical features were extracted from audio ﬁles using MIR Toolbox (Lartillot &
Toiviainen, 2007). For each excerpt, the ﬁrst three moments and two other measures
were calculated across the frame-based analysis of them, resulting in 400 features for
each. PLS regression was applied to reduce the dimensionality of the musical feature
space in the analysis as this technique preserves the link between independent and
dependent variables unlike other data reduction methods (e.g., PCA). An optimum
set of predictors was discovered using PLS regression. This consisted of 4–6 latent
vectors which each explained a high proportion of covariance between the features
and ratings. In sum, a high degree of correspondence was obtained for categorical
emotion concepts (R2 =.55 - .79) and dimensional ratings (R2 =.59 - .84). Descriptors
and explanations for the latent vectors were obtained by correlating them with the
features and by visualising them.
Research on music and emotions have used high-level musical features (mode,
tension), expressive features (articulation, tempo) and low-level acoustic features
(loudness, roughness) to explain emotional expression in music. Often such studies
focus on single level of features and a wide variety of acoustically-derived features
has not been exhaustively studied due to inherent limitations of mapping techniques
(e.g. multicollinearity problems in regression), lack of computational models, or
insuﬃcient coverage of features due to pre-selection of musical material or design
constraints in analysis-by-synthesis studies.
The aim was to uncover such musical features from audio that are relevant for
perception of emotions using a speciﬁc multivariate analysis method, Partial Least
Squares (PLS) regression.
An experiment was conducted where non-musicians (N=116) rated either basic
emotions or emotional dimensions (3) of ﬁlm music excerpts (n=110). The musical
features were extracted from audio ﬁles using MIR Toolbox (Lartillot & Toiviainen,
2007). For each excerpt, the ﬁrst three moments and two other measures were
calculated across the frame-based analysis of them, resulting in 400 features for
each. PLS regression was applied to reduce the dimensionality of the musical feature
space in the analysis as this technique preserves the link between independent and
dependent variables unlike other data reduction methods (e.g., PCA).
An optimum set of predictors was discovered using PLS regression. This consisted
of 4–6 latent vectors which each explained a high proportion of covariance between
the features and ratings. In sum, a high degree of correspondence was obtained for
categorical emotion concepts (R2 =.55 - .79) and dimensional ratings (R2 =.59 - .84).
Descriptors and explanations for the latent vectors were obtained by correlating
them with the features and by visualising them.
The results extend the previous research (a) by providing a solution to the reduction
of the musical feature space and (b) by uncovering novel feature combinations. The
approach advocates using real music and sophisticated acoustic features in music
and emotion research.




                                         26
ICMPC 10                                                                   Symposia


4AM1-R01 : Symposium 10 : Experiencing Musical
Multimedia: Empirical Investigations of Cross-Modal
Perception & Cognition
Room 1, 8:30 – 10:30 Thursday 28 August 2008, Oral session

Experiencing Musical Multimedia: Empirical Investigations of
Cross-Modal Perception & Cognition
Scott D. Lipscomb 1 , Roger A. Kendall 2 , Zohar Eitan 3 , Assi Schupak 3 ,
Lawrence E. Marks 4 , Annabel J. Cohen 5 , Yee-May Siau 5 , Shin-ichiro
Iwamiya 6 , John Hajda 7 ; 1 University of Minnesota, USA; 2 University of
California at Los Angeles, USA; 3 Tel Aviv University, Israel; 4 Yale
University, USA; 5 University of Prince Edward Island, Canada;
6
  Kyushu University, Japan; 7 University of California at Santa
Barbara, USA
SUMMARY-10
The purpose of this symposium is to present international, empirically-based per-
spectives related to the cognition of the multimedia experience, with its high level
of sociological signiﬁcance. Presenters from several regions of the world (Canada,
Israel, Japan, and the United States) will share results of their most recent research.
The presentations demonstrate a number of approaches to the serious study of
multimedia, informing our current understanding of related artforms and their aﬀect
on those who experience them.

Stratiﬁcation of Musical and Visual Structures II: Visual and Pitch
Contours
Roger A. Kendall; University of California at Los Angeles, USA
4AM1-R01-01
This series of studies acknowledges the important elements of congruence and
association, but emphasizes an often overlooked domain that is in-between, or a
composite, of these two elements. In Experiment 1, animations in two-dimensional
space were created consisting of a simple ﬁlled-circle that moves in x-y space.
Patterns of ascending and descending ramps, arches, and undulations, used in
previous experiments, were superimposed. These were accompanied by two-part
pitch patterns. For Experiment 2, the relation of visual and melodic contour was
investigated using monophonic pitch and monovisual animations. Animations
consisted of an arch pattern, a circle, a triangle, a square, a pentagon, a hexagon, and
a stair structure. The single pitch pattern combined with these visual structures was
a melodic arch with 8 equally-spaced contour inﬂection points. Finally, Experiment
3 was based on several patterns used in Experiment 1 that crossed at C5, creating
a paradoxical composite that could be interpreted in several ways. Musical pitch
patterns were created using the piano tones as in Experiment 1 and also with two
timbres, piano and oboe; visual patterns included monochromatic vs. multiple
colors. In all experiments subjects rated the degree of ﬁt between the visual and
musical composites. Repeated measures ANOVA using MGLH generally showed that
hypothesized patterns of pitch and visual motion have the strongest perceptual ﬁt,
with some notable exceptions.

Louder is Higher: Cross-Modal Interaction of Loudness Change
and Vertical Motion in Speeded Classiﬁcation
Zohar Eitan 1 , Assi Schupak 1 , Lawrence E. Marks 2 ; 1 Tel Aviv
University, Israel; 2 Yale University, USA
4AM1-R01-02
Garner’s speeded discrimination paradigm was used to determine whether loudness
change and spatiovisual vertical motion interact perceptually. 32 participants
(16 musically trained) served in 2 experiments, where auditory stimuli (1000 Hz
sinusoids) increasing or decreasing in loudness accompanied visual stimuli (dots on
a screen) that simultaneously moved up or down. Participants rapidly discriminated
values in one, “relevant,” stimulus (auditory in Expt1, visual in Expt2) while ignoring
the other. Each experiment included 2 baseline conditions, 2 correlated conditions
(congruent and incongruent), and an orthogonal condition. Two outcomes would



                                          27
Symposia                                                                  ICMPC 10

indicate interaction: Garner interference, (Orthogonal condition RT > baseline RT),
implying failure to attend selectively to the relevant stimulus, and congruence eﬀects
(RTs, incongruent stimuli > RTs, congruent stimuli). Results in Exp1 (auditory
discrimination) indicate signiﬁcant congruence eﬀects both within the orthogonal
condition and between correlated conditions. Exp2 (visual discrimination) gave no
signiﬁcant congruence eﬀects. No Garner interference emerged in either experiment.
Congruence eﬀects were larger for musically untrained participants. Results suggest
that loudness change and visually perceived vertical motion interact perceptually.
The combination of congruence eﬀects and no Garner interference conforms to a
model in which information deriving from auditory and visual stimuli combines into
a single variable, continuously compared to bipolar response boundaries.

The Narrative Role of Music in Multimedia Presentations: The
Congruence-Association Model (CAM) of Music and Multimedia
Annabel J. Cohen, Yee-May Siau; University of Prince Edward Island,
Canada
4AM1-R01-03
The present paper presents recent empirical work conducted within the context of
the Congruence-Association Model of music in multimedia. Participants watched a
20-minute “silent” ﬁlm twice with either an appropriate, inappropriate, or no music
background. They responded as quickly as possible when an extraneous target “X”
appeared in the corner of the screen at intervals of approximately 1 min. After the
ﬁrst (warm-up) presentation, response time was signiﬁcantly slower with the original
music as compared to the no-music condition. This ﬁnding was consistent with the
view that appropriate music increased the narrative coherence of the presentation
and inhibited detection of information extraneous to the story. One-half the partici-
pants were instructed to attend to the story, so as to answer some questions later
about it. Results were consistent with the notion that music may play a similar role
to telling a viewer-listener to attend to the story. Of secondary interest, response
time was signiﬁcantly correlated with individual diﬀerence measures on both the
Tellegen Absorption Scale and experience with video games.

Subjective Congruence Between Moving Picture and Sound
Shin-ichiro Iwamiya; Kyushu University, Japan
4AM1-R01-04
To make impressive visual media production, subjective congruence between motion
picture and sound is important. It is indicated that subjective congruency between
motion picture and sound has two aspects. One is formal congruency: the matching
of auditory and visual structures. The other is semantic congruency: the similarity
of the auditory and visual meanings. The synchronization of auditory and visual
accents of audio-visual stimuli created formal congruency. The similarity between
auditory and visual impressions evoked from audio-visual stimuli created semantic
congruency. Rating experiments showed both types of audio-visual congruencies
had eﬀects to raise subjective congruence. Furthermore, the relationship between
the transformation of visual images and the pitch or loudness pattern of sound
also contributed to rising subjective congruence. For example, combination of an
ascending pitch scale and an enlarging image pattern created subjective congruence.
Combination of a descending pitch scale and a reducing image pattern created
subjective congruence. Furthermore, a combination of an ascending pitch scale and
a sliding movement from left to right and that of a descending pitch scale and a
sliding movement from right to left. Vertical correspondence of direction between
movement of image and pitch shift had strong eﬀect on the subjective congruence.




                                         28
ICMPC 10                                                                  Symposia


4AM2-R01 : Symposium 11 (Invited) : Animal Calls,
Music, and Language: Search for Common Themes in
Evolution
Room 1, 10:40 – 12:40 Thursday 28 August 2008, Oral session

Animal Calls, Music, and Language: Search for Common Themes in
Evolution
Kazuo Okanoya 1 , Aniruddh D. Patel 2 , Yoichi Inoue 3 , Thomas
Geissmann 4 , Ryo Oda 5 , Kazutoshi Sasahara 1 ; 1 RIKEN Brain Science
Institute, Japan; 2 The Neurosciences Institute, USA; 3 Nishimaizuru
High School, Japan; 4 Zürich University, Switzerland; 5 Nagoya
Institute of Technology, Japan
SUMMARY-11
Animal calls, music, and language are vocal behaviors that depend upon precise
motor control and auditory feedback. By comparing these behaviors, we can ﬁnd
common themes in evolution that are constrained by environmental, psychological,
and physiological factors.

Empirical Comparisons of Pitch Patterns in Music, Speech, and
Birdsong
Aniruddh D. Patel 1 , Adam T. Tierney 1 , Frank A. Russo 2 ; 1 The
Neurosciences Institute, USA; 2 Ryerson College, Canada
4AM2-R01-01
Musical melodies are characterized by certain statistical regularities. For example,
large intervals (“skips”) are often followed by reversals, and phrases often have an
arch-like shape and ﬁnal durational lengthening. These regularities could reﬂect
motor constraints on pitch production or the melodic characteristics of speech. To
distinguish between these possibilities we compared pitch patterns in instrumental
musical themes, sentences, and birdsongs. Patterns due to production-related con-
straints should be present in all three domains, whereas patterns due to statistical
learning from speech should be present in speech but not birdsong. Sequences were
taken from classical music of 5 countries, sentences from 4 languages, and songs of
56 songbird families. For sentences and birdsongs each vowel/note was assigned one
pitch. For each sequence we quantiﬁed patterns of post-skip reversals, the direction
of the initial and ﬁnal interval, the relative duration of the ﬁnal vowel/note, and
pitch contour shape. Final lengthening and post-skip reversals predominated in all
domains, likely reﬂecting shared motor constraints; the latter may result from skips’
tendency to take melodies toward the edges of the pitch range, forcing subsequent
reversals. Arch-like contours were found in music and speech but not birdsong,
possibly reﬂecting an inﬂuence of speech patterns on musical structure.

Vocal-Auditory Segmentation of Sound Sequence in Songbirds and
Human Babies
Kazuo Okanoya, Miki Takahasi, Noriko Kudo; RIKEN Brain Science
Institute, Japan
4AM2-R01-02
Segmentation and chunking are characteristics that are shared by human speech,
music, and bird song. Male Bengalese ﬁnches sing complex songs with deterministic
and probabilistic note-to-note transitions. When chicks were reared in a multi-tutor
environment, they eventually learned to sing complex songs that contain 1–3 tutor
songs segmented and arranged in personalized fashion. In humans, we recorded
brain activity when hearing sound stream that were organized to have high- and
low-transition points. At the scalp electrode placed at FCz, we found a negative
potential corresponded with segmentation. The strength of the potential was corre-
lated with the degree of learning to segment the statistical sequence, suggesting that
this potential could be a marker for sound segmentation. The same procedure was
applied to examine whether new-born infants can also segment ongoing statistical
sequence. We found a positive potential corresponded with segmentation at the
same electrode. Thus, right after the birth, infants can also demonstrate ability
of segmentation. Results suggest that both in birds and humans segmentation of
ongoing sound is enabled by the higher auditory-vocal integration area and by the


                                         29
Symposia                                                                 ICMPC 10

basal ganglia and this ability is probably innately prepared in humans and birds and
help them to acquire songs, music, and language.

Gibbon Song Syntax Decodes Behavioral Contexts
Yoichi Inoue 1 , Shigeto Yosida 2 , Kazuo Okanoya 2 ; 1 Nishimaizuru
High School, Japan; 2 RIKEN Brain Science Institute, Japan
4AM2-R01-03
Gibbons are small, arboreal apes distributed throughout the tropical rainforests of
Southeast Asia. Gibbons are known to produce songs. But, there has been little study
about the syntax of gibbon song. So, I have conducted ﬁeld observation in northern
Borneo (Danum Valley Conservation Area). I followed one gibbon group for one week
in succession in August and December every year since 2001. The data collection
method is to record the voice, while collecting the activity data. From 2001 to 2007,
I followed one group for a total of 82 days and collected 63 male songs. The Muller’s
gibbon (Hylobates Muelleri) lives in northern Borneo. Concerning the organization of
this gibbon’s song, the details were reported by Haimoﬀ (1985). According to him,
male songs consist of wa and oo notes. I examined the male songs recorded in my
research acoustically and conﬁrmed it. I studied whether one male gibbon named
Sigyu changed the combination of wa and oo notes in various situations. As a result,
the orders of 3 successive notes in his songs were observed to be diﬀerent according
to the situations. These observations suggest that gibbon song syntax may decode
behavioral contexts.

Song-Diversity in a Gibbon Species: The Silvery Gibbon (Hylobates
Moloch) from Java (Indonesia)
Thomas Geissmann; Zürich University, Switzerland
4AM2-R01-04
Because male silvery gibbon songs generally exhibit a higher degree of structural
variability than female songs, the syntactical rules and the degree of variability in
male singing have rarely been examined. The unusual rarity of male singing in
the silvery gibbon makes such a study particularly challenging. The results can
be summarized as follows: (1) Males appear to exhibit individual preferences in
the order of diﬀerent note types used in their phrases. (2) Male phrase variability
both within and between individuals appears to be higher in silvery gibbons than
in most, perhaps all, other gibbon species. This high variability appears to be a
derived characteristic among the Hylobatidae. I will discuss the implications of this
ﬁnding for the interpretation of song function. It appears that song function cannot
be identiﬁed for “the gibbon”. Gibbon songs appear to be multi-functional, and the
relevance of these functions appears to exhibit strong diﬀerences among gibbon
species.




                                        30
ICMPC 10                                                Monday 25 August 2007


1PM1-R02 : Pitch
Room 2, 13:00 – 15:00 Monday 25 August 2007, Oral session

The Correlation Between Absolute Pitch and Adaptation to
Transposed Keyboards
Keun-Ah Jeon 1 , Suk Won Yi 1 , Kyungil Kim 2 ; 1 Seoul National
University, Korea; 2 Ajou University, Korea
1PM1-R02-1
The purpose of this study is to see whether there is a correlation between absolute
pitch(AP) and sight-reading ability when playing on a transposed keyboard. A pilot
survey showed that people assume AP possessors to have more diﬃculty using
transposed keyboards than non-AP possessors do. The majority of the 25 college
students with musical experience reported that the possession of AP may negatively
aﬀects sight-reading on transposed keyboards, resulting in perceptional confusion,
psychological strain and mistakes in performance.
In order to investigate the correlation between AP index and sight-reading ability on
transposed keyboard, two experiments were conducted:
   1. The measuring of each subject’s AP index. Subjects were instructed to identify
      the pitch class of presented tones.
   2. The measuring of the adaptability to transposed keyboards. Subjects were
      asked to sight-read given scores on a keyboard in both the original and trans-
      posed key. When played in the transposed key, the subjects hear diﬀerent pitch
      classes from those notated on score.
The result of the correlation analysis showed that AP index has no signiﬁcant eﬀect
on the adaptability to transposed keyboards in contrast to the pilot survey. In
addition, it appeared that the correlation between AP index and ability to sight-read
was not signiﬁcant.

Simultaneous Pitch Perception in Absolute and Non-Absolute Pitch
Possessors
David J.T. Marco, Neil McLachlan, Sarah Wilson; University of
Melbourne, Australia
1PM1-R02-2
This study explored the perception of simultaneous pitches between musicians with
and without absolute pitch (AP). Listeners performed estimations of pitch number
and pitch height for musical chords of increasing inharmonicity and spectral density.
Sounds consisted of one, two or three simultaneous pitches. Results showed that
AP performance accuracy decreased signiﬁcantly for all multi-pitch sounds in a
similar manner to non-AP (NAP) musician performance. In contrast, performance
eﬀects due to increasing inharmonicity and spectral density were only witnessed in
NAP musicians. Comparison of pitch height judgment accuracy and pitch number
estimation results provided evidence that pitch number estimation is not based on
prior pitch extraction mechanisms as proposed in multiplicity models. The reduced
ability to identify pitches in multiple pitch stimuli is likely to be due to reductions
in the salience of individual pitches caused by increased masking of higher-order
harmonics and co-modulation of harmonics occurring within the same critical band.

Perceived ‘Closeness’ in Pitch Depends in Part on Perceived
‘Closeness’ in Time: Further Support for an Auditory Motion
Hypothesis
Molly J. Henry, J. Devin McAuley; Bowling Green State University, USA
1PM1-R02-3
Support for an auditory motion hypothesis was previously provided by a demon-
stration that increasing the pitch velocity of three-tone melodies increases the
magnitude of the auditory kappa eﬀect, which is characterized by dependence of
time judgments on to-be-ignored variations in pitch (Henry & McAuley, in press). The
aim of the current study was to test the auditory motion hypothesis with respect to
the auditory tau eﬀect, which is characterized by the dependence of pitch judgments
on to-be-ignored variations in timing. Participants judged the relative pitch of a
target tone embedded in three-tone ascending sequences, while ignoring changes to
the target’s timing; pitch velocity was varied between subjects. In accord with an


                                         31
Monday 25 August 2007                                                     ICMPC 10

auditory motion hypothesis, increasing the pitch velocity of the sequences increased
the magnitude of the auditory tau eﬀect. Findings were supported by quantitative
ﬁts to an imputed pitch velocity model. Musical training was not correlated with
magnitude of the auditory tau eﬀect. Generally, the results of the current study
provide support for the inherent interdependence of the dimensions of pitch and
time in music perception.

Exposure to Ambiguous Tone Sequences Induces Short-Term
Plasticity in Pitch Perception
A. Seither-Preisler 1 , L. Johnson 2 , S. Seither 2 , B. Luetkenhoener 2 ;
1
  University of Graz, Austria; 2 Muenster University Hospital, Germany
1PM1-R02-4
Eleven non-musicians were tested with the Auditory Ambiguity Test (AAT), which
assesses a subject’s tendency to categorize ambiguous two-tone sequences either
according to their concrete physical sound attributes (present upper harmonics) or
to their implicit tonal meaning (virtual pitches at the respective missing fundamental
frequencies). The exposure group (N=5) was repeatedly tested with the AAT over
a period of 8 weeks (1 session per week; 3 consecutive AAT presentations). The
training group (N=6) underwent the same procedure, but received training between
the 3 presentations. There was one block of spectral training with the ‘Noise
Shift Test’ and one block of virtual pitch training with the ‘Present Fundamental
Frequency Test’. The AAT scores increased steadily over the eight weeks period,
both in the exposure group (p=0.0012) and the training group (p=0.011), regardless
of the type of training (p=0.59), thus indicating a general increase in the salience of
virtual pitch sensations. The same eﬀect, although weaker, was observed over the 3
subsequent AAT presentations of a single session (exposure group: p=0.062; training
group: p=0.01). These results suggest that short-term exposure to harmonic sounds
enforces virtual pitch perception regardless of explicit training.


1PM1-R03 : Rhythm, Meter and Timing I
Room 3, 13:00 – 15:00 Monday 25 August 2007, Oral session

Investigating the Human-Speciﬁcity of Synchronization to Music
Aniruddh D. Patel 1 , John R. Iversen 1 , Micah R. Bregman 1 , Irena
Schulz 2 , Charles Schulz 2 ; 1 The Neurosciences Institute, USA;
2
  Birdlovers Only Rescue Service Inc., USA
1PM1-R03-1
One universal of human music perception is the tendency to move in synchrony
with a periodic beat (e.g., in dance). This response is not commonly observed in
nonhuman animals, raising the possibility that this behavior relies on brain circuits
shaped by natural selection for music. Consequently, if a nonhuman animal can
acquire this ability, this would inform debates over the evolutionary status of music.
Speciﬁcally, such evidence would suggest that this ability did not originate as an
evolutionary adaptation for music. We present data from an experimental study of
synchronization to music in a Sulphur-crested cockatoo (Cacatua galerita eleanora),
“Snowball”, who spontaneously dances in response to certain music (see YouTube:
“dancing cockatoo”). Snowball’s preferred song was presented at diﬀerent tempi
(original, +/- 2.5%, 5%, 10%, 15%, and 20%), and his rhythmic movements while
dancing were quantiﬁed from video. The results reveal occasional bouts of synchro-
nization at a subset of these tempi on ∼20% of the trials. This demonstrates that a
nonhuman animal can synchronize to a musical beat, though with limited reliability
and tempo ﬂexibility. These ﬁndings are consistent with the “vocal learning and
rhythmic synchronization” hypothesis, which suggests that vocal learning provides
the auditory-motor foundation for synchronization to a musical beat.

Identifying Timing by Sound: Timbral Qualities of Micro-Rhythm
Anne Danielsen 1 , Carl-Haakon Waadeland 2 , Henrik G. Sundt 3 ;
1
  University of Oslo, Norway; 2 Norwegian University of Science and
Technology, Norway; 3 Norwegian Network for Technology Acoustics
and Music, Norway
1PM1-R03-2
Temporal “deviations” from a given reference grid, such as in standard musical


                                         32
ICMPC 10                                               Monday 25 August 2007

notation, are well known and an identiﬁed characteristic of music performance.
Yet, when a musician performs temporal patterns, strategies of movement and
interaction with the instrument also aﬀect the sound produced. Hence, sound and
micro-timing are invariably closely related. In this paper we provide a report from
an experiment aimed at investigating how variations in micro-timing are reﬂected
through variations in sound. Five experienced drummers performed a rock groove
in three diﬀerent tempi, whereby the performances were subjected to three diﬀerent
playing conditions: a) playing as “natural” as possible, b) playing “laid-back”, and,
c) playing “pushed”. Our focus was on how these diﬀerent playing conditions inﬂu-
enced the timbral content of the snare drum sound. In this experiment the recorded
sounds of the snare drum strokes were isolated and analysed for spectral content
by using FFT, where the results showed that in a majority of the performances the
sound of the snare drum is characteristically altered when the drummer attempts
to play laid-back or pushed. This demonstrates that timbral content might contain
information regarding micro-temporal positioning, and, moreover, that timbre needs
to be taken into account in studies that deal with micro-timing.

Stealing Time: How Grace Notes Can Be Added
Peter Desain, Renee Timmers; Radboud University Nijmegen, The
Netherlands
1PM1-R03-3
Previously collected data was re-analysed for data-driven appearance of distinct types
of grace notes. Grace notes are one-note ornaments that are special in at least two
ways: their duration is relatively inﬂexible over performances in diﬀerent tempi, and
they are timed by “stealing” time from surrounding notes rather than by inserting
time. Two types of graces are usually distinguished that are either performed in time
of the previous note, or in time of the main note.
Using time-shifts of note onsets between without and with grace note performances
in seven tempi, the proportion of the grace note duration stolen from the previous
note, the main note, and inserted was calculated. Additionally, the duration of the
grace note at tempo 60 was measured and the slope of tempo scaling of the grace
duration. These ﬁve parameters were used as input to a clustering analysis. The
time-shifts proﬁles showed an amazing consistency in time-shifts over repeated
performances. They also revealed diﬀerent behaviour across participants. Their
musical interpretations fall within two groups, as reﬂected in a clear clustering of
parameter values for time stealing and grace note duration. The slope of scaling with
tempo seems to allow for freedom on a continuous range.

Exploration and Imitation of the Timing of Grace Notes
Renee Timmers, Peter Desain; Radboud University Nijmegen, The
Netherlands
1PM1-R03-4
The eﬀectiveness of visual feedback to assist in the exploration and imitation of
the timing of grace notes, which are short ornamental notes, was investigated in an
experimental study. The ability of piano students to imitate target performances
with diﬀerent grace note timing was tested before and after a training session.
During training, the piano students explored ways of grace note performance with
the instruction to explore grace note timing in order to improve the imitation task
in the posttest. Half of the group of participants received visual feedback during
training. The results showed no signiﬁcant improvement of the imitation of the
target performances between pretest and posttest and no signiﬁcant eﬀect of visual
feedback, suggesting that training using exploration was not helpful. However, the
visual feedback did inﬂuence the timing of ornaments in the exploration training.
Although overall participants in the no-feedback condition explored the timing of
the grace note and surrounding notes more freely than the participants in the visual
feedback condition, participants who received visual feedback explored diﬀerent
types of grace note timing more systematically.




                                         33
Monday 25 August 2007                                                     ICMPC 10


1PM1-R04 : Education I
Room 4, 13:00 – 15:00 Monday 25 August 2007, Oral session

Learning Rhythm: The Impact of Visual Presentation
Frank Heuser 1 , Scott D. Lipscomb 2 , Glenn Pohland 2 ; 1 University of
California at Los Angeles, USA; 2 University of Minnesota, USA
1PM1-R04-1
This investigation aims to determine whether the visual organization of rhythmic
information impacts learning by comparing two approaches. The repetitive approach
presents a speciﬁc rhythm in the ﬁrst measure of a four measure, visually discrete
exercise with the same rhythm repeated in each measure. In the varied approach,
diﬀerent rhythm patterns are presented in each measure of a continuous exercise in
which no pattern is repeated and with no visual separation of presented patterns.
The study seeks to determine whether the repetitive or varied approach results in
greater eﬃcacy in learning.
Sixth grade beginning instrument students participated. Treatment commenced
at the beginning of the second semester of instruction, after tone production was
established. Regular classroom teachers provided rhythmic instruction to students
and were asked to supplement their teaching with researcher-provided instructional
materials for ﬁve minutes every class period. These materials used either the
repetitive or varied approach and incorporated rhythms studied during the second
semester. After two months of instruction, students were tested on their ability
to perform the rhythms on their instruments. Results of the present study will
provide empirical evidence to either conﬁrm or disconﬁrm this hypothesis, providing
useful cognition-based information for use in the creation of the next generation of
beginning instrumental methods texts.

Teaching Expressivity to Advanced Instrumentalists
Katie Zhukov; Sydney Conservatorium of Music, Australia
1PM1-R04-2
Research into teaching of expressivity typically uses questionnaires and interviews to
ascertain the teaching methods employed by instrumental instructors and the prac-
tising strategies of students. Little is known regarding the importance of expressivity
in instrumental teaching in higher education or the role gender diﬀerences may play
in its teaching. This study takes an encompassing view of expressivity by including
tempo, articulation, dynamics and emotion in its deﬁnition as suggested by recent
research. It re-examines observational data on lesson content from 24 instrumental
music lessons in higher education by amalgamating the scores in four categories
of tempo, articulation, dynamics and emotion as expressivity and comparing this
to the scores in technique. The individual aspects of expressivity are re-calculated
as percentages of expressivity to ascertain their relative importance with regard to
gender. Results show greater emphasis on teaching of expressivity than technique,
with subtle gender diﬀerences emerging between the two teacher groups, the two
students groups and the four same-gender/ diﬀerent-gender pairings. Findings
support the new concept of expressivity and challenge traditional views on the
importance of teaching technique to advanced instrumentalists. Gender diﬀerences
in emphasis on particular aspects of expressivity add to our understanding of
instrumental teaching in higher education.

The Signiﬁcance of Qualitative Approach in the Research of
Musical Cognition: A Study Methodology Using the Concept of
Inter-Subjectivity
Yasuko Murakami; Kyoritsu Women’s University, Japan
1PM1-R04-3
The aim of this study is to explore the signiﬁcance of a qualitative approach to musi-
cal cognition research using the concept of inter-subjectivity in order to understand
the internal sense of the player.
This paper will ﬁrst give a brief overview of past research on the principle and
methodology of qualitative approaches to educational psychology. It also looks at
research that has used the concept of inter-subjectivity. Secondly, there will be a
brief presentation of the author’s research on piano lessons using such qualitative
methods. Third is a comparison of the results of previous scientiﬁc research with


                                         34
ICMPC 10                                                Monday 25 August 2007

those of qualitative research and clariﬁcation of the distinction between them.
Finally, the paper suggests the signiﬁcance of using a qualitative approach in music
cognition research.
Using the concept of inter-subjectivity allows us to glimpse the subjectivity of
other people through our own subjectivity. This paper proposes the feasibility of
implementing the methodology used here to clarify the internal sense of the player.

73 Forms of Actions (Diverse Exercises) Included in German
Gehoerbildung-Books Published Between 1889 and 1983
Luis Estrada-Rodríguez; Universidad Nacional Autonoma de Mexico,
Mexico
1PM1-R04-4
This paper presents a part of a larger research project in all the didactic components
(exercises, ﬁelds of exercises, thematic contents and sequences) found in the most
used Gehoerbildung-books between 1889 and 1985 in Germany. The author focuses
on the analysis of 1209 exercises included in these books. Some cognitive functions
have been used in these books together with musical actions as consecutive or
simultaneous components of this kind of exercises. The basic forms of action
(listening, singing, playing, reading, writing, analysis, recognition, memorization
and others) as well as their combination constitute a total of 73 forms of actions
used by the authors of the analyzed books. Some of these forms of action are little
known outside Germany; they are thought to help the student develop the internal
audition and are alternatives to take a dictation, one traditional exercise used with
skepticism today. An important part of the didactic strategies of the authors rely on
the applied exercises. These have also a strong inﬂuence on the future behavior of
the students. However, there are various problems that make the evaluation diﬃcult
of the contribution of these exercises to the overall development of the student in an
objective way.


1PM2-R02 : Memory
Room 2, 15:15 – 17:15 Monday 25 August 2007, Oral session

Musical Change Deafness: The Inability to Detect Change in a
Non-Speech Auditory Domain
Kat R. Agres, Carol L. Krumhansl; Cornell University, USA
1PM2-R02-1
This article presents two experiments investigating the degree to which listeners
can detect changes in melodies. In both studies, pairs of melodies were presented
to a group of professional musicians and a group of non-musicians. In Experiment
1, musical structure and musical expertise were explored with stylistic, non-stylistic,
and random melodies. Experiment 2 utilized a full-factorial design to examine
tonality, musical interval, metrical position, note duration, and musical expertise.
Signiﬁcant eﬀects were found for several variables, but tonality had a particularly
large eﬀect on performance. Under some conditions, large changes between the
melodies went undetected even by professional musicians. These results suggest
that listeners form a memory representation for schematically consistent tones,
which we refer to as the “musical gist”. These results also suggest a comparison with
change blindness, in which viewers can fail to notice salient changes in a visual scene,
raising the question of whether similar processing operates in both modalities.

The Eﬀect of Timbre and Pitch Level on the Suzuki Violin
Student’s Processing of Familiar Melodies
Crystal Peebles; Florida State University, USA
1PM2-R02-2
The Suzuki method teaches students to play the violin by ear, focusing on daily lis-
tening and review of older repertoire. This study builds on work by Saah and Marvin
(2004) to determine whether students who learn violin using this pedagogical method
encode Suzuki repertoire with absolute pitch and timbre information. If so, then
changing timbre and key might inﬂuence the student’s ability to perform an error-
detection task on a melody from the Suzuki repertoire. Thirty-eight Suzuki-trained
students (ages 6–16) heard two-measure excerpts and evaluated whether they were
correctly performed. Both familiar key and familiar timbre were shown to improve


                                          35
Monday 25 August 2007                                                     ICMPC 10

performance: participants detected errors signiﬁcantly faster and more accurately
when melodies were played in their original key (p < .001) or on the violin (p < .001)
and older children performed the task slightly faster and more accurately and than
younger children, although this diﬀerence in performance was not signiﬁcant. These
results demonstrate that timbre and pitch level aﬀect processing in Suzuki students,
suggesting that these parameters may be encoded along with musical intervals and
contour.

Modelling Memory Responses in a Melody Recognition Task
Andrea R. Halpern 1 , Daniel Müllensiefen 2 , Geraint Wiggins 2 ;
1
  Bucknell University, USA; 2 Goldsmiths University of London, UK
1PM2-R02-3
Most empirical studies investigating memory for melodies relate memory perfor-
mance to a number of inﬂuencing and experimentally controlled factors. Memory
performance is usually deﬁned as a function of the correctly recognised items. In
contrast to this conventional approach, we seek a more comprehensive model of
melodic memory that predicts all types of memory response in a melodic recognition
task, i.e. that also explains memory failures. The predictions are based on a set of
algorithmically extracted analytical features of the melodic stimuli. In a presentation
phase, 21 untrained participants rated the familiarity of 40 short melodic phrases.
In the test phase, the participants were presented with 80 melodic phrases, half of
which they had listened to in presentation. The participants’ explicit and implicit
memory was assessed by two rating judgements for each melody item. Employing
several diﬀerent statistical modelling techniques we show how the diﬀerent analytical
features and their combinations act upon the subjective memorability of a melody.
From the prominence of the analytical features within the statistical model, we
discuss how the presence of certain melodic structures can lead to speciﬁc subjective
memory responses. The relationship between the traditional approach to musical
memory analysis and this new one is discussed.

An Exploration of How Music Can Modify Long Term Memory
Sherilene M. Carr, N.S. Rickard; Monash University, Australia
1PM2-R02-4
Physiological arousal induced by emotion has been identiﬁed as an important
modulator of long term memory consolidation. Animal and human research has
shown elicitation of emotion can improve memory for an event compared to control
non-emotion conditions, and this enhancement is believed to be mediated by the
beta-adrenergic system. The clinical potential of emotional arousal to be used as a
mnemonic strategy is exciting but limited by at least two methodological barriers.
First, this research has tended to focus upon intrinsic sources of emotional arousal
in that the emotion is an integral part of the material to be remembered. Such
methodology typically involves participants viewing disturbing images combined
with a negative narrative. Second, the majority of experimental evidence has relied
on the use of negatively valent content to induce emotion; clearly this method would
be undesirable in applied settings. Emotionally powerful music is a source of arousal
induction not yet exploited in physiological models of memory-emotion research.
Music is unique in that it has the potential to increase physiological arousal while
at the same time infusing aﬀect into otherwise neutral material. This research
will investigate the utility of using emotion inducing music as an extraneous and
externally valid source of emotional arousal that may then be used to enhance
memory for otherwise neutral material.


1PM2-R03 : Performance I
Room 3, 15:15 – 17:15 Monday 25 August 2007, Oral session

Priming Preferred Tempo in Multi-Sectional Music
Peter Martens; Texas Tech University, USA
1PM2-R03-1
In most experiments studying preferred tempo (tactus), participants are kept met-
rically naïve, and the preferred tempo of each piece is deﬁned from their unbiased
responses. Much music that we experience in our daily lives, however, is multi-
sectional. Logically, preferred tempo in subsequent sections of music might be
“primed” by the preceding music. A pilot and follow-up experiment were conducted



                                         36
ICMPC 10                                               Monday 25 August 2007

to investigate how preferred tempo in a piece’s second section (generically, the B
section) was inﬂuenced by a preexisting preferred tempo from the piece’s A section.
Participants tapped their preferred tempo in response to 15 excerpts of diverse
genres and styles, hearing either a “primed” (A-B) or “unprimed” (B only) version
of each excerpt. The junctures between A and B sections in these excerpts were
of two types. In seven excerpts, the metric structure of A and B sections were
similar, with at most two layers added or subtracted across the sectional boundary.
Participants’ preferred tempo in the B section of these excerpts was signiﬁcantly
diﬀerent under primed and unprimed conditions for all seven excerpts. In the
remaining eight excerpts, the A and B sections shared at most two metrical layers.
Only two of these eight excerpts showed signiﬁcant diﬀerences between conditions,
showing the ease with which participants experienced these musical boundaries as a
kind of reset button for preferred tempo. This study demonstrates the malleability
of preferred tempo given a preceding metrical context, and explores the types of
musical boundary that will most aﬀect the preferred tempo of subsequent musical
sections.

MuSA.RT and the Pedal: The Role of the Sustain Pedal in
Clarifying Tonal Structure
Elaine Chew, Alexandre R. J. François; University of Southern
California, USA
1PM2-R03-2
Pianists use the sustain pedal to clarify and project tonal structure in performance.
The eﬀect of sustain-pedal use on the projected tonal structure, while potentially
useful to piano pedagogy and automated transcription, remains a little-studied
phenomenon. Our goal is to discover ways to model and measure, quantitatively,
the eﬀect of the sustain pedal on tonal coherence. We use the MuSA.RT system
for tonal analysis and visualization to capture the tonal patterns analyzed from
a pianist’s performance of Bach’s Prelude No. 1 from The Well-Tempered Clavier,
paced by a metronome, with and without the use of the sustain pedal. The analysis
is based on the Spiral Array Model, as implemented in MuSA.RT. Tonal contexts are
mapped to short-term and long-term centers of eﬀect (CEs) that trace out spatial
trajectories over time. The likelihood of a triad/key is given by the distance between
the short-term/long-term CE and that triad/key. We present quantitative results that
show that increased tonal coherence in a pedaled performance can be observed as
stronger likelihood of the nearest keys (i.e. shorter distances between the long-term
CE and closest key), and that use of the sustain pedal results in smoother transitions
to and from the nearest triads.

On the Eﬀect of Performance Evaluation in Acquiring Samba
Rhythm
Masato Kawakami, Tsutomu Fujinami; JAIST, Japan
1PM2-R03-3
It is diﬃcult for anyone to acquire a new rhythm if it is unfamiliar to him. It is
diﬃcult, for example, for Japanese to acquire a Samba rhythm as its rhythmic
structure is so diﬀerent from the ones they have been familiar with. To study how
one learns to play a new rhythm, we investigated the process in which Japanese
subjects are trained to play Samba rhythms. We employed 18 subjects to separate
them into three groups. An instructor showed all of them how to play Samba rhythm
with a shaker. Two groups were provided with additional information, which was
calculated based on the data obtained from acceleration sensors put on their wrists.
We calculated for each subject’s acceleration data its auto correlation function
to compare it with the instructor’s by calculating the cross correlation function
between them. The cross correlation function represents how his arm movement is
close to the one of the instructor, which we regard as an index to evaluate his or
her performance. Of the two groups given additional information, one group was
only informed whether or not his performance was correct. The other group was
furthermore shown his auto correlation along with the instructor’s, expressed as
wave form on computer display. We compared these three groups to investigate
how the diﬀerent instructions aﬀected the learners. It turned out the group who
were shown auto correlation as wave forms learnt better than the group who were
not given any information of their wrist movement. The group who were only told
whether his performance was correct did however not outperform the group without
any feedback. The result suggests that the learner needs a direction, not just a
judgment, to acquire a new rhythm when they imitate the instructor.


                                         37
Monday 25 August 2007                                                    ICMPC 10

A Pianist’s Expression in the Role of Co-Performer: Changes in
Timing and Dynamics Through Communicative Interaction with a
Violinist
Yuriko Kubota; Independent Researcher, Japan
1PM2-R03-4
This paper is based on Seashore’s theory of ‘aesthetic deviation from the regular’
(1938, p. 9). Studies of expression have been carried out by many researchers.
Although studies of performance and social interaction are currently mainstreams of
psychology of music research, the study of expression in ensemble performance has
been paid relatively little attention. This study aims to investigate how a pianist’s
expression changes in timing and dynamics when playing with a violinist, through
four conditions: solo performance, performance before and after rehearsal, and
performance nineteen days after the rehearsal. A further purpose of the study
is an exploration of the eﬀects of the performers’ interactions in rehearsal and
performance on expression. The experiment took place at the University of Sheﬃeld;
Mozart’s Sonata for Violin and Piano K.454 was played by two student participants.
Quantitative research with MIDI data and supplementary data, comprising audio and
video recording data, and qualitative data from semi-structured interviews were used
for analysis. Findings included changes in the style of expression, the occurrence
of diﬀerent metrical features, and a ‘characteristic’ expressive style in the piano
accompaniment. In ensemble performance, the treatment of temporal structures
may be critical for an eﬀective expressive style.


1PM2-R04 : Development I
Room 4, 15:15 – 17:15 Monday 25 August 2007, Oral session

Categorization of Melody During the First Year of Life
Eugenia Costa-Giomi, Leslie Cohen, Danielle Solan, Ashley Borck;
University of Texas at Austin, USA
1PM2-R04-1
The ability to recognize a melody despite variations in some of its musical char-
acteristics is essential for the understanding of music. The cognitive process of
recognizing equivalence among stimuli that are not the same is called categorization.
In order to group similar stimuli under the same category, listeners must disregard
perceived diﬀerences among the stimuli and focus on their common properties.
In other words, categorization requires the perception of both diﬀerences and
similarities between stimuli.
We completed a series of experiments with 7- and 11-month olds to study the
development of melodic and timbre discrimination and categorization during the
ﬁrst year of life. A habituation-novelty preference procedure was used. We found
that 7- and 11-month olds can discriminate between instruments playing the same
melody and can also discriminate between melodies played by the same instrument.
We then proceeded to study categorization by determining whether infants can
recognize a melody played by diﬀerent instruments and an instrument playing
diﬀerent melodies. The results indicated that 7- and 11-month olds could do the
latter but not the former. In other words, they could categorize timbre but not
melody after short-term habituation to the stimuli.

Development of Tonal Organization: A Case Study in Melodic
Improvisation
Pirkko A. Paananen; University of Jyväskylä, Finland
1PM2-R04-2
In a previous cross-sectional study of melodic keyboard improvisation of children
aged 6 to 11 years it was found that tonal stability developed as a function of
age, in accordance with the some earlier perceptual studies of tonal hierarchy. To
further investigate whether tonality develops from a more global representation of
pitch to hierarchical organization in melodic improvisation, a 2-year longitudinal
case study was conducted with one child, who in the beginning of the study was
6 years 4 months (January 2006) and in the end of the study 8 years 4 months of
age (January 2008). The child improvised on an accompaniment identical to the
previous cross-sectional study, using eight marked keys of the synthesizer (diatonic
scale, C major) for instrumental improvisation, and a headset and microphone for
vocal improvisation. The child used stable tones as reference in both keyboard and

                                        38
ICMPC 10                                               Monday 25 August 2007

vocal improvisations increasingly with age. The ambitus of the melody was larger
and motor exploring was typical in keyboard melodies. Vocal melodies were more
static and pitch accuracy changing. Instrumental and vocal improvisation revealed
diﬀerent aspects of the development of the tonal organization in childhood.

Origin of Singing; Infants’ Vocalization in Solitude
Yohko Shimada, Shoji Itakura; Kyoto University, Japan
1PM2-R04-3
The present study reported here investigated the psychological meaning of infants’
vocalization when they are alone without any responses. In the experiment, we
compared frequency and duration of the sound in three conditions; 1) an adult
(caregiver) respond naturally to infant’s vocalization, 2) infants are kept alone and
begin to vocalize spontaneously, 3) Infants are kept alone and begin to vocalize
spontaneously, with ampliﬁed sound feedback by stereo speakers. In all conditions,
infants did not express uncomfortable such as crying or fussing. As a result, the
duration of the behavior was signiﬁcantly higher in the ampliﬁed condition than the
respond condition. Ratio of sound during the recording was marginally higher in the
alone condition than the respond condition, and the alone with ampliﬁed feedback
condition than alone condition. The results suggested that infants vocalize not only
for response to others but also listening the sound of their own.

Music in Our Lives: An Investigation of Music Learning Between
9–19 Years of Age
Jane Davidson 1 , Paul Evans 1 , Robert Faulkner 1 , Gary E. McPherson 2 ;
1
  University of Western Australia, Australia; 2 University of Illinois at
Urbana-Champaign, USA
1PM2-R04-4
This paper to investigate the factors that inﬂuence the current role of music in the
lives of young adults (19 years old) who had formal music learning opportunities
in primary school and who either persisted or gave up performance in the interim.
It draws together data from a research project started in 1997 in which McPherson
and Davidson began tracing the learning experiences of beginning instrumentalists,
and continued for ﬁve years, by which time only 45 of the original 158 were still
playing their instruments. Results compare the early opportunities with current
achievements and factors crucial to continuing or quitting the musical learning are
shown to include incremental beliefs about abilities and learning, family support,
ﬂexible strategies to problem solving when practising. To date there have been few
longitudinal research opportunities to explore music learning in such a manner,
especially how this music learning experience aﬀects their adult lives, whether
they persist in their learning or not, thus this paper contributes signiﬁcantly to the
research in music learning.


1PM2-R05 : Neuroscience I
Room 5, 15:15 – 17:15 Monday 25 August 2007, Oral session

Time Courses of Cortical Beta and Gamma-Band Activity During
Listening to Metronome Sounds in Diﬀerent Tempi
Takako Fujioka 1 , Edward W. Large 2 , Laurel J. Trainor 3 , Bernhard
Ross 1 ; 1 Rotman Research Institute, Canada; 2 Florida Atlantic
University, USA; 3 McMaster University, Canada
1PM2-R05-1
Oscillatory cortical activities in beta-band (13–20 Hz) are related to somatomotor
system and gamma-band (> 20Hz) are involved with feature binding in perception.
Previously gamma-band activity in electroencephalography was found to modulate
with musical pulse with a two-beat metric accent. The present study examined beta-
and gamma-band activities in auditory cortices recorded via magnetoencephalog-
raphy (MEG) when subjects listened to musical pulse in various tempi. Tones were
presented with an interval of (1) 390, (2) 585 and (3) 780 ms, and (4) with irregular
intervals between 390 to 780 ms, and (5) with 390 ms-interval with a two-beat
accent while occasionally a tone was omitted at the accented one, and (6) at the
unaccented one. Results showed that beta-band activity decreased immediately
after the stimulus and returned to the previous level just before the next stimulus,


                                         39
Monday 25 August 2007                                                        ICMPC 10

regardless of the interval (tempo) while this pattern disappeared in the irregular
condition. The omission of a tone resulted in an extra beta rebound. In contrast,
gamma-band activity exhibited a peak right after the pulse, or the omission. We
propose that beta oscillations may support encoding of predictable regularity in
the stimulus sequence, whereas gamma oscillations likely reﬂect processing of the
current auditory events.

The Eﬀect of Musical Training on the Subcortical Processing of
Musical Intervals
Kyung Myun Lee, Erika Skoe, Nina Kraus, Richard D. Ashley;
Northwestern University, USA
1PM2-R05-2
How does the early (subcortical) stage of the auditory pathway encode musical
intervals? We sought to examine how the subcortical auditory system of the human
brain, speciﬁcally the brainstem, represents the acoustic properties of consonant and
dissonant intervals, and how musical training might alter this representation. The
auditory brainstem response (ABR) represents the spectral and temporal properties
of sound with considerable ﬁdelity. In the ABR, the fundamental frequency (pitch)
and its harmonics (up to about 1000 Hz) are represented by phaselocking. There is a
growing body of evidence to suggest the subcortical representation can be modiﬁed
by long-term auditory experience such as musical training. Using scalp-electrodes,
we measured ABRs (10 musicians and 10 non-musicians) to two chords: major 6th
(G2 and E3) and minor 7th (F#2 and E3). The ABR accurately reﬂects the frequency
and time-varying components of the two chords, however, musicians show more
robust representations of higher harmonics than non-musicians, especially for the
dissonant chord. Interestingly, the two groups did not diﬀer in their representation
of the fundamental frequency, nor in their representations of the individual notes in
each interval. These results suggest that musicians have specialized sensory systems
for processing musical intervals that occur as a consequence of their extensive
musical experience. [supported by NSF Grant 0544846]

Beat Initiation versus Continued Beat Perception: The Role of
Motor Areas in the Brain
Jessica A. Grahn, James B. Rowe; MRC Cognition and Brain Sciences
Unit, UK
1PM2-R05-3
Certain motor areas in the brain (e.g., basal ganglia) are involved during auditory
perception of sequences with a beat even when no movement is made. It is unclear,
however, if the activity is due to early processing, such as *searching* for the beat,
or later processing, such as internal beat generation. We investigated neural activity
for diﬀerent stages of beat perception using fMRI. Participants listened to a series of
beat and non-beat sequences. The beat sequences could be preceded by a non-beat
sequence, (the beat therefore must be perceived anew (‘beat-new’ condition)), or
could follow another beat sequence with the same beat rate (‘beat-continuation’, or a
diﬀerent rate (‘beat-interference’). If the basal ganglia are involved in ‘ﬁnding’ a beat,
activity should be greater for ‘beat-new’ and ‘beat-interference’ sequences, whereas
if the basal ganglia are involved in later beat processing, activity should be greater
for ‘beat-continuation’ trials. We found greatest activity for beat-continuation trials,
and less activity for beat-new and beat-interference trials. Other motor areas did
not show this pattern of activation. Thus, the basal ganglia are more responsive to
later stages of beat processing, suggesting a role in beat generation or prediction, as
opposed to searching for the beat.

fMRI Investigation of an Enculturation Eﬀect Among Western and
Turkish Listeners
Steven M. Demorest 1 , Steven J. Morrison 1 , Laura A. Stambaugh 1 ,
Munir N. Beken 2 , Todd Richards 1 , Clark Johnson 1 ; 1 University of
Washington, USA; 2 University of California at Los Angeles, USA
1PM2-R05-4
The purpose of this study was to test the hypothesis that listeners demonstrate
patterns of activation associated with music structural processing, when encoding
and retrieving both culturally familiar and unfamiliar stimuli. Participants (8 US,
8 Turkish) were right-handed adults and musical novices. Stimuli consisted of 9


                                           40
ICMPC 10                                                 Monday 25 August 2007

30-second instrumental music examples: 3 Western classical, 3 Turkish classical, 3
Chinese traditional followed by a 12-item memory test completed in the scanner.
Replicating the ﬁndings of previous research, all subjects were more successful
remembering novel music from their home culture. There was increased activation in
right frontal areas when subjects listened to culturally unfamiliar music suggesting
greater cognitive demands for music constructed according to unfamiliar structural
principles. We interpret the ﬁndings to mean that listeners interact with both
culturally familiar and unfamiliar music in similar ways. That is, listeners employ
common cognitive strategies to information identiﬁed as music, though easy access
to deeper structural information appears restricted to individuals encultured to
a given musical tradition. These results reﬂect the cultural speciﬁcity of music
schemata that allow listeners to eﬃciently and eﬀectively encode and recall musical
information.


1PM3-R01 : Social Interaction
Room 1, 17:30 – 19:00 Monday 25 August 2007, Oral session

Exploring Enforced Occupational Change in Opera Singers: A Case
Study Investigation into the Eﬀects of Musical Identity in the
Context of Career Change
Jane Oakland, Raymond MacDonald, Paul Flowers; Glasgow
Caledonian University, UK
1PM3-R01-1
This paper is concerned with issues of construction and maintenance of identity
within the context of occupational crisis. Interpretative Phenomenological Analysis
(IPA) is used to investigate one opera singer who has been forced to abandon an op-
eratic career as a result of physical disabilities, which despite extensive investigation
have no medical diagnosis. The study examines how an identity concept, which has
been constructed as a result of many years of professional opera work, has been
challenged and questioned as a result of this illness. Under discussion will be the
multifaceted, sometimes contradictory relationship the participant has experienced
between singing, performance and control of musical expression and in particular
the way his physical disabilities have impacted his sense of self as he re-negotiates
an identity perception outside the operatic community. The study also evaluates the
use of IPA as a methodology to investigate areas of musical identity in a case study
situation.

Non-Musician Adult Perspectives on the Role of Music in the
Formation and Maintenance of Musical Identities: An
Interpretative Phenomenological Analysis (IPA)
G.N. Caldwell, Raymond MacDonald, B. Duncan; Glasgow Caledonian
University, UK
1PM3-R01-2
Identity research is growing rapidly. Many current studies on musical identity
focus upon childhood- and/or adolescent-centred investigation through quantitative
measurement. Also, participants, often musicians, are usually deﬁned by genre
and/or duration of music lessons/levels of technical achievement attained. However,
there is a need for research that investigates the role of music as a constituent part
of identity from the perspective of adults who deﬁne themselves non-musicians.
The current paper seeks to highlight the utility of Interpretative Phenomenological
Analysis (IPA) for psychological research into musical identity. This paper, based
on 10 open-question semi-structured interviews, examines the relationship between
identity and music. Results suggest that musical identity is a subjective and socially
constructed phenomenon. Emergent, recurring and superordinate thematic obser-
vations are discussed and analysed from a ‘participant as expert’ standpoint. Data
from this study support the utility of IPA and provide avenues of further research
employing both quantitative and qualitative methods. Music is a key factor in the
formation and maintenance of personal and musical identities and plays a central
role in mediating underlying psychological constructs across the lifespan. This
research provides further evidence to support the assertion that all individuals,
regardless of musical experience, have musical identities.




                                          41
Monday 25 August 2007                                                     ICMPC 10


1PM3-R02 : Computational Models and Analyses I
Room 2, 17:30 – 19:00 Monday 25 August 2007, Oral session

Realizing Feature Exaggeration in Scale-Performance on the Piano
Shinya Morita 1 , Norio Emura 1 , Masanobu Miura 1 , Seiko Akinaga 2 ,
Masuzo Yanagida 3 ; 1 Ryukoku University, Japan; 2 Shukugawa Gakuin
College, Japan; 3 Doshisha University, Japan
1PM3-R02-1
This paper aims at realizing a feature exaggeration system for automatic scale
performance on the piano with performance parameters modiﬁed from the originals
extracted from the input human performance. The proposed system generates
simulated performances by manipulating the extracted performance parameters.
This paper proposes a set of parameters for describing performance features of
scale-playing on the piano. The set consists of 15 parameters, or three sets of ﬁve
parameters pi0 through pi4 where i ∈ {t, v, d} distinguishes three basic features;
onset time, velocity, and duration. Each of these basic features is modeled as the sum
of a global curve and deviations from it, where the spline interpolation is employed
using locally averaged points, or representative points, as the points to be passed.
The suﬃx j in pij distinguishes the standard deviations (j=0), the rms deviation
from the spline curve (j=1), the range of the spline curve (j=2), the rms diﬀerence
between successive notes (j=3), and the rms of the spline curve from the metronomic
line (j=4). All parameters are made controllable on the screen with slider bars from
0% up to 200% for synthesizing from “suppressed” to “exaggerated” performance,
where 100% represents the original performance.

Development of an Automatic Basso Continuo Playing System for
Baroque Music Performers
Masahiro Niizuma, Masaki Matsubara, Hiroaki Saito; Keio University,
Japan
1PM3-R02-2
The purpose of this study is to develop an automatic basso continuo playing system.
In order to ﬁnd musically appropriate sequence of chords, we deﬁne “harmony
cost” as the sum of two diﬀerent costs; One is the “local cost” which indicates the
likelihood of a certain chord allocation and the other is the “transition cost”, which
indicates the likelihood of a certain connection between two chords. Automatic
basso continuo playing is realized by searching the most optimal chord sequence,
which minimize the accumulated harmony cost. Our system is evaluated by three
experiments. Each result showed the eﬀectiveness of our approach.

Gaussian Process Regression for Rendering Music Performance
Keiko Teramura 1 , Hideharu Okuma 1 , Yuusaku Taniguchi 1 , Shimpei
Makimoto 1 , Shin-ichi Maeda 2 ; 1 NAIST, Japan; 2 Kyoto University,
Japan
1PM3-R02-3
So far, many of the computational models for rendering music performance have
been proposed, but they often consist of many heuristic rules and tend to be
complex. It makes diﬃcult to generate and select the useful rules, or perform the
optimization of parameters in the rules. In this study, we present a new approach
that automatically learns a computational model for rendering music performance
with score information as an input and the corresponding real performance data as
an output. We use a Gaussian Process (GP) incorporated with a Bayesian Committee
Machine to reduce naive GP’s heavy computation cost, to learn those input-output
relationships. We compared three normalized errors: dynamics, attack time and
release time between the real and predicted performance by the trained GP to
evaluate our proposed scheme. We evaluated the learning ability and the gener-
alization ability. The results show that the trained GP has an acceptable learning
ability for ‘known’ pieces, but show insuﬃcient generalization ability for ‘unknown’
pieces, suggesting that the GP can learn the expressive music performance without
setting many parameters manually, but the size of the current training dataset is not
suﬃciently large so as to generalize the training pieces to ‘unknown’ test pieces.




                                         42
ICMPC 10                                               Monday 25 August 2007


1PM3-R03 : Psychoacoustics I
Room 3, 17:30 – 19:00 Monday 25 August 2007, Oral session

Sensitivity to Temporal Deviations on the Starting Point or the
Ending Point of “Frequency” Glides
Satomi Tanaka 1 , Minoru Tsuzaki 1 , Eriko Aiba 1 , Hiroaki Kato 2 ;
1
  Kyoto City University of Arts, Japan; 2 ATR-CIS, Japan
1PM3-R03-1
The occurrences of sound events in a sound stream can be perceived without critical
diﬃculty even when it is diﬃcult to deﬁne the corresponding acoustic boundary. This
study is designed to investigate which acoustical feature functions as an eﬀective cue
to “mark” the occurrence of a new event. In the case where a sound glides quickly
in frequency, at which point do we perceive the occurrence of the second sound, the
starting point of the frequency glide or the ending point? To answer this question,
experimental stimuli are designed so that an isochronous structure is conveyed
exclusively by the starting or by the ending points of the glides. The frequency glide
modulates the carrier frequency of a sinusoid in the ﬁrst experiment so that the
stimulus has the change in pitch, and the resonant frequency of a complex tone
in the second experiment so that the stimulus has the change in timbre with no
pitch change. In both experiments, the deviation from the isochronous structure is
detected more easily under the starting-point condition than under the ending-point
condition. The results suggest that the starting point function as a more eﬀective cue
to mark a new event than does the ending point [Work supported by the Grant-in-Aid
for Scientiﬁc Research (B) No. 20300069, JSPS].

Factors Inﬂuencing Spatial Pattern Recognition in a Musical
Context
Blas Payri, José-Luis Miralles-Bono; Universidad Politécnica de
Valencia, Spain
1PM3-R03-2
Our experiment studies the recognition of spatial patterns and their usability as
a musical language element. We used 4 rhythm patterns played at 2 diﬀerent
tempi. The sounds moved between left and right, either continuously or with abrupt
changes. 7 timbres were used: 4 synthetic (sine and square wave, white noise) akin
to psychoacoustics typical material, and 3 that represented real musical material
(voice, orchestra). The stimuli were played in a silent room over two loudspeakers
to 20 musicians that were asked to recognize the rhythm pattern out of a set of 4
patterns. A simpliﬁed version was played to 30 non-musicians that had to guess
whether the movement accelerated.
The statistical analysis shows that mean diﬀerences in right/wrong pattern recogni-
tion were signiﬁcant when at the .01 level depending on timbre harmonicity, listener
position, jitter, movement shape, and rhythm pattern. Thus spatial patterns can
be perceived better for harmonic sounds with neither temporal variations (jitter or
tremolo) nor noise components. The literature is rich in studies of sound localization
with simple sounds, but few studies deal with material actually used by electroa-
coustic composers (spatial trajectories, complex timbres) and we show that spatial
trajectories recognition in a concert hall is fragile.

On the Factors of the Spatial Impressions of Reproduced Music in
Surround Sound Comparing Recording Techniques
Toru Kamekawa, Atsushi Marui; Tokyo University of the Arts, Japan
1PM3-R03-3
Microphone technique for surround sound recording of an orchestra are discussed.
Seven types of surround microphone sets recorded in a concert hall were compared
in subjective listening test on the attributes such as powerfulness and spaciousness
using a method inspired by MUSHRA (MUltiple Stimuli with Hidden Reference and
Anchor). To minimize temporal change in music, Phase Randomized Signal (PRS) was
proposed. From the average score of the listening test, the impression diﬀerence
between original source and PRS was found in some microphone arrays at some
pieces. It means that the impression of these arrays depend on temporal changes
in music. The data from the listening test between original source and PRS showed
that impression of powerfulness had slightly higher correlation. The relations of the


                                         43
Monday 25 August 2007                                                    ICMPC 10

physical factors of each array were also compared, such as SC (Spectral Centroid)
and LFC (Lateral Fraction Coeﬃcient) of each array. The correlation of these physical
factors and the attribute scores show that the contribution of these physical factors
depends on music and its temporal change. ‘Powerfulness’ is related to timbral
character and ‘spaciousness’ is related to temporal change. Furthermore signals
synthesized from IR (impulse response) of each array were also compared. It was
reaﬃrmed that sound of orchestra is consisted from more complicate signals.


1PM3-R04 : Perception of Musical Structures
Room 4, 17:30 – 19:00 Monday 25 August 2007, Oral session

Beginnings, Middles, and Endings: The Eﬀect of Musical
Parameters on the Perception of Intrinsic Formal Functionality
Michel Vallières 1 , Daphne Tan 2 , William E. Caplin 1 , Joseph Shenker 1 ,
Stephen McAdams 1 ; 1 McGill University, Canada; 2 University of
Rochester, USA
1PM3-R04-1
This study investigates the perception of intra-thematic formal functions — the
temporal role of beginning, middle, or ending played by a musical passage within the
boundaries of a theme — as expressed through musical materials. We examined the
role of various musical parameters in the perception formal functionality in short,
out-of-context musical excerpts. Participants (separate groups of 20 musicians and
20 non-musicians) heard two types of stimuli: original excerpts of Mozart’s piano
sonatas and modiﬁcations thereof. Their primary task was to identify the excerpts
as a beginning, middle, or end of a theme. The original excerpts were modiﬁed by
altering individual musical parameters. All modiﬁcations had to remain stylistically
coherent. The results obtained in the ﬁrst two experiments of this project showed,
notably, an eﬀect of (1) the excerpt’s opening tonic and dominant harmonies on
musicians’ perception of beginnings and middles; (2) the unaccompanied anacrusis
on musicians’ and non-musicians’ perception of beginnings and middles. The
current study presents the results from the third experiment. They provide, notably,
greater insight into the form-functional impact of (1) non-tonic harmonies other
than the dominant on musician’s perception of beginnings and middles; and (2)
several aspects of the unaccompanied anacrusis — such as the opening scale degrees,
texture contrasts, and duration — on musicians’ and non-musicians’ perception of
beginnings and middles.

Eﬀects of Leading and Following Contexts on the Music Sound
Restoration
Takayuki Sasaki; Miyagi Gakuin Women’s University, Japan
1PM3-R04-2
When a sound in a melody is deleted and replaced by a noise, the absent sound can
be restored perceptually. This phenomenon, Music Sound Restoration, has been
studied by indirect methods, that is, noise localization method (Sasaki, 1980) and
discrimination method (DeWitt & Samuel, 1990). In the present study, Music Sound
Restoration was studied by means of a report method. It was examined whether a
leading or a following context has an inﬂuence on the perceptually restored sound
(pitch). Ten musically trained listeners, without any knowledge about Music Sound
Restoration, were asked to write down what they heard in a music notation. Two
sounds in each of simple short tunes were removed and replaced by pink noises. As
context information, a major or a minor chord was added previous to or following
after the tunes. When the leading chord was major, the listeners reported the
melodies in a major key. And when the leading chord was minor, they reported
minor melodies. This context eﬀect was observed also when the context chord was
added after the tunes. It was revealed that the restored pitch is strongly dependent
upon the context cue.

Children’s Impression and Expression of Major, Minor,
Whole-Tone, and Korean Traditional Scales
Eunsil Park; Seoul National University, Korea
1PM3-R04-3
The present study examined perception of major, minor, whole-tone, Korean tradi-
tional scales on condition of diﬀerent tempo, and expression of those scales through

                                        44
ICMPC 10                                                Monday 25 August 2007

their own compositions. To examine impression of musical scales, I had the 5th
grade students listen to 9 short musical phrases. While students listened to music,
they checked on a meaning discrimination test. To examine children’s expression
of musical scales, 32 children of the 5th grade divided into 6 groups composed
music. Their composing procedure involved listening to music related with a scale,
searching common features, deciding title suitable for the scale, making music and
performing, revising and recording.
The results of discrimination test displayed music in speciﬁc scale evoked speciﬁc
mood, major showed positive but minor, Korean, and whole-tone negative. And
children’s composition showed diﬀerent scale produced diﬀerent composing charac-
teristics, that is, title, measure, rhythm, sequence, closure, central tone were changed
according to scales.
Children tend to be aﬀected more by tempo than scale, when they listen to music.
Generally diﬀerent scales evoke diﬀerent moods. Fast music makes more positive
emotional responses than slow music. The titles of compositions are similar to the
emotional responses of each scale. And children tend to concentrate more on the
melody progress than rhythm and any other components. The products of composi-
tion reﬂect well many characteristics of each scale.


1PM3-R05 : Music Listening I
Room 5, 17:30 – 19:00 Monday 25 August 2007, Oral session

Metaphors of Motion in Listeners’ Verbal Reports
Riitta Rautio; University of Jyväskylä, Finland
1PM3-R05-1
It is generally acknowledged that music elicits a sense of motion in listeners. There
are, however, diﬀering views (physiological, ecological, psychological, metaphorical)
concerning the origins of the experience of motion. This study explored how lis-
teners express their experience of musical motion in free verbal reports. Based on
the assumptions of cognitive semantics and cognitive linguistics it is held that by
exploring linguistic expressions one can conclude how the speaker understands the
phenomenon under discussion. The data consisted of verbal reports (in Finnish),
submitted to content analysis. Five musical excerpts were chosen from the reper-
toire of 20th century orchestral music, each excerpt possessing a clear pattern of
ascent and/or descent. Preliminary results showed that the respondents tended to
use motion verbs quite frequently when conceptualizing the semantic content of the
excerpts. Descending pitch contours were connected either to subsiding dynamic
processes or descending physical motion. In the latter case, descending motion was
systematically conceptualized with verbs, which refer to locomotion caused by an
external force, more speciﬁcally by gravity. The ascending pitch contour was concep-
tualized with verbs referring to motion caused by the mover him/herself. It seems
that listeners use their bodily experience of gravity to conceptualize their experience
of music.

Factors Inﬂuencing Music Preference Among Japanese Listeners
Over 50 Years Old: Why do They Like Certain Songs?
Eri Hirokawa; Nagoya College of Music, Japan
1PM3-R05-2
The purpose of this study was to investigate why Japanese people miss certain songs
when they grow older. The 233 Japanese people who participated in this study were
50 years old or older, and were either interview or they completed a survey. When
answering the survey, they indicated which songs they miss and want to sing or listen
to and why. Results showed that a third of the comments indicated that they missed
particular musical selections because those songs are associated with speciﬁc memo-
ries, personal experiences or life events. Approximately 100 comments indicated that
they miss certain music because those songs made them feel nostalgic. Results also
showed that people miss songs that they used to sing or listened to. Many comments
also indicated that people miss particular songs because they like musical elements
of those songs, the mood or styles of those songs, or the composer or singers of those
songs. People also enjoy listening or singing songs that provide positive eﬀects on
their mood or health. Such information is important for music therapists to enhance
the quality of music applications and the quality of the lives of older adults. When
music therapists pay attention to the music preferences of clients, the quality of mu-
sic applications will become more therapeutic.


                                          45
Monday 25 August 2007                                                     ICMPC 10

The Enjoyment in Opera — An Empirical Study of Visitors’
Experience in Music Theatre
Johanna Jobst, Sabine Boerner; University of Konstanz, Germany
1PM3-R05-3
Our paper investigates the perceived artistic quality of the performance, visitors’ af-
fective responses, and context factors as predictors for the statement on their overall
enjoyment in opera. After attending a performance of “The Magic Flute” by Mozart
in the opera house of Dessau, members of the audience (n = 114) were asked to an-
swer a questionnaire. Regression analysis yields visitors’ aﬀective responses to the
performance as a better predictor of their overall enjoyment than the perceived artis-
tic quality of the performance. However, satisfaction with the auditorium and service
quality is shown to make virtually no contribution to the audience’s overall enjoyment.
Investigating intersubjective agreement on the overall enjoyment in opera, only minor
individual diﬀerences (e.g., experts versus non-experts) are found.




                                         46
ICMPC 10                                              Tuesday 26 August 2008


2AM1-R02 : Melody
Room 2, 8:30 – 10:30 Tuesday 26 August 2008, Oral session

Polynomial Contour as a Core Feature for Representing Melodies
Daniel Müllensiefen, Geraint Wiggins; Goldsmiths University of
London, UK
2AM1-R02-1
Melodic contour is often regarded as one of the most important features in the con-
text of modelling music perception and melodic memory. Numerous psychological
studies in the past have found contour to be an important mental representation of
melodies or short melodic phrases. From an analytical point of view there have been
several attempts to abstract a melodic contour representation from any given melody.
These approaches diﬀer in the degree of abstraction and the type of information
used. In this paper we propose a novel method for representing melodic contour.
This method is based on ﬁtting a polynomial curve to the onset and pitch data of a
melody using multiple regression. The melodic contour is then represented by the
resulting set of polynomial coeﬃcients. This representation of melodies allows for
several interesting and useful applications. 1. To determine the commonness of any
given melody with respect to a corpus of melodies by computing a probability space
over the multi-dimensional distribution of the polynomial coeﬃcients of melodic
phrases. 2. This probability space also allows for the clustering of melodies into
groups of similar contour. 3. The polynomial representation of melodies makes
it also possible to construct several elegant and intuitively convincing similarity
measures.

Perceptual Segmentation of Melodies: Ambiguity, Rules and
Statistical Learning
Marcus T. Pearce 1 , Daniel Müllensiefen 1 , Geraint Wiggins 1 , Klaus
Frieler 2 ; 1 Goldsmiths University of London, UK; 2 University of
Hamburg, Germany
2AM1-R02-2
Models of perceptual segmentation in melody tend to rely either on local Gestalt-like
rules that indicate boundary strength or statistical learning and the information
dynamics of melody perception. In an experimental study, 25 musically trained
participants were asked to indicate phrase boundaries during four repeated lis-
tenings of 15 unfamiliar, popular melodies. Several computational models of
melodic segmentation are compared in predicting the participants’ majority-voted
boundaries. These include rule-based models (Cambouropoulos, 2001; Frankland &
Cohen, 2004; Temperley, 2001), as well as simple (Saﬀran et al., 1999) and complex
(Pearce & Wiggins, 2006) models based on statistical learning. The results indicate
that the complex statistical model achieves comparable performance to the best
of the rule-based models. Combining models selected on the basis of prediction
accuracy and simplicity yields a hybrid model (containing both rule-based and
statistical components) that outperforms its component models. To address low
inter-rater agreement for many melodies, a further analysis is conducted in which
participants sharing a similar segmentation strategy are clustered together for each
melody separately. Nearly all melodies yield several perceptually valid and stable
segmentations. This allows a more detailed comparative analysis of the various
models’ performance.

Pitch Space Processing and Melodic Expectancies in Tonal and
Atonal Contexts
Juan Fernando Anta; National University of La Plata, Argentina
2AM1-R02-3
The extent to which tonal (TPS) or atonal (APS) pitch space, proximity and range,
aﬀect each other on melodic expectation was assessed. In Experiment 1, fourteen
non-musicians rated how well test tones continued tonal melodies; in Experiment 2
fourteen non-musicians performed the same task with atonal versions of previously
used fragments. Experiment 1 suggested that TPS did not aﬀect expectation beyond
a TPS-activated (TPS-a) area hypothesized for each fragment, and that events towards
the TPS-a bulk were preferred. Experiment 2 showed that events placed towards
the range bulk were preferred. Comparisons between data from both experiments



                                        47
Tuesday 26 August 2008                                                     ICMPC 10

indicated that expectancies for events in the range bulk were greater in tonal than
in atonal condition, suggesting that TPS-a should be considered a separate factor
reinforcing expectations.

Recognition of Microtonal Musical Intervals by Performers and
Composers
Terumi Narushima 1 , Greg Schiemer 1 , Emery Schubert 2 , Richard
Parncutt 3 ; 1 University of Wollongong, Australia; 2 University of New
South Wales, Australia; 3 University of Graz, Austria
2AM1-R02-4
Little is known about the ability of performers to perceive and reproduce microtonal
intervals (intervals smaller than a semitone) relying purely on intervallic perception.
The aim of this study is to explore the ability of trained musicians and composers to
recognize microtonal intervals. Our experiments have demonstrated that within two
weeks of learning, some participants are able to recognise with reasonable accuracy
two tone sequential intervals that are varied by amounts smaller than 15 cents (15
hundredths of an equal tempered semitone). The results shed light on the ability to
recognize microtonal intervals and the process by which such knowledge is acquired.
Our data suggest that more complex forms of microtonality, such as 19-tone equal
temperament or just tuning based on unfamiliar frequency ratios, can be performed
successfully given suﬃcient practice over an extended period.


2AM1-R03 : Performance II
Room 3, 8:30 – 10:30 Tuesday 26 August 2008, Oral session

Experimentally Investigating the Use of Score Features for
Computational Models of Expressive Timing
Sebastian Flossmann, Maarten Grachten, Gerhard Widmer; JKU Linz,
Austria
2AM1-R03-1
In the expressive performance of music variation of tempo plays a major role in
shaping and structuring the piece. We distinguish two aspects of tempo, the current
tempo and the timing of individual notes with respect to the current tempo. Those
two notions are inﬂuenced diﬀerently by the characteristics of the performed score.
The relation between score and timing/tempo has many facets, one of which we
examine more closely in the following. More precisely we provide experimental evi-
dence for the hypothesis that timing is more aptly modeled with score characteristics
from a small temporal score context, while tempo modeling proﬁts from a bigger
temporal score context.

Chopin’s Rubato: A Solution
Manfred Clynes; Georgetown University, USA
2AM1-R03-2
We report on the Chopin rubato, not encountered with other composers, that lend to
his music a unique power of communication, intimacy and often a touching charm
not otherwise evident. No-one in is lifetime could equal Chopin’s seemingly magical,
and personal use of this.
Here we report on the discovery, using Superconductor II of a surprising but simple
feature that appears to accomplish aspects of this elusive magic. The ritard curve is
simply inverted. It is a faithful temporal inversion of a ritard function , with similar
exponents: starting as a suddenly slowed tempo gradually returning to the normal
tempo, with a dynamic curve similar to the ritard, but inverted in time. The gradual
acceleration is carried beyond the normal tempo towards the end of the rubato
phase, so that the total time taken may be only slightly diﬀerent as if no rubato
had taken place. Rubato often occurs initiated through a harmonic suspension with
the resolution appropriately postponed. Several Chopin music examples of rubato
realization are included in mp3 format. The complete ﬁrst movement of the E minor
piano concerto is being realized, including orchestra (well ahead of the 2050 Rencon
deadline).




                                          48
ICMPC 10                                                Tuesday 26 August 2008

Intuitive Visualization of Gestures in Expressive Timing: A Case
Study on the Final Ritard
Maarten Grachten 1 , Werner Goebl 2 , Sebastian Flossmann 1 , Gerhard
Widmer 1 ; 1 JKU Linz, Austria; 2 McGill University, Canada
2AM1-R03-3
Expressive timing is vital for the aesthetic quality that makes us appreciate performed
music. It is a largely tacit skill that musicians acquire by practice. A long-standing
intuition is that expressive timing is closely related to the concept of motion. This
view leads naturally to the adoption of a dynamical systems approach to the study
of expressive timing. A well-known visualization technique from dynamical systems
theory is the phase-plane representation. The application of this technique, that
highlights the dynamic aspects of the data, is demonstrated in a case study on the
ﬁnal ritard in performances of Schumann’s Träumerei. We argue that expressive
gestures are visible in a clear and intuitive manner in the phase-plane representations.
Another striking aspect of the phase-plane trajectories is their suggestion of human
gestural motion.

Timing in Piano Music — Testing a Model of Melody Lead
Johan Bjurling, Roberto Bresin; KTH, Sweden
2AM1-R03-4
The main aim of this study was to investigate if a model of note synchronization,
based on measurements of the mechanics of a real piano, would provide as side
eﬀects the so-called “melody lead eﬀect” and more naturalness in automatic mu-
sic performance of piano music. A real-time model was validated with a listening test.
Seven experienced pianists participated as subjects. For each of ﬁve pieces of music,
they were instructed to adjust two sliders corresponding to parameters tempo and
synchronization for making the piece sounding “as realistic as possible”. Subjects
could exaggerate the eﬀect of both melody lead and inverted melody lead by up to
200 percent. In the second part of the test subjects were asked to adjust tempo to a
preferred value with a slider, and to choose the most realistic synchronization with
the mouse from three anonymous alternatives, in a forced choice fashion. The three
alternatives corresponded to (1) perfect synchronization, (2) 100 percent melody
lead, and (3) 100 percent inverted melody lead.
Main results show that on average subjects preferred positive synchronization values,
corresponding to normal melody lead. Subjects preferred positive synchronization
(normal melody lead) correlated with tempo, and this result assimilates reality. For
negative synchronization values (inverted melody lead) the relationship to tempo
seems more arbitrary.


2AM1-R04 : Emotion in Music I
Room 4, 8:30 – 10:30 Tuesday 26 August 2008, Oral session

Why do Listeners Enjoy Music That Makes Them Weep?
David Huron; Ohio State University, USA
2AM1-R04-1
Tearing of the eyes, nasal congestion, a constriction in your throat, and erratic
breathing: your doctor would conclude that you are suﬀering from a severe allergic
reaction. But in special circumstances, music can evoke precisely such symptoms.
How does music evoke feelings akin to sadness or grief? And why do people willingly
listen to music that may make them cry? In this presentation, some physiological,
evolutionary, and behavioral aspects of adult crying are brieﬂy surveyed. It is sug-
gested that the pleasure of musically-induced weeping arises from cortical inhibition
of the amygdala, and is linked to the release of the hormone prolactin.

Construction of a Quantitative Scale for Cheerfulness of Short
Melodies
Kenta Shoji, Masashi Yamada; Kanazawa Institute of Technology,
Japan
2AM1-R04-2
It has been shown that the cheerfulness of music depends on many factors such
as tonality, tempo, performing register, and so on. However, it has not been



                                          49
Tuesday 26 August 2008                                                    ICMPC 10

quantitatively deﬁned how these factors determine cheerfulness. In the present
study, three listening experiments were conducted to determine how the tonality,
speed, and performed register vary the cheerfulness of music. In each experiment,
listeners listened to each pair of stimuli and compared the cheerfulness of them
with seven-step categories. Using Scheﬀé’s method, each stimulus is plotted on
cheerfulness scales. The performing register of a stimulus is evaluated as the
centroid for the overall-term spectrum, and is shown in ERB (Equivalent Rectangular
Bandwidth)-rate scale, in the present study. The results show that the degree of
cheerfulness increases proportionally as the register shown in ERB-rate increases.
Using this relationship, a quantitative scale of the cheerfulness can be constructed.
On this scale, it is estimated that the eﬀect of tonality (major or minor) corresponds
to the centroid shift of 4 ERB-rate, and doubling the speed corresponds to an increase
of the controid in 3.5 ERB-rate.

The Inﬂuence of Social Feedback on the Emotional Eﬀects of Music
Hauke Egermann, Oliver Grewe, Reinhard Kopiez, Eckart Altenmüller;
Hannover University of Music and Drama, Germany
2AM1-R04-3
Numerous studies have shown that music aﬀects the so-called “subjective feeling”
component of emotion. Our study investigates whether the emotional eﬀects of
music can be manipulated by social feedback. We used a web-based experiment
because web experiments oﬀer many advantages over lab experiments (like bigger
external validity). The study was implemented into an online music-personality-test,
in order to motivate subjects to take part. In about 10 minutes, participants could
join data-collection and get personalised test results at the end, which described
their music preferences and personality. The 2576 participants (mean age: 32,5 years
(SD=12,6); 1443 male and 1133 female) were randomly assigned to 2 groups. All
participants listened to 5 from 23 music excerpts (30 sec each) in random order. After
each excerpt, participants rated the induced emotions using arousal and valence
dimensions. Additionally, group 2 received faked feedback based on the emotional
ratings of preceding participants. This was presented while the participants listened
to the stimuli. Results show, that participants of group 2 (with social feedback) rate
their own emotions signiﬁcantly diﬀerent compared to group 1 (without feedback)
for 21 of the 23 pieces on at least one dimension. Looking at those group diﬀerences
the feedback is proven to inﬂuence the participants’ ratings. Thus, a social factor
inﬂuencing emotional eﬀects of music could be conﬁrmed.

Construction and Validation of a Music-Directed Attention Scale
(MDAS): A Preliminary Study
Joanna Kantor-Martynuska; Warsaw School of Social Psychology,
Poland
2AM1-R04-4
Music-directed attention (MDA) is a tendency to automatically shift attention towards
extant musical stimuli. MDA is deﬁned with reference to theories of attentional
resources and attentional ﬁltering. A questionnaire measure of MDA, Music-Directed
Attention Scale (MDAS), was constructed and validated. This paper presents the
structure and psychometric characteristics of the MDAS. 422 musicians and non-
musicians ﬁlled out a pilot version of MDAS which consisted of 44 items. Principal
components factor analysis was performed on the results and suggested two inde-
pendent factors named Distractability and Involvement. Distractability is a tendency
to have music divert one’s attention from tasks of primary focus, with a resulting
decline in performance. Involvement consists in experiencing frequent and strong
emotional responses evoked by music, which disturbs performance of the tasks
of immediate importance. Three expert judges later freely categorized the items
considering their content validity and consistency. Items with a strong response bias
and those of insuﬃcient validity were excluded from the present version of MDAS
that consists of 13 items. Musical expertise was found to signiﬁcantly aﬀect the total
MDAS scores and Involvement but not Distractability. MDAS is supposed to measure
individual diﬀerences in a tendency to have one’s attention involuntarily driven by
music in a range of every-day contexts. The analysis of MDA’s relationship with
general attentional control and auditory style is in progress.




                                         50
ICMPC 10                                              Tuesday 26 August 2008


2AM1-R05 : Therapy
Room 5, 8:30 – 10:30 Tuesday 26 August 2008, Oral session

The Eﬀects of Music Therapy on Declarative Memory Processes in
Moderately Impaired Dementia Populations
M. Lynch 1 , S.R. Toukhsati 1 , D. O’Connor 1 , M. Thaut 2 , P.C. Bennett 1 ,
B. Barber 3 ; 1 Monash University, Australia; 2 Colorado State University,
USA; 3 National Aging Research Institute, Australia
 2AM1-R05-2
The aim of this study was to examine the eﬀects of music therapy (MT) on declarative
memory processes in persons diagnosed with probable Alzheimer’s dementia.
Participants were 57 individuals (MMSE = 15.57 ± 6.00; mean age = 83.09 years, SD
= 7.46) from ten Residential Aged Care Facilities (RACFs). The eﬀects of MT were
compared with those following animal assisted therapy (AAT), diversional therapy
(DT) and a no treatment control. Treatments were randomly assigned to RACFs and
conducted in small groups, twice weekly for one hour over four weeks. A nested,
mixed-model design compared pre- and post-intervention general semantic memory,
personal semantic and episodic memory between groups. The ﬁndings revealed
a signiﬁcant improvement in autobiographical recall of events during childhood
following AAT in comparison to MT (p < .05). In contrast, MT yielded an improvement
in autobiographical recall of events during early-adulthood in comparison to DT (p <
.01), but not in comparison to the no treatment control (p > .05) or the AAT group
(p > .05). There were no signiﬁcant changes evident with regard to general semantic
memory or personal semantic memory. Taken together, these ﬁndings suggest that
declarative memory processes are not generally responsive to MT. Methodological
limitations and implications for future research will be discussed.

The Eﬀects of Music Therapy, Animal-Assisted Therapy and
Diversional Therapy on Attention and Memory Processes of
Individuals with Probable Alzheimer’s Dementia
R. Kharsas 1 , S.R. Toukhsati 1 , D. O’Connor 1 , M. Thaut 2 , P.C.
Bennett 1 , B. Barber 3 ; 1 Monash University, Australia; 2 Colorado State
University, USA; 3 National Aging Research Institute, Australia
2AM1-R05-3
The aim of this study was to evaluate the eﬀects of exposure to music therapy
(MT), animal assisted therapy (AAT), diversional therapy (DT) and a no treatment
control (NT), on the attention and memory processes of cognitively impaired, elderly
participants. The sample comprised 60 participants (mean age = 83.96 years, SD
= 7.39) with mild to severe dementia (MMSE = 14.48 ± 6.89). A nested, mixed-
model design was employed whereby each condition (MT, AAT, DT and NT) was
randomly assigned to at least two Residential Aged Care facilities. Hour long group
therapeutic interventions were scheduled twice weekly over four weeks. Pre- and
post-intervention data on measures of cognition (MMSE), recognition memory, visual
selective attention and auditory selective attention was collected. Findings revealed
a signiﬁcant interaction for the MMSE Language subscale (p < .05), with performance
improvements observed following MT and DT in contrast to decrements following
AAT and NT. An improvement on the MMSE Registration subscale was also observed
(p < .05); however, improvements following MT were not diﬀerentiated from other
conditions. There were no signiﬁcant changes observed on the memory and attention
measures. The application of a randomised controlled methodology has yielded data
that do not support the utility of alternative treatments to ameliorate attention and
memory deﬁcits in dementia populations.

An Empirical Study of Proactive Multimedia Therapy Contents for
Public: Production Design and Cognitive Response Measurements
Irene Eunyoung Lee, Charles-François Latchoumane, Jaeseung Jeong;
KAIST, Korea
2AM1-R05-4
Whether it is under conscious or unconscious circumstances, people in today’s
society experience a range of multi-sensory stimulations through diverse confronted
media that bring changes in feelings or moods. Multimedia contents including


                                        51
Tuesday 26 August 2008                                                     ICMPC 10

arts and therapies are being developed increasingly nowadays, but their cognitive
mechanisms and eﬀects are still ambiguously ascertained. Can we really induce
target emotions from audiences with short multimedia contents (60 sec. long) that
employed abstractive visuals and non-lyrical musical expressions? If yes, would there
be any common thread in audience responses to these purpose-driven new creations?
In this study, under a hypothesis that it is possible to create certain emotion/mood
inducing multi-modal contents, we ﬁrst researched various psychology (and/or
therapy) ﬁelds (e.g., music, colors, images, and motiongraphics) for guidelines to
design three speciﬁc types of positive emotion elicitations (i.e., Relaxation, Happy,
and Vigorous), and produced audio-visual contents based on the learned expressive
attributes. Then we investigated the response of 12 subjects (6 males and 6 females,
mean age 22 year old ) on their EEG power diﬀerences between rest and watching
movie sections, alpha asymmetry, cognitive performances during visual congruent
continuous performance tasks (cCPT, attentional task), and self-evaluation question-
naires. We concluded that emotional/mood induction using multi-modal contents
could bring out changes in attention, visible from a behavioral study, however milder
in the electrophysiological response.


2AM2-R07 : Musical Scales and Melody / Harmony and
Tonality / Timbre / Psychophysics and Music Acoustics
Rooms 7, 10:40 – 12:30 Tuesday 26 August 2008, Poster session

The Relative Prevalence of Medieval Modes and the Origin of the
Leading Tone
Richard Parncutt, Daniela Prem; University of Graz, Austria
2AM2-R07-01
The rank order of ﬁnals in the Index of Gregorian Chant by Bryden and Hughes
(1969) is G (Myxolydian), D (Dorian), F, C, A, E, B. Why? Melodically, major-second
intervals are universally preferred (cf. Vos & Troost, 1989), presumably for reasons
of stream fusion and critical bandwidth. Adding M2s above/below a ﬁnal creates a
three-tone core, relative to which the tritone is avoided — presumably due to its low
pitch commonality (Stoll & Parncutt, 1987). Harmonically, medieval musicians may
have preferred the perceptual coherence created by optimizing pitch commonality
between the spectral pitches of the ﬁnalis and the virtual pitches of the other tones.
Both melodic and harmonic models are consistent with statistical prevalence data.
In 13th -century polyphony, the leading tone lies a semitone below a following tone in
a (consonant) harmonic dyad (Barsky, 1996). Why? Since B (and E) seldom function
as ﬁnals, they are unstable, so the progression from B to C (or E to F) is more
prevalent than the reverse. The tonality of major-minor music involves chroma
stability proﬁles, which depend on chroma prevalence (Krumhansl, 1990). That in
turn depends on two eﬀects with distinct historical origins: the leading tone (c. 13th
century) and chroma salience in the tonic triad (c. 16th ). The success of key-tracking
models may depend on how they separate these.

The Impact of Uniqueness in Musical Scales on Mistuning
Detection
Marco Lehmann, Reinhard Kopiez; Hannover University of Music and
Drama, Germany
2AM2-R07-03
In musical scales, the structural feature “uniqueness” refers to the idea that each
scale tone can be unequivocally identiﬁed by its set of distances to the other scale
tones (Balzano, 1982). The concept has been confused with the concept of scales with
unequal step sizes (e.g., Trehub, Schellenberg, & Kamenetsky, 1999), and it is neces-
sary to assert which one of the two is perceptually more prominent in musical scales.
This study empirically compared two unique scale structures with one non-unique
scale structure of unequal steps. We predicted that the two unique scales would allow
for a better detection of mistuned single tones in repeated presentations. In contrast,
on average, N=17 participants were more sensitive to mistunings in the non-unique
scale condition. Moreover, exploratory analysis of the sensitivity for mistunings at
diﬀerent scale positions revealed that the vicinity of the target tones is possibly more
important for the detection of mistuning than is the global scale structure. In fact,
for each of the three scale structures, sensitivity diminished for targets following a
comparably small scale step. In this study, uniqueness could not be promoted as


                                          52
ICMPC 10                                                Tuesday 26 August 2008

perceptually important for the detection of mistuning in musical scales. Conversely,
the local structure of the scales appeared to be crucial in completing the task.

Assessing the Role of Sensory Consonance in Trained Musicians’
Tuning Preferences
Johanna Devaney, Ichiro Fujinaga; McGill University, Canada
2AM2-R07-05
This study tests whether trained musicians’ tuning preferences align with the interval
sizes that can be calculated from the overtone series and whether there are any
diﬀerences in the subjects’ preferences when the intervals are produced with sine
waves versus complex tones. This comparison is useful in assessing the impact
of the partials in the tuning preferences. In the experiment, subjects were asked
to tune one or two tones in relation to a sounding bass note; these tones were ei-
ther a perfect octave, a perfect ﬁfth, a major third, or a minor seventh above the bass.
This work builds on Terhardt’s theory sensory consonance, which is rooted in
Helmholtz’s theory of consonance and dissonance. Helmholtz postulated that
the coincidence of a signiﬁcant number of partials between two pitches produces
a consonance whereas the absence of such coincidence produces a dissonance.
Terhardt found an inverse relationship between the degree of consonance and the
amount of beating present in the sound. His theory suggests that the intervals that
occur between lower partials and the fundamental sound more consonant when they
are tuned to the integer ratio between that partial and the fundamental.

Estimating the Perception of Complexity in Musical Harmony
Jose Fornari, Tuomas Eerola; University of Jyväskylä, Finland
2AM2-R07-07
The perception of complexity in musical harmony is here seeing as being directly
related to the psychoacoustic entropy carried by the musical harmony. As such,
it depends on a variety of factors mainly related to musical chords structure and
progression. The literature shows few examples of the computational estimation, di-
rectly from audio, of the complexity in musical. So far, the perception of this feature
is normally rated by means of human expert listening. An eﬃcient computational
model able to automatic estimate the perception of complexity in musical harmony
can be useful in a broad range of applications, such as in the ﬁelds of: psychology,
music therapy and music retrieval (e.g. in the large-scale search of music databases,
according to this feature). In this work we present an approach for the computational
prediction of harmonic complexity in musical audio ﬁles and compare it with its
human rating, based on a behavioral study conducted with thirty-three listeners
that rated the harmonic complexity of one hundred music excerpts. The correlation
between the results of the computational model and the listeners mean-ratings are
here presented and discussed.

A Hybrid Model for Timbre Perception — Part 2: The Texture of
Sound
Hiroko Terasawa, Patty Huang, Jonathan Abel, Jonathan Berger;
Stanford University, USA
2AM2-R07-09
We propose a hybrid model of timbre integrating two complementary component
models, one of “color” and the other of “texture”. Previous studies on timbre
perception describe a multidimensional space, in which spectral centroid, spectral
ﬂuctuation, and temporal attack and decay characteristics constitute the principal
components. We propose that these factors can be eﬀectively described in terms of a
waveform’s instantaneous spectral envelope (the color), and instantaneous temporal
irregularity (its texture). The texture model employs normalized echo density (NED),
developed by Huang and Abel (“Aspects of Reverberation Echo Density,” AES 123rd
Convention, New York, October 2007), as a metric to characterize the texture of noise
with diverse granularity. Originally developed to describe perceived reverberation
echo density, NED also appears to be useful for describing the temporal irregularities
(texture) found in any sound object. In this experiment, we investigated the correla-
tion between NED and perception of noise-like sounds with various static textures.
The predictability of NED is consistent across the bandwidths tested, and at average
NED explains 93% of the subjective judgments of texture dissimilarity. We therefore
propose NED as a useful perceptual descriptor of a sound’s time variant texture
density.


                                          53
Tuesday 26 August 2008                                                     ICMPC 10

Eﬀects of Temporal Synchrony Between Two Sounds on
Perceptual Impression Space and Its Relation to the Cochlear
Delays
Eriko Aiba 1 , Minoru Tsuzaki 1 , Satomi Tanaka 1 , Masashi Unoki 2 ;
1
  Kyoto City University of Arts, Japan; 2 JAIST, Japan
2AM2-R07-11
Onset synchrony is widely assumed to be an important cue for perceptual uniﬁcation
as a single tone. However, even if all the components physically begin simultaneously,
their temporal relation might not be preserved at the cochlear level. This is called the
cochlear delay. Our previous experiments about accuracy of judgment on perceptual
synchrony between two sounds suggested the accuracy such as enhance the cochlear
delays was higher than that such as cancel out the cochlear delays. Therefore, there
is a possibility that the easiness of the perceptual uniﬁcation diﬀers according to
the sound, because the range of the gap in time that assumed to be simultaneous
is diﬀerent depending on the sound. The perceptual impression space induced by
the variation in the temporal gap between two sounds was estimated by the multi-
dimensional scaling method. Three types of complex tones were used as stimuli
to investigate whether the cochlear delay imposes a systematic bias in judging the
perceptual uniﬁcation of two sounds. As a result, the perceptual uniﬁcation appears
to occur more easily on the sounds such as cancel out the cochlear delays than that
such as enhanced the cochlear delays. This result suggests the auditory system
appeared more “tolerant” of the delay following the intrinsic, natural direction, i.e.,
the cochlear delay.

The Chromelodeon Scale: A Psychoacoustical Model of Roughness
versus Harry Partch’s One Footed Bridge
Alexandre Torres Porres; University of São Paulo, Brazil
2AM2-R07-13
Under a Psychoacoustical point of view, and because of the partials’ alignment, it is
the spectrum of sounds that determines the consonance of given musical intervals.
For example, harmonic spectrums do align their components in harmonic musical
intervals. On this paper, I adopt a computer software to analyze the spectrum of the
Harmonium built by Harry Partch (the Chromelodeon). Its results show which are the
most consonant steps in the span of an octave. I use this data to compare to a similar
graph designed by Harry Partch, his One Footed Bridge. A major concern behind this
research is to question at what extent musical intervals related to higher harmonics
do provide a perceptual signiﬁcant consonance perception. Another consequence is
to promote a revision of Harry Partch’s theoretical work.


2AM2-R08 : Rhythm and Timing / Body Movement /
Memory
Rooms 8, 10:40 – 12:30 Tuesday 26 August 2008, Poster session

Eﬀects of Musical Training and Tapping the Beat on Perceived
Complexity of a Rhythm
Louis N. Vinke, J. Devin McAuley; Bowling Green State University, USA
2AM2-R08-01
Musical rhythms vary in complexity. However, many questions remain concerning
the best way to measure rhythmic complexity in a manner that corresponds to
listeners’ perceptions. The present study (1) examined the potential mediating roles
of musical training and tapping a beat on listeners’ subjective ratings of rhythmic
complexity and (2) evaluated previously proposed measures of rhythmic complexity.
Of particular interest was the PS-measure proposed by Shmulevich & Povel (2000).
Musicians and non-musicians rated the complexity of sixty-ﬁve auditory rhythms on
a six-point scale (1 — very simple, 6 — very complex) in listen-and-tap and listen-only
conditions. In the listen-and-tap condition, participants tapped along with rhythms
at a steady rate that they felt coincided with the ‘beat’ before making a complexity
judgment. In the listen-only condition, participants did not tap before making a
complexity judgment. Results reveal that tapping the beat increased complexity
ratings, but there were no clear diﬀerences attributable to musical training. Overall,
mixed support was found for the PS measure of rhythmic complexity. Although the
PS measure was positively correlated with judged complexity, tapping the beat while


                                          54
ICMPC 10                                                Tuesday 26 August 2008

listening to each rhythm, which was expected to strengthen the induction of a ‘beat’,
increased rather than decreased complexity ratings.

Examining the Relationship Between Phonological Skills and
Temporal Processing in Very Young Children
Charles Wigley, Janet Fletcher, Jane Davidson; University of Western
Australia, Australia
2AM2-R08-03
It is widely acknowledged that phonological processing plays a critical part in literacy
acquisition. Recently however, questions have been raised about the nature of these
processes and currently there is evidence that basic temporal processing abilities
may also signiﬁcantly contribute to literacy development. This study investigates the
links between a purely motor-rhythmic measure of temporal processing ability (syn-
chronous beat tapping) and two pre-literacy indicator measures: Rapid Automized
Naming [RAN] and Phonological Awareness [PA], at the early stages of literacy
acquisition. Fifty kindergarten children were tested on PA, RAN, synchronized
beat tapping, IQ and musical perception. Success for synchronised beat tapping
was calculated as the maximum cross-correlation (Max r) between the transformed
stimulus and response waveforms and the top and bottom performing quartiles
were examined for diﬀerences on the pre-literacy skill measures using ANCOVA.
Controlling for age, IQ and musical perception skills, a signiﬁcant diﬀerence was
found for RAN (F (1, 23) = 7.17, p < .05, η2p = .27) but not PA (F (1,23) = 1.45, p
> .05). These results suggest that at the earliest stages of literacy development,
basic temporal processing skills (such as those used to perceive beats and generate
rhythmic movements) may contribute to at least one behavioural measure strongly
associated with literacy acquisition (RAN).

Moving to Music: The Inﬂuence of Familiarity, Enjoyment, and
Groove on Spontaneous Dance
Bradley W. Vines 1 , Petr Janata 2 ; 1 University of British Columbia,
Canada; 2 University of California at Davis, USA
2AM2-R08-05
Moving spontaneously while listening may be the most common act of musical
engagement. The experience of familiarity, enjoyment, and groove may aﬀect one’s
propensity to move. This research aims to determine how these three factors aﬀect
spontaneous dance. Participants completed online questionnaires pertaining to their
general background in music and dance, and suggested music that makes them feel
like dancing. They listened to segments of their own suggestions, as well as the other
participants’, and rated them on familiarity, enjoyment, groove, and the “urge to
move.” In the laboratory, a movement-tracking system measured participants’ body
movements while they listened to the music presented online. A preliminary analysis,
using linear regression and correlation statistics, revealed that the perception of
groove accounted for the most variability in the urge to move, and that judgments
of the urge to move best modelled the amount of head movement. Further analyses
will explore patterns of spontaneous motion associated with the experience of being
in the groove with music, as well as the eﬀects of interactions among familiarity,
enjoyment, and groove on body movement. These data provide evidence for the
connection between perceived groove and motor activity in the brain.

‘Notes Inégales’ in Contemporary Performance Practice
Dirk Moelants; Ghent University, Belgium
2AM2-R08-07
Playing the ﬁrst of two equally notated notes notably longer than the second,
the so-called ‘notes inégales’, is a common practice in the performance of French
baroque music. It is a means of expression and enhances the metric structure of
the (dance) music. Although there is a general agreement between performers about
the application of ‘inequality’, its exact performance is an ongoing source of debate.
In an experiment 8 harpsichordists and 8 baroque violinists performed 6 melodies
of French baroque gavottes in three tempo conditions 40-60-80 bpm, along with a
metronome. The mean ratio of inequality was about 1.63:1. Yet, a lot of variability
was found with mean ratio’s of individual performers varying between 1.89 and 1.33.
Another main source of variance is the metric structure, with larger inequality found
at metrically important points. The base tempo also has an important inﬂuence on
the performance of the ‘inégalité’, but it is treated in very diﬀerent ways by diﬀerent


                                          55
Tuesday 26 August 2008                                                    ICMPC 10

performers. Pitch factors have only a minor impact. Even in simple pieces individuals
convey a personal expressivity through their use of ‘notes inégales’. The results
can be related to historical evidence (e.g. from mechanical instruments) and to the
prosody of the French language.

Long-Term Memory for Simple and Complex Music: Quantity and
Quality of Practice
Jane Ginsborg, Jenny Pitkin; Royal Northern College of Music, UK
2AM2-R08-09
Research into the recall of musicians for the music they perform from memory
has focused on the eﬀects of musical expertise, mental representations and the
development of performance cues, practice and memorizing strategies and the
characteristics of the music itself. The process whereby the memory trace of a
performer for a speciﬁc piece of music fades over the course of time has also
been described. The present study aimed to explore the eﬀects of the expertise
of performers and their practice quantity and quality on accuracy of recall for
one of two newly composed pieces of music, simple and complex, in a series of
performances from memory. Conservatoire students, all wind instrumentalists, kept
practice diaries as they memorized either a simple or complex piece, which they
then performed from memory over a four-week period. Errors and hesitations were
analyzed in order to calculate accuracy and ﬂuency of recall in each performance.
There was a signiﬁcant eﬀect of expertise but no eﬀects of quantity of practice or
level of complexity. Content analysis of the diaries suggested that quality of practice
— like quantity — varied widely. Strategies used by the more successful memorizers
included mental practice, auditory and visual approaches, and attention to details.
These ﬁndings have obvious applications for the teaching of eﬀective practicing
strategies at all levels.

Nameability: Can Unique Verbal Labels Improve Recollection of
Music and Faces?
Jack D. Birchﬁeld 1 , James C. Bartlett 1 , Andrea R. Halpern 2 ;
1
  University of Texas at Dallas, USA; 2 Bucknell University, USA
2AM2-R08-11
The present research examines whether nameability, deﬁned here as the association
of a proper name or some other uniquely identifying description with a given stimu-
lus item, enhances the role of recollection in recognition of melodies. An additional
goal was to extend our ﬁndings with melodies to another class of non-verbal stimuli:
photographs of faces. To dissociate familiarity and nameability, we assembled
familiarity-matched sets of highly nameable and low nameable items, as well as
sets of unfamiliar items. Stimulus items were presented in a study/test procedure,
using the remember-know paradigm to measure recognition performance. Our main
hypothesis, that remember responses (indicating a recollective experience) would be
more frequent for high-nameable items than for low-nameable items, was conﬁrmed
for both melodies and faces. A secondary hypothesis, that know responses (suggest-
ing judgments based on familiarity) might be more frequent for low-nameable items
than for high-nameable items, was also conﬁrmed for both stimulus types. However,
overall hit rates (remember + know responses) for high- and low-nameable items were
signiﬁcantly diﬀerent for faces but not for melodies. We suggest that recognition
memory for melodies may be more dependent on a sense of global familiarity than
on a speciﬁc verbal label.

Memory for Tempo in Oral Music Traditions: Evidence for
Absolute Tempo in Aboriginal Ceremonial Song?
Freya Bailes 1 , Linda Barwick 2 ; 1 University of Western Sydney,
Australia; 2 University of Sydney, Australia
2AM2-R08-13
While long-term memory for the tempo of highly familiar music can be precise, it has
been suggested that there is greater variability in the recalled tempi of performances
of music for which no ﬁxed version is known, as in oral music traditions. This
study examines the tempi of multiple performances of one particular Aboriginal
dance-song, to determine tempo stability within a speciﬁc oral music tradition.
Djanba are ceremonial dance-songs from the Murriny Patha speaking people in the
Wadeye area of the Northern Territory of Australia. The songs are characterized
by a distinctive clap stick beat. We analysed the tempo of 49 extant recordings of


                                         56
ICMPC 10                                                  Tuesday 26 August 2008

djanba 14, spanning a period of 34 years. The mean tempo of the recordings is 141
bpm (range 127–148), and a standard deviation of 3 bpm indicates that performance
tempi deviated positively or negatively, on the average, by only 2%. There was no
overall tendency for the tempo to decrease or increase either during a performance,
or across the 34-year span. The lack of variation in the tempo of djanba 14 is
remarkable in the face of the numerous factors that are likely to modulate both
choice of tempo and its stability through time, and these factors are discussed.


2AM2-R09 : Music Listening and Preferences /
Development / Performance / Audio-Visual Interactions
/ Psychoacoustics
Rooms 9, 10:40 – 12:30 Tuesday 26 August 2008, Poster session

Eﬀects of Orchestration on Musicians’ and Nonmusicians’
Perceptions of Musical Tension
Brian A. Silvey; University of Texas at Austin, USA
2AM2-R09-01
The purpose of this study was to examine the eﬀects of orchestration on musicians’
and nonmusicians’ (N = 40) perception of musical tension. Participants were asked
to register their perceptions of tension using the Continuous Response Digital
Interface (CRDI) dial while listening to three orchestrations of the movement Bydlo
from Mussorgsky’s Pictures at an Exhibition. The full orchestra and brass quintet
stimuli were digitally altered to have the same amplitude, frequency, and duration
as the third orchestration for solo piano. Graphic analysis revealed similarities in
how musicians and nonmusicians perceived tension across time for each of the three
stimuli. Pearson product-moment correlations between the participant groups were
all statistically signiﬁcant at the p < .001 level and highest for brass quintet (r = .96),
followed by full orchestra (r = .91), and piano (r = .78). Notably, musicians perceived
the piano orchestration as being the least tense of the stimuli while nonmusicians
felt this was the most tense, perhaps suggesting a diﬀerence in perceived tension
response based on timbre. Overall, musicians’ tension responses displayed less
variability among the stimuli than nonmusicians who tended to use more of the dial
when registering their responses.

Musicians’ Transformations of the Listening Process: An
Exploratory Study
Caroline Davis; Northwestern University, USA
2AM2-R09-03
Since the advent of recording technology, listeners have been accustomed to hear-
ing music they know at the push of a button. Professional musicians often use
recordings to hone their craft, repeatedly listening to ﬁnd inspiration and inﬂuence.
Previous studies have found evidence for increased emotional response, structural
and thematic awareness, and preference over repeated listening, but these ﬁndings
depend on a number of musically related features. This study addresses alterna-
tives for cognitive changes during repeated exposure, drawing upon sociocultural
variables such as shared mental representations and schemata. Semi-structured in-
terviews were conducted with eleven professional musicians. In individual sessions,
musicians were asked to describe both ingrained and barely familiar recordings.
Many described similar processes of listening, spanning from the induction of
enjoyment to analysis, and in many cases, to transcription. Musicians also speak
of the importance of discovery in the listening process, suggesting that they seek
out changes in schemata. In addition, musicians related by multiple performance
collaborations experience similar cognitive transformations. Interview statements
show that musicians’ cognitions change not only according to structure, emotion,
and preference, but also according to cultural schemata.

Who Listens to What Music, and Why? Correlations Between
Personality, Music Preferences, and Motives for Listening in a
Sample of Older Adults
Petri Laukka; Uppsala University, Sweden
2AM2-R09-05
A questionnaire was sent to a random sample of 500 Swedish elderly (65–75 years).
It included (a) a brief Big-5 personality measure, and questions about (b) how often

                                           57
Tuesday 26 August 2008                                                     ICMPC 10

various music listening strategies were used in everyday listening and (c) preference
for diﬀerent musical styles. Several positive correlations were found between per-
sonality and music preferences: e.g., extraversion/(jazz & classical), (agreeableness
& conscientiousness)/easy listening, and (emotional stability & openness)/jazz.
Positive correlations were also found between personality and motives for listening:
e.g., extraversion/motives related to identity and agency, agreeableness/mood
regulation, and (conscientiousness & emotional stability)/enjoyment. Emotional
stability and openness were further negatively correlated with relaxation. Finally,
correlations were found between music preferences and motives for listening: e.g.,
easy listening/(enjoyment & relaxation), folk music/(enjoyment & mood regulation),
jazz/enjoyment, and classical/(enjoyment & mood regulation). All eﬀect sizes were
small to moderate. The results suggest that some associations between person-
ality and music preferences generalize across age-groups, though the particular
styles of music diﬀer across generations. Like in previous research, openness was
associated with liking of both complex (classical, jazz) and intense (rock) styles,
whereas agreeableness was associated with preference for more conventional styles
(easy listening). Results also suggest that motives for listening are mediated by
the listener’s personality characteristics. Finally, certain styles of music may be
preferred depending on the particular motives for listening.

Does Music Taste Last? A Mixed Methods Study of Music Taste
Over the Youth-Adult Transition
Douglas Lonie 1 , Patrick West 1 , Graeme Wilson 2 ; 1 MRC Social and
Public Health Sciences Unit, UK; 2 Newcastle University, UK
2AM2-R09-07
Although much work has been conducted to identify and map diﬀerent music tastes
there is a lack of longitudinal data in the literature. The aims of this paper are to
investigate the nature of music taste over the period 1987–2006 and to understand
perceptions of the factors contributing to music tastes and how this changes over
time. To do this, data regarding music taste were collected from a cohort every
three to ﬁve years, since participants were aged 15. This was followed by qualitative
interviews with a subsample of 18 participants, designed and analysed using Inter-
pretative Phenomenological Analysis (IPA).
The statistical analysis indicated 5 taste trajectories, labelled: Consistent, Prodigal,
Early Shift, Late Shift, and Transient. Around half the sample displayed consistent
taste across the youth-adult transition. The IPA analysis revealed a range of expo-
sures and diﬀering life events over the youth-adult transition as cited by participants
to account for their changing music tastes.
The social factors inﬂuencing music taste emerged as primary within individuals’
accounts of their taste trajectories. The inﬂuence of social structure (e.g. marriage,
child rearing and employment) was recognised by participants as the strongest
explanation for their changing taste and changing musical participation. These
ﬁndings indicate that music taste should be considered as dynamic and related to
other social and personal circumstances over the lifecourse.

The Eﬀects of Kindermusik Training on Infants’ Abilities to
Encode Musical Rhythm
David W. Gerry, Ashley Faux, Laurel J. Trainor; McMaster University,
Canada
2AM2-R09-09
Previous work showed that when infants were bounced on every second beat of an
ambiguous (unaccented) rhythmic pattern, they later preferred to listen to an auditory
alone version of the pattern with accents every second beat (march), whereas infants
bounced on every third beat of the same unaccented rhythmic pattern preferred to
listen to an accented version of the pattern with accents every third beat. We tested
infants in Kindermusik classes in the identical procedure to see whether enriched
experience with rhythm aﬀects the development of metrical perception. As with
infants not in Kindermusik classes, movement inﬂuenced metrical interpretation
in those enrolled in Kindermusik. Overall, those enrolled in Kindermusik listened
longer during the test, indicating heightened interest in music. However, unlike
the infants in the previous study, those taking Kindermusik showed a general bias
to interpret patterns in groups of two rather than in groups of three. This bias is
consistent with the musical materials of their classes, which are predominantly in
march as opposed to waltz rhythms. We conclude that musical classes for infants
can aﬀect the development of metrical perception.

                                          58
ICMPC 10                                                Tuesday 26 August 2008

A Perceptual Study on Asynchrony Between Auditory and Visual
Stimuli: The Eﬀect of Anticipation by Motion or Periodicity
Minori Saikawa, Kohei Washikita, Masashi Yamada; Kanazawa
Institute of Technology, Japan
2AM2-R09-11
It has been shown that the delay of an auditory stimulus has a greater range of accept-
ability than the case of an auditory stimulus prior to a visual one for the perception
of synchrony in audio-visual stimuli. In these studies, the following audio-visual
stimuli were typically used: For visual stimulus, anticipation of timing is available by
motion of an object. In the present study, another type of anticipation by periodicity
was also prepared. These two types of anticipation took place in auditory and visual
stimuli respectively. The detection threshold of asynchrony was determined for
each type of stimulus described above, and compared with the threshold of the
audio-visual stimulus where no anticipation is available. The results showed that the
threshold for the sound-prior condition was consistent in 80–100 ms for the stimuli
where anticipation was available, but for the “no anticipation” stimulus, it showed a
quite large value of 150 ms. For the sound-later condition, the threshold commonly
showed a large value at 160–220 ms, not only for the “no anticipation” stimulus, but
also for the “anticipation” stimuli, except for “visual periodicity” stimulus for which
the threshold was 98 ms. The results imply that anticipation aﬀects the detection of
asynchrony, and suggest that there is an interaction between the type of anticipation
and modality.

Computational Model of Congruency Between Music and Video
Tetsuro Kitahara 1 , Masahiro Nishiyama 2 , Hiroshi G. Okuno 2 ;
1
  Kwansei Gakuin University, Japan; 2 Kyoto University, Japan
2AM2-R09-13
We propose a method for calculating congruency between music and video based on
similarity of accent structure and mood. There are two types of congruency between
music and video: temporal congruency related to synchronization of accents and
semantic congruency related to similarity of mood. Previous works, however, have
dealt only with either congruency. We model the temporal congruency based on
the correlation between accent feature sequences extracted from audio and visual
content, and the semantic congruency based on mutual mapping between two feature
spaces representing music and video respectively. Then, we integrate the two types
of congruency as a weighted linear sum. Our experiments with real-world content
show the eﬀects of our method.


2AM2-R10 : Emotional Aspects / Rhythm and Timing
Rooms 10, 10:40 – 12:30 Tuesday 26 August 2008, Poster session

Music and Emotion: An Experimental Study on Emotional
Responses from Musicians and Nonmusicians to Modal Musical
Excerpts with Tempo Variation
Danilo Ramos 1 , José Lino Oliveira Bueno 1 , Emmanuel Bigand 2 ;
1
  University of São Paulo, Brazil; 2 University of Bourgogne, France
2AM2-R10-01
The purpose of this study was to verify emotional responses from Musicians and
Nonmusicians on modal musical excerpts with tempo variation. Participants were
30 Musicians and 30 Nonmusicians. Three musical excerpts on moderato tempo,
constructed on the Ionian mode, and transposed to the Dorian, Phrygian, Lydian,
Mixolydian, Aeolian and Locrian modes and to the largo and presto tempi were
employees. The tasks of the participants were listening to each musical excerpt
and associating them to Joy, Sadness, Serenity or Fear/Anger. The results show a
greater uniformity in the emotional responses from Musicians than Nonmusicians.
The results suggest a greater inﬂuence of the tempo on the mode in the triggering
of speciﬁc emotions in listeners, except the Locrian mode, where there was a greater
inﬂuence of the musical mode, regardless in which tempo the musical excerpts were
heard.




                                          59
Tuesday 26 August 2008                                                    ICMPC 10

Cognitive Styles Inﬂuence Perceived Musical Coherence
Gunter Kreutz 1 , Laura Mitchell 2 , John McDowall 2 , Emery Schubert 3 ;
1
  University of Oldenburg, Germany; 2 Glasgow Caledonian University,
UK; 3 University of New South Wales, Australia
2AM2-R10-03
Baron-Cohen and co-workers propose a theory of individual diﬀerences that is based
on two characteristic cognitive styles (Baron-Cohen, Knickmeyer and Belmonte, 2005),
called empathizing and systemizing. The present study explores the inﬂuences of
general and music cognitive styles on perceived musical coherence. A total of 64
participants (33 female) were selected who were scoring high in either (general or
music) empathizing or systemizing. Participants listened to pairs of pre-selected
music excerpts, four representing classical and four representing popular music.
Within each genre, two excerpts expressed a ‘happy’ tone and two excerpts conveyed
a ‘sad’ tone. Pairs of excerpts were matched in genre and mismatched in emotion, or
vice versa. Participants rated how well excerpts within each pair went together on
a 10-point Likert-type scale. Results show that coherence ratings were signiﬁcantly
higher for emotion-matched than for genre-matched pairs. Signiﬁcant interactions
were found between both general and musical cognitive styles on the one hand and
type of matching on the other. In particular, general systemizing shows a tendency
to be associated with higher values of genre-matching (p = .051) whereas music
empathizing leads to signiﬁcantly higher values of emotion-matching (p < .05). These
results suggest that cognitive styles may inﬂuence high-level cognitive processes in
music listening. [Acknowledgement: We would like to thank the Carnegie Trust for
the Universities of Scotland for funding this study.]

Comparison Between Perceived Emotion and Felt Emotion in
Music Listening: Analysis of Individual Diﬀerence and Musical
Expertise by Use of Factor Score
Etsuko Hoshino; Ueno Gakuen University, Japan
2AM2-R10-05
Two experiments were conducted to investigate the diﬀerences in perceived emotion
and felt emotion using classical music excerpts. In Exp.1, 54 students who did not
major in music were asked to listen to each phrase from four orchestral pieces
twice. Half of them (Perceived-Emotion group) were instructed to rate the emotional
characters of the phrases on 30 seven-point adjective scales each time, while the
other half (Felt-Emotion group) rated the emotion aroused by them. Factor analysis
was conducted then separately for each group. In addition, the data of two groups
was also analyzed together by other factor analysis once more to compare the factor
scores of all of the subjects directly. In Exp.2, the same procedure was carried out
with 17 other students majoring in music, except that only one factor analysis was
conducted together for the two music groups. Standard factor scores of the subjects
in these two experiments were calculated, and compared between both listening
conditions in each group of subjects. The results suggested that (1) Perceived
emotion was not exactly identical to felt emotion in each semantic structure, and that
(2) musical expertise and emotional responsiveness of music were related closely.

Therapeutic Forgetting? Relaxing Music Counters
Hyperconsolidation of Emotionally Arousing Stimuli
Lauren Velik, N.S. Rickard; Monash University, Australia
2AM2-R10-07
Memory for emotionally powerful events can sometimes be unusually heightened
and unwanted, for instance, in the case of post-traumatic stress disorder. Recent
research has shown that the strength of memory for emotionally intense events
can be reduced by blockade of beta-noradrenergic receptors. The current study
explored whether enhanced memory for emotional events could be prevented by
concurrent exposure to relaxing music. Participants (29 males, 60 females; mean
age=22years, SD 5.83) viewed an emotional or non-emotional slideshow accompanied
by either silence or Satie’s Gymnopedie No. 1. Recall of the slideshow was tested one
week later, using 76 four-alternative forced-choice (4AFC) questions. Groups were
equivalent on mood and arousal levels prior to treatment, and the groups exposed
to the music reported equivalent familiarity and likeability. The ﬁndings revealed
that recall of the slides shown immediately after the emotionally arousing images
was signiﬁcantly greater than recall of the non-emotional images. However, this
eﬀect was entirely prevented in the presence of calming music. A group by phase


                                         60
ICMPC 10                                              Tuesday 26 August 2008

interaction eﬀect was observed, F(2,168)=3.45, p=.003, with Tukey’s post-hoc tests
conﬁrming that calming music resulted in similar levels of recall of the emotional
story as observed with the non-emotional story. These ﬁndings provide promise
for a non-pharmacological means of ameliorating excessively strong and intrusive
memories associated with trauma.

Emotion-Related Autonomic Nerve Activity with Musical
Performance and Perception
Hidehiro Nakahara 1 , Shinichi Furuya 2 , Tsutomu Masuko 3 , Satoshi
Obata 4 , Hiroshi Kinoshita 4 ; 1 Morinomiya University of Medical
Sciences, Japan; 2 Kwansei Gakuin University, Japan; 3 Mukogawa
Women’s University, Japan; 4 Osaka University, Japan
2AM2-R10-09
Previous studies have demonstrated signiﬁcant changes in cardiac and autonomic
measures during musical perception. The changes are commonly evident when the
listeners are emotionally aroused to a pleasurable music. However, there has been
no attempt on a similar issue for musical performance. The present study therefore
examined the eﬀect of emotion on the heart rate (HR) and heart rate variability (HRV)
associated with musical perception and performance. Eleven pianists underwent
experiments under the expressive piano playing, non-expressive piano playing,
expressive listening, and non-expressive listening conditions. The music selected
was the well-tempered Clavier, Vol. I, No.1 prelude (J. S. Bach), which was played 60
bpm and needed 2.5 min for one tune. HR and HRV data (HFpower, LFpower, HF/total
power, LF/total power, RMSSD) were evaluated using 5-min (two tunes) data, and 20
sec data at the self-reported highest pleasant period. It was found that expressive
eﬀort in perception as well as performance modulated HR and HRV, and that such
modulations were much greater for the musical performance than perception. The
results conﬁrmed that musical performance was far more eﬀective in modulating
emotion-linked cardiac and autonomic nerve activity than musical perception in
musicians.

Rhythm Play and Enhanced Emotional Sensitivity in Children
Yuriko Nakanishi 1 , Takayuki Nakata 2 ; 1 Nagasaki Prefectural Center
for Children Women and Persons with Disabilities, Japan; 2 Future
University-Hakodate, Japan
2AM2-R10-11
This study examined if three months of group rhythm activities enhance 3–4
year-olds’ emotional expressivity. Twelve children from one child care center were
assigned to the rhythm play condition and seven children from another child care
center were assigned to the control condition. Children in rhythm play condition
experienced rhythm play for three months that emphasized synchronizing rhythm
with others. Children in rhythm play condition were individually presented with
10-minute long puppet shows before and after the rhythm play and video recorded.
For the children in the control condition, two puppet shows with video recordings
were separated by three months without experiencing group rhythm play in between.
Between puppet shows, all children experienced usual musical activities at the child
care centers, including singing or listening to music CDs between recordings. Anal-
yses of adults’ ratings of the videos revealed that when emotional events involved
the target child, ratings were higher for children in rhythm condition than in control
condition, but no diﬀerence was found between groups when emotional events
involved puppets only. Also, when events were positive in valence, ratings were
higher for children in the rhythm play condition than in the control condition, but
no group eﬀects were found for negative emotional events. These ﬁndings suggest
that learning to synchronize rhythm with others may help 3–4 year-olds to express
their positive emotions.

                                                                                   th
Steady Beat Production (SBP) with Various External Stimuli by 7
Grade Students
Yo Jung Han, Sun-Hee Chang; Seoul National University, Korea
2AM2-R10-13
To investigate the characteristics of Steady Beat Production (SBP) 30 music-majored
students (trained students) and 30 general students in 7th grade participated. There
were 5 tasks consisted of SBP without external signals, with visual and auditory
signals and with metric and rhythmic sequences. Means of tempo, steadiness of beats

                                         61
Tuesday 26 August 2008                                                    ICMPC 10

and accuracy of synchronization were analyzed by recording the means of individual
IOIs, the SD of individual IOIs and synchronization errors. The study found that
participants tended to produce faster than the tempo was given with stimuli. SBP
without external signals was the fastest and the diﬀerence of its individual results
was the largest. SBP with auditory signals was steadier and more accurate than
with visual signals. SBP with rhythmic sequences was steadier than with metric
sequences. Trained students were more accurate and steadier in all tasks except
SBP with auditory signals. In regard to tempo and steadiness, a relationship was
found between SBP both with and without signals. With regard to steadiness, a
similar relationship seemed to exist, except for with auditory signals. In conclusion,
the sensory modality and structural feature of stimuli plays an important role in
SBP. Musical training aﬀects extracting the periodicity from external stimuli and
responding to the stimuli more accurately.


2AM2-R11 : Education / Performance
Rooms 11, 10:40 – 12:30 Tuesday 26 August 2008, Poster session

Rhythm and Reading: Improvement of Reading Fluency Using a
Rhythm-Based Pedagogical Technique
Scott D. Lipscomb 1 , Dee Lundell 2 , Corey Sevett 3 , Larry Scripp 4 ;
1
  University of Minnesota, USA; 2 Minneapolis Public Schools, USA;
3
  Music-in-Education National Consortium, USA; 4 New England
Conservatory, USA
2AM2-R11-01
In the Fall of 2007, a music-integrated reading program was introduced to a north
Minneapolis urban school to help third grade students improve reading ﬂuency. The
purpose of this investigation was to determine to what extent the use of musical
rhythm could facilitate the acquisition of reading ﬂuency and improve accuracy when
reading high frequency sight words.
The “Rhythm & Reading Group” was established in each of three third grade class-
rooms. This pedagogical method involves establishment of a clear beat to which
students read lists of 25 words. A variety of tempos and word orders were used to
provide variety so that the activity continuously engaged the students. A carefully
designed assessment schedule was established to measure reading ability at two- to
three- week intervals throughout the testing period.
Results revealed impressive levels of improvement for both students below grade-
level reading ability and for those at or above grade-level. Students improved
dramatically in their reading ﬂuency on the list of 25 words, as measured by the
number of words read correctly (increased signiﬁcantly) and the time required to
read the list of words (decreased signiﬁcantly). Results revealed signiﬁcant transfer
of reading ﬂuency, as both the accuracy and reading time improved signiﬁcantly for
lists of words not seen since the initial pre-test.

Eﬀects of Metacognitive Instruction on Thinking During Piano
Practice in Experienced Pianists and Novices
Iwao Yoshino; Hokkaido University of Education, Japan
2AM2-R11-03
This study aims to investigate whether musical experts are more metacognitively
aware during practice than novices, and whether any instruction can enhance
metacognitive activity. Eight experienced pianists (music majors) and ten novices
practiced a short piece for piano under “normal condition” or “metacognitive
instruction condition”. When they temporally stopped physical practice, they were
asked to tell what they had thought during practice and what they were thinking at
that time. Participants of metacognitive instruction condition were also asked to
set their goal before practice, to evaluate their performance, and to reﬂect and plan
their practice at the time stopping physical practice. Their video-recorded verbal
reports were classiﬁed into metacognitive reports and cognitive reports. Metacogni-
tion scores were calculated according to quality and quantity of the metacognitive
reports. Under the normal condition, experienced pianists demonstrated signiﬁ-
cantly higher metacognition scores than novices. Metacognitive instruction yielded
signiﬁcantly higher metacognition scores to only novices. Experienced pianists were
not inﬂuenced by metacognitive instruction because they appeared to have high
metacognitive skills. This study demonstrated that metacognition was relevant to

                                         62
ICMPC 10                                               Tuesday 26 August 2008

the acquisition of expertise also on a musical instrument, and that metacognitive
instruction could enhance metacognitive activity particularly for beginners. Making
instrument students image their goal, monitor their problems and think methods of
solving them may develop their capacity of self-regulated learning.

Stirling Silver: Understanding the Psychology of Group Singing for
Health and Wellbeing
Jane Davidson 1 , Andrea Lange 1 , Bev McNamara 1 , Sue Jenkins 2 , Gill
Lewin 2 , Lorna Rosenwax 2 ; 1 University of Western Australia,
Australia; 2 Curtin University of Technology, Australia
2AM2-R11-05
This poster aims reports on the degree to which various health beneﬁts of singing can
be claimed. Stress reduction, feelings of wellbeing and a general sense of being better
exercised have been claimed (Bailey and Davidson, 2003, 2005, and 2008). But the
question still remains whether it is the singing activity itself which promotes these
espoused beneﬁts or other factors. A choir of people who had never participated
in group singing or singing in any formal context was formed, matched for age and
social group. All were over 70 years of age, half were in receipt of home-help services
and were regarded as being marginally socially isolated. The other half was not
receiving any social assistance. Standardised measures of social wellbeing, health
and emotional state as well as detailed interviews were made and pre and post a
singing course that lasted 12 weeks. The choral leader’s remit was to introduce basic
techniques of singing (posture, breath control, voice placement) alongside facilitating
social group process: jokes, coﬀee breaks, group tasks etc. All participants reported
positive beneﬁts of singing and feelings of having had exercise, but none found
these as signiﬁcant as the social aspects, including their interactions with the choral
leader. These results have important implications revealing the important social
psychological beneﬁts of the group activity of singing.

Music Provision in Young Children’s Education: Scottish
Perspectives
Raymond MacDonald 1 , Lana Carlton 1 , Katie Reid 2 , Tom Bancroft 3 ,
Cerin Richardson 4 ; 1 Glasgow Caledonian University, UK; 2 University
of Glasgow, UK; 3 ABC Creative Music, UK; 4 Edinburgh City Council,
UK
2AM2-R11-07
From an international perspective there is increasing interest in early years music
interventions in terms of how best to implement such education programmes and
the types of outcomes that can be expected from these activities. This study aimed
to gain insight into the organisation and delivery of music provision in Scottish early
education. It explores the experiences and perspectives of music service providers
at various levels across nursery schools and primary schools (Children aged 3 to
12). The present study utilised four semi-structured focus groups and an individual
interview. Participants (n=20). included primary and nursery teachers, project
workers, educational oﬃcers, and music programme developers. Results highlight
a number of key themes. For example: wider social and educational beneﬁts for
children in music education; a perceived lack of music education specialists; issues
relating to teacher training. Appraisal of various music provision programmes
was also discussed and issues of language and culture in music were highlighted
as important. The heterogeneous mix of contributors in this research project has
provided a unique opportunity to stimulate discussion between those who make
music programmes available to teach and those who are expected to teach them.
As a result, key issues relating to the success of early music provision have been
identiﬁed. These include tackling teachers’ conﬁdence in their ability to ‘teach’ music
and identifying ‘core’ components in the music programme utilised.

The Eﬀect of Contextual Interference on Instrumental Practice: An
Application of a Motor Learning Principle
Laura A. Stambaugh, Steven M. Demorest; University of Washington,
USA
2AM2-R11-09
The purpose of this study was to examine the eﬀects of low (blocked), moderate
(hybrid), and high (serial) levels of contextual interference during one practice


                                         63
Tuesday 26 August 2008                                                    ICMPC 10

session on the technical accuracy and musicality of seventh grade clarinet and
saxophone students at acquisition and retention and their attitude towards the
practice conditions. In motor learning, the contextual interference hypothesis
predicts the blocked condition would exhibit higher performance at acquisition and
the serial condition at retention. Students (N=19) practiced three eight-measure
songs during one 18-minute practice session. They were randomly assigned to
either the blocked group (switch songs after 6 minutes), the hybrid group (switch
songs every 2 minutes), or the serial group (switch songs every 1 minute), until all
songs were practiced a total of 6 minutes. Test recordings were made immediately
after practice concluded (acquisition) and 24 hours later (retention). Students also
completed an attitude questionnaire. Performances were scored on technical and
musical achievement. There were no signiﬁcant diﬀerences in technical accuracy,
musicality, or attitude among the groups at acquisition and retention. There was
a signiﬁcant interaction between practice condition and trial for musicality [F (2,15)
= 4.84, p<.05]. Though students demonstrated considerable technical accuracy,
averaging at the 95th percentile, they did not as often perform musically, with mean
scores at the 49th percentile.

Timing and Dynamics in Infant-Directed Singing
Takayuki Nakata 1 , Sandra E. Trehub 2 ; 1 Future University-Hakodate,
Japan; 2 University of Toronto at Mississauga, Canada
2AM2-R11-13
The temporal and dynamic features of mothers’ performances (n=10) of Twinkle,
Twinkle, Little Star for their infants (i.e., ID singing) were compared with perfor-
mances of non-mothers (n=10) singing the same song while alone (solo singing). ID
singing had a slower tempo and was more temporally more stable than solo singing.
ID singing also showed smaller timing deviations at phrase endings than did solo
singing. With respect to dynamics, ID singing was more expressive than solo singing,
as indicated by more gradual changes in dynamics at phrase endings. Also, the
correlation between pitch height and dynamics for the ﬁrst two notes explained 5.22
times more of the variance in ID singing than in solo singing. In sum, the ﬁndings
revealed systematic diﬀerences in the timing and dynamics of ID and non-ID versions
of the same song. The greater predictability and smoother transitions of maternal
performances may have important aﬀective and attention-regulating consequences
for infant listeners.


2AM2-R12 : Performance / Neuroscience
Rooms 12, 10:40 – 12:30 Tuesday 26 August 2008, Poster session

Eﬀect of Harmonic Distance on Performance Expression
Christopher Bartlette; Baylor University, USA
2AM2-R12-01
In two experiments, I investigate the eﬀect of harmonic distance — the extent
to which a harmony is “closely” or “distantly” related to a context — on volume
and timing levels for performances of 20 newly composed musical excerpts by 12
graduate piano students. Several earlier studies have suggested that performance
expression may be aﬀected by harmonic factors, but this issue has not been studied
in a controlled experiment with multiple participants. For the ﬁrst experiment, one
chord changes within each pair of excerpts: One chord is either “close” or “distant”
from its preceding chord, and all other chords are held the same. Distance between
chords is measured using a simple, key-neutral, two-dimensional model. For the
second experiment, each pair of excerpts begins with the same musical material;
at a point of modulation, one excerpt continues in a “close” key, while the other
excerpt continues with the same material transposed to a “distant” key. The two
experiments agree in their results: “Distant” chords are performed louder and
with delayed onsets. In addition, the results oﬀer insights into the eﬀect of meter,
key, texture, and “sequential” (chord-to-chord) versus “contextual” (multiple-chord)
measurements of harmonic distance on performance expression.

Pre-Symbolic Musical Expressiveness: A Case Study Related to the
Performer’s Expression in Singing
Jin Hyun Kim; University of Cologne, Germany
2AM2-R12-03


                                         64
ICMPC 10                                              Tuesday 26 August 2008

This study aims at giving evidence for musical expressive features as rule-based
pre-symbolic “signs”, focusing their symptomatic aspect related to the performer’s
emotional states.
Taking up the concept of micro-intonation, the author carries out FFT-based spectral
analyses of acoustic features of singing voices consisting of mechanisms such as
attack, vibrato, ampliﬁcation of singing formant, sforzando. Diﬀerent combinational
possibilities of some of these mechanisms are associated with a variety of emotional
expression guided by singing. The analyzed musical pieces include opera arias,
contemporary art songs, and Korean traditional songs. The FFT-based analyses
of single individual tones are used for observer training and veriﬁed by observer
agreement.
The result of this study shows that acoustic features of singing voices related to
diﬀerent categories of emotional expression are rule-conducted. This supports the
author’s thesis that the capacity of singing voices to additionally form a temporary
structure of individual sounds renders singing performance a means of mediating
emotions in an intended way: The aspect of micro-intonation allows a singer to gen-
erate expressive musical features not only immediately expressing her/his emotional
inner states, but rather intentionally controlled.
This study opens future research perspectives on expressive acoustic features in mu-
sic performance, which — albeit their symptomatic characters — act as pre-symbolic
“signs” that are based on rules applied within a cultural system.

An Investigation into the Relationship Between Student
Typologies and the Experience of Performance Anxiety in
Adolescent Musicians
Ioulia Papageorgi; IOE University of London, UK
2AM2-R12-05
Data presented in this paper form part of a study exploring the experience of per-
formance anxiety by adolescent musicians. The aim is to investigate the presence of
diﬀerent music student typologies and explore relationships between student type,
experiences of performance anxiety and examination achievement. Students aged
12–19 (N = 410) in two geographical locations (UK and Cyprus) responded to a new
self-report questionnaire dealing with a range of learning and performance issues.
Participants attended junior conservatoires and/or youth orchestras. K-Means cluster
analysis was used, grouping students into clusters based on similarity of responses
and allowing the establishment of student typologies. Further in-depth analysis
of student responses was conducted through thematic analysis of qualitative data
for representative cases from each of the student clusters. Results suggested three
typologies: ‘unmotivated students feeling ineﬀective but guarding self-esteem’, ‘stu-
dents susceptible to maladaptive performance anxiety’ and ‘conﬁdent students that
experience adaptive performance anxiety’. Chi-square tests revealed relationships
between typology and gender, age group, nationality and examination achievement.
The study suggests that diﬀerent types of students are aﬀected by anxiety in distinct
ways and evidence diﬀerent patterns of examination achievement. Students may be
predisposed to experiencing maladaptive or adaptive performance anxiety. Findings
have implications for education.

Kinematics and Muscular Activity of Upper Extremity Movements
in Piano Keystroke by Professional Pianists
Shinichi Furuya 1 , Tomoko Aoki 2 , Hidehiro Nakahara 3 , Hiroshi
Kinoshita 4 ; 1 Kwansei Gakuin University, Japan; 2 Prefectural
University of Kumamoto, Japan; 3 Morinomiya University of Medical
Sciences, Japan; 4 Osaka University, Japan
2AM2-R12-07
This study investigated the eﬀects of sound volume and striking tempo on control of
upper extremity movements in expert pianists (N=8) when they performed repetitive
octave keystrokes. It was found that at all levels of sound volume and striking
tempo, the upper limb angles at the moment of ﬁnger-key contact were invariant.
The eﬀects of sound volume and striking tempo on limb movements were revealed
diﬀerently. The proximal segments contributed more to the movement of the limb
for the production of larger sound, whereas they contributed less for increasing
striking tempo. We propose that sound volume control is achieved by an “impulse
strategy”, whereas striking tempo control is made by a “moment of inertia strategy”.


                                        65
Tuesday 26 August 2008                                                    ICMPC 10

To control sound volume and striking tempo simultaneously, pianists selected an
intermediate way of these two strategies where movements at the elbow joint played
a major role in keystroke.

The Diﬀerence in Neural Correlates of Singing a Familiar Song and
a Newly Learned Song: An fMRI Study
Shizuka Uetsuki 1 , Tatsuya Kakigi 1 , Hiroshi Kinoshita 2 , Kazumasa
Yokoyama 1 ; 1 Hyogo Prefectural Rehabilitation Center Nishi-Harima,
Japan; 2 Osaka University, Japan
2AM2-R12-09
The understanding of the neural network for singing familiar songs and its diﬀerence
from singing newly learned songs are utmost importance for the conduct of eﬀective
music and/or speech therapy. The present study used the fMRI to investigate
the brain correlates for singing a familiar song(FS), and those for singing a newly
learned song(NS). Fifteen normal subjects performed the experimental singing task,
sung the FS and NS songs covertly during fMRI scans. The FS was a well-known
nursery song while the NS was composed by one of the authors. Both songs had
the same musical components but had diﬀerent melodies and lyrics. The subjects
completely memorized two songs before the scan. Greater activation was found for
the left hemisphere at the superior temporal gyrus, angular gyrus and uncus for
the FS compared to the NS. The uncus is a part of the entorhinal cortex(EC) which
interconnects to the hippocampus with an important role in memory retention and
consolidation. It could be related to the retrieval process of the melodies and lyrics
from semantic memory. The NS had greater activation in the left IPL including
the precuneus, SMG, SFG, MFG, and the right IFG, posterior cingulate gyrus and
precuneus. The wider activation in the prefrontal area and precuneus with the NS
should reﬂect the diﬃculty of retrieving the song from episodic memory as well as
attention to perform the task accurately.

Music Playing Enhances Auditory Memory Trace: Evidence from
Event-Related Potentials
Keiko Kamiyama, Kentaro Katahira, Dilshat Abla, Kazuo Okanoya;
RIKEN Brain Science Institute, Japan
2AM2-R12-11
In this study, we examined the relation between practice and memory of sound
sequences, speciﬁcally the hypothesis that practice involving physical performance
enhances auditory memory. Musicians learned two unfamiliar sound sequences
with diﬀerent type of trainings. In the key press condition, they learned a melody
with key press while listening to the auditory input. In the no key press condition,
they listened to another melody without any key press. These two melodies were
presented alternatively, and the subjects were given each of these trainings. They
were instructed to pay suﬃcient attention in both conditions. Following the training
stage, they listened to these two melodies again without any key press and the
ERPs were recorded. During the ERPs recordings, 10% tones of these melodies were
deviated. The analysis showed that the amplitudes of mismatch negativity (MMN)
for deviant stimuli were larger in the key press condition than in no key press
condition. The result suggests that training with key press eﬃciently promoted
auditory memory.

Comparison Between Expert and None-Expert Pianists’ Cognitive
Processes in Piano Playing: Quantitative and Qualitative Studies
Michiko Ono 1 , Toshihiko Matsuka 1 , Masakazu Iwasaka 1 , Masaki
Hara 2 , You Nakayama 2 ; 1 Chiba University, Japan; 2 Yamaha Music
Foundation, Japan
2AM2-R12-13
Recent studies in music cognition have revealed that the way in which experts process
musical information while playing musical instruments is qualitatively diﬀerent from
that of none-experts. Oura and Hatano (2001), for example, showed that experts
tended to pay close attention to artistic expression while novices gave emphasis to
the technical aspects in piano playing. A majority of these comparative studies on
music cognition, however, has employed only qualitative methods (e.g. interview). In
order to facilitate better understanding of the nature of diﬀerential music cognition
by experts and none-experts, we replicated Oura and Hatano’s (2001) experiment


                                         66
ICMPC 10                                                  Tuesday 26 August 2008

using both qualitative and quantitative (i.e., neuroimaging) methods. In our study,
participants were asked to practice a previously unseen score for seven minute,
followed by a detailed interview. In addition, we also recorded neuro-physiological
measures using multichannel near infrared spectroscopy (NIRS) while the partic-
ipants were practicing the score. The marked diﬀerences were obtained in both
qualitative and quantitative data. The results of detailed interview were similar to
that of the precedent experiment (i.e., Oura & Hatano, 2001). The neuroimaging
data (i.e., multichannel NIRS) revealed that the experts and the none-experts indeed
processed music diﬀerently — the marked diﬀerences were observed in the left
hemisphere, the right lateral temporal and the right dorsal frontal regions.


2AM2-R13 : Neuroscience
Rooms 13, 10:40 – 12:30 Tuesday 26 August 2008, Poster session

Neural Mechanism of Melody Perception Revealed by Functional
Magnetic Resonance Imaging
Miho Yamauchi, Takuya Hayashi, Akihide Yamamoto, Hiroshi Sato,
Hidehiro Iida; National Cardiovascular Center Research Institute,
Japan
2AM2-R13-01
A neural process of melody perceptions has not been fully understood. A key
process of melody perception is that we recognize “melodic shape” (temporal change
of relative pitch) in a piece of music. To locate brain areas associated with melody
perception, we used a discrimination task of temporal changes of relative pitch in
functional magnetic resonance imaging (fMRI) study. Twelve non-musician subjects
received fMRI scan while performing a discrimination task of sound sequences: they
listened to a “target melody” followed by an “object melody”, and were asked to
answer whether the relative pitch of the object was same or not as the target. One
half of object melodies diﬀered in pitch of only 6th tone while a half of them altered
in pitch of all the tone from the target. The rate of correct answer signiﬁcantly
decreased when all tones of the object melody changed in pitch. Analysis of fMRI re-
vealed signiﬁcant task-related activations in inferior frontal gyrus, medial prefrontal
area, and right inferior parietal lobule. Particularly the inferior parietal lobule related
to a correct answer rate. Our ﬁndings suggest that the right inferior parietal lobule,
regarded as subserving visuo-spatial information, has a pivotal role in recognition of
melodic shape.

Investigating the Perception of Harmonic Triads: An fMRI Study
Takashi X. Fujisawa 1 , Norman D. Cook 2 ; 1 Kansei Gakuin University,
Japan; 2 Kansai University, Japan
2AM2-R13-03
We have undertaken an fMRI study of harmony perception in order to determine the
relationship between the diatonic triads of Western harmony and brain activation.
We have run an fMRI study on 12 right-handed, Japanese, male non-musicians in
order to determine the sites of brain activation in response to the common triads
of Western diatonic harmony. All stimuli consisted of 2 triads of 1.5 sec duration
each and presented as grand piano sounds. None of the chords contained intervals
of 1 or 2 semitones, but diﬀered in terms of their inherently resolved/unresolved
character (major, minor and tension [diminished and augmented] chords). Stimuli
were presented in blocks of 5 chord pairs per 30 seconds. Subjects were not aware
of the block design. Subtracting out the brain activation in response to a white noise
condition, the strongest response was found in right frontal and temporal regions.
The brain response to these three types of chords could be distinguished within the
right orbitofrontal cortex and cuneus/posterior cingulated gyrus in occipital lobe.

Investigation of the Musician’s Brain Activation During Diﬀerent
Music Listening Modes: A Near-Infrared Spectroscopy Study
Toshie Matsui 1 , Koji Kazai 2 , Minoru Tsuzaki 3 , Haruhiro Katayose 2 ;
1
  JST, Japan; 2 Kwansei Gakuin University, Japan; 3 Kyoto City
University of Arts, Japan
2AM2-R13-07
Some well-trained musicians empirically argue that they listen to the music in diverse


                                           67
Tuesday 26 August 2008                                                     ICMPC 10

modes. They may change their listening “mode” in accordance with situations;
trying to understand the musical structure, evaluating the level of performance
and so on. This study investigated the prefrontal cortex activation during listening
to music in diﬀerent modes by Near-Infrared Spectroscopy (NIRS). Two tasks and
two stimuli conditions were prepared. Two tasks were to detect a target (Detection
task) and to simply listen to the stimuli (Listening task). Two types of stimuli were
unknown piano pieces (Original condition) and scrambled fragments of Original
stimuli (Scrambled condition). In Detection task of Original condition, participants
were required to respond to their subjective phrase boundaries. In Detection task
of Scrambled condition, participants were asked to detect a noise burst presented
at the timing when the participants in the Detection task of Original condition had
marked. As a result, superior frontal cortex activation was signiﬁcantly decreased
for Original condition stimuli in Detection task, while it was signiﬁcantly increased in
the other conditions. The result suggests that speciﬁc cognitive processes included
in analyzing a musically structured stimulus aﬀect activities in the superior frontal
cortex.

Non-Right-Handedness as a Neurophysiological Selection Variable
in Musicians: The Inﬂuence of Early Beginning and Practice on the
Incidence of Handedness
Reinhard Kopiez 1 , Niels Galley 2 , Andreas C. Lehmann 3 , Marco
Lehmann 1 , Hauke Egermann 1 ; 1 Hannover University of Music and
Drama, Germany; 2 University of Cologne, Germany; 3 Hochschule für
Musik Würzburg, Germany
2AM2-R13-09
For musicians, practising plays an important role. However, the question remains
whether particular occurrences of handedness are the result of intensive bimanual
training or the neurophysiological and genetically determined prerequisite for
year-long successful practising (Kopiez, Galley & Lee 2006). The theoretical basis is
given by Annett’s (2002) “right-shift theory” which classiﬁes people as right-handers
(RH) and non-right-handers (NRH). We assume that bimanual training could result in
a higher proportion of NRH in bimanually performing musicians (pianists, violinists),
compared with the normal population. 128 music students (76 pianists, 47 string
players, and 5 various instrumentalists) participated in a performance test of hand-
edness (speed tapping), resulting in a lateralisation coeﬃcient (LC=100*[L-R/L+R]).
Based on the subjects’ self-declaration of handedness (11.7% NRH), the proportion of
RH and NRH was identiﬁed by binary logistic regression. We found a proportion of
30.8% of NRH in the group of musicians, while in the control group of non-musicians
(matched for age range), a proportion of 21.7% of NRH was observed. The correlation
between LC and the age at which pianists and string players began playing was found
to be r(128) = .06; (n.s.). We argue that this increased proportion is based on an early
onset of a selection eﬀect in the course of instrumental lessons. In other words,
pianists do not become ambidextrous, but ambidextrous people can become pianists.

Comparing Cortical Networks Underpinning Singing with Lyrics
and Propositional Language
Sarah Wilson 1 , David Abbott 2 , Anthony Waites 2 , Regula Briellmann 2 ,
Dean Lusher 1 , Gaby Pell 2 , Jenni Ogden 3 , Michael Saling 1 , Graeme
Jackson 2 ; 1 University of Melbourne, Australia; 2 Austin Health,
Australia; 3 University of Auckland, New Zealand
2AM2-R13-11
Research has suggested that singing with words may facilitate propositional language
in patients with aphasia, however mechanisms supporting this observation remain
poorly understood. We compared cortical activation associated with covert propo-
sitional language and singing with familiar lyrics in the same group of 26 healthy
individuals undergoing functional magnetic resonance imaging (fMRI). For each task,
the BOLD response compared to rest was modeled assuming the SPM canonical
haemodynamic response function and assessed with unpaired t-tests, using a cluster
threshold of p<0.05 corrected for multiple comparisons. To control for variable
singing ability, an out-of-scanner behavioural measure of pitch accuracy was included
in the model as a covariate of no interest. The Sing-Rest contrast revealed signiﬁcant
bilateral activation in the middle frontal gyri, maximal on the right at 50, 2, 44 and
58, 4, 28 (BA6) and on the left extending more posteriorly into the pre-central gyrus
at -46, -14, 32. The Speech-Rest contrast revealed that this left-sided activation


                                          68
ICMPC 10                                              Tuesday 26 August 2008

overlapped with a portion of the left middle frontal activation observed during
generative language. Our ﬁndings conﬁrm previous research indicating involvement
of the right frontal lobe during singing. They extend this research by suggesting
involvement of the propositional language network in singing with familiar lyrics.


2AM2-R16 : Demonstration I
Rooms 16, 10:40 – 12:30 Tuesday 26 August 2008,

Illusions Related to Auditory Grammar: Ten Demonstrations in
Musical Contexts
Yoshitaka Nakajima; Kyushu University, Japan
2AM2-R16-1
Auditory illusions that can be interpreted in the theoretical framework of auditory
grammar are demonstrated in contexts that are related to music. Auditory grammar
is a conceptual tool to construct auditory streams out of auditory subevents, i.e.,
onsets, oﬀsets, ﬁllings, and silences. Cues of subevents are detected independently
to be interpreted, or suppressed, by the auditory system. Interpreted subevents
should be concatenated grammatically to form auditory streams. A few Gestalt
principles work in this process. If subevents are reconstructed subjectively in such a
way to show an inconsistency with the physical structures of stimulus patterns, or if
some subevents are suppressed or restored perceptually, illusory phenomena should
appear. My colleagues and I explained, and sometimes predicted the existence of,
several auditory illusions in this framework. Ten auditory demonstrations made of
synthesized instrumental and vocal sounds are presented to indicate how the human
auditory system as described by auditory grammar can work in music. The illusions
to be presented are the gap transfer illusion, the illusory auditory completion, the
illusory split-oﬀ, subjective reconstruction of melody, and time-swelling. These
illusions appear clearly in the musical contexts, and often more clearly than when
simpler sounds are employed.


2AM2-R17 : Demonstration II
Rooms 17, 10:40 – 12:30 Tuesday 26 August 2008,

Collecting Continuous Data in Music and Listeners: PsySound3
and RTCRR, Two Free Resources
Densil Cabrera 1 , Emery Schubert 2 , Farhan Rizwi 2 , Sam Ferguson 1 ;
1
  University of Sydney, Australia; 2 University of New South Wales,
Australia
2AM2-R17-1
This demonstration describes two software resources for collecting data on musical
features of sound recordings and continuous human responses: PsySound3 and
RTCRR. PsySound3 is a soundﬁle analysis framework that enables analysis of various
general and psychoacoustical parameters of sound recordings including time-series
data. The general parameters relate to spectrum, autocorrelation, sound level,
and the like, and psychoacoustical parameters include loudness, sharpness, pitch,
roughness and binaural modelling. In several cases, more than one algorithm is
implemented for a type of output parameter, and new analysis algorithms are easily
introduced into the software framework. PsySound3 is Matlab-based. RTCRR (Real
Time Continuous Response Recorder — pronounced ‘Arty-car’) is a Macintosh based
tool that samples continuous responses of experimenter-selected response scales
(up to 2 recordable simultaneously) to multimedia stimuli. The time-based responses
are synchronised with the audio stimulus and can be exported for further analysis
at a sampling rate of up to 30 Hz. Examples of the use of each will be provided. The
resources are freely available from www.psysound.org and under software/facilities
at empa.arts.unsw.edu.au/em/. PsySound3 received ﬁnancial support from the
Australian Research Council grant LE0668448 and the University of New South Wales
via its Strategic Investment in Research Scheme. RTCRR received ﬁnancial support
from the Australian Research Council grant DP0452290.




                                         69
Tuesday 26 August 2008                                                    ICMPC 10


2PM1-R02 : Rhythm, Meter and Timing II
Room 2, 13:30 – 15:30 Tuesday 26 August 2008, Oral session

An Empirically Validated Model of Complexity: Longuet-Higgins
and Lee Reconsidered
Olivia Ladinig, Henkjan Honing; Universiteit van Amsterdam, The
Netherlands
2PM1-R02-1
This paper aims to validate and elaborate the Longuet-Higgins and Lee’s (1984)
model of syncopation (L-model for short) using previously published empirical data
(Ladinig & Honing, 2007) on complexity judgments. First, the L-model event salience
is combined with a sequential component, so as to better reﬂect the empirical results.
Second, formal musical training will be a parameter of the model accounting for the
diﬀerences found in the responses of non-musicians and musicians. The empirical
data suggest that not only do listeners use hierarchical templates to make sense of
a rhythm (as suggested by the L-model), but their judgments are also inﬂuenced by
the temporal extent and absolute position of the events. For all listeners, events at
the beginning of a bar in general receive higher salience ratings, presumably because
they help in establishing a mental framework (primacy eﬀect). For non-musicians, an
additional strong recency eﬀect is observed. We show that when models of rhythm
perception are extended with a sequential component based on heuristics, a better
ﬁt with the data is obtained. This shows particularly useful when considering on-line
perception of ecologically plausible patterns.

Musical Rhythm Parsing Using Mixture Probabilistic Context-Free
Grammar
Makoto Tanji, Daichi Ando, Hitoshi Iba; University of Tokyo, Japan
2PM1-R02-2
The metrical structure is important to analyze music pieces. And there has been
cognitive interest in it. Listener perceives the metrical structure relatively eas-
ily. However machine like approach has been diﬃcult. It implies that listener have
a model containing prior knowledge for music and listen music applying to the model.
In this paper, we attempt to model the metrical structure by mixture model of
Probabilistic Context-Free Grammar named Metrical PCFG Model. Because the
Metrical PCFG Model is probabilistic model, it allows us to estimate maximum
likelihood structure on the model based on Bayes’s Theorem. And the mixture model
distinguishes musical meters 4/4, 3/4 and 6/8.
The quantitative performance of metrical structure estimation and score quantization
problem are investigated. We found that our model needs relatively small amount of
training data compared to HMM. The best result of the metrical structure problem
was about 90% in F-Score. For score quantization problem, 98.8% notes are correctly
estimated from MIDI data. The result of simple threshold quantization method
works about 88%. We infer that our model works a part of human perception.

Role of Partner’s Feedback Information in Rhythm Production
Taiki Ogata, Takeshi Takenaka, Kanji Ueda; University of Tokyo,
Japan
2PM1-R02-3
Although rhythm production between two people might be inaccurate, we can
produce a rhythm more smoothly with a partner than with a metronome. This study
was intended to investigate the role of a partner’s feedback in rhythm production
using a continuous and alternating tapping task. The alternate tapping task was to
tap a pressure sensor using an index ﬁnger alternately with metronomes of two kinds
(constant inter-stimulus intervals (ATm) or constant time diﬀerence from response
to stimulus (ATf)) or with a partner (ATp) following the eight pacemaker stimuli. The
continuous tapping task (CT) was to keep twice the tempo of the pacemaker. Tapping
performance under the ATp condition was better than under the CT condition.
Furthermore, tapping performance under ATp condition was better than under ATm
and ATf conditions in means and standard deviations of time diﬀerences from
stimulus to response (ds). The results of time series analyses suggest that under ATp
condition, inter-tap intervals (ITIs) were corrected to adapt to the partner’s ITIs and
ds were corrected using long past ds of the participant’s own and of the partner in
mutually complementary manner.

                                         70
ICMPC 10                                              Tuesday 26 August 2008

Phase Correction in Sensorimotor Synchronization with
Non-Isochronous Rhythms
Bruno H. Repp 1 , Justin London 2 , Peter E. Keller 3 ; 1 Haskins
Laboratories, USA; 2 Carleton College, USA; 3 MPI CBS, Germany
2PM1-R02-4
Phase correction is required to maintain synchronization of a movement with a
rhythm. It has been studied primarily with isochronous sequences. We used a phase
perturbation method to examine how phase correction operates in cyclically repeated
non-isochronous rhythms containing two or three intervals whose long and short
durations form a 2:3 ratio. Musically trained participants tapped in synchrony with
computer-controlled sequences containing small local phase shifts. The immediate
reaction to a phase shift (the phase correction response, PCR, of the next tap,
expressed as a percentage of the phase shift in the rhythm) was the dependent
variable. In isochronous control sequences, we conﬁrmed previous ﬁndings that the
PCR is larger when tone inter-onset intervals are long (600 ms) rather than short
(400 ms). In non-isochronous two-interval rhythms containing alternating intervals
of these same durations, we found a similar dependence of the PCR on the duration
of the interval following a phase shift. In three-interval rhythms, however, where
either the short or the long interval occurred twice, there was no clear dependence
on interval duration. In general, phase correction in non-isochronous rhythms was
as eﬀective as in isochronous rhythms. The metrical interpretation of the rhythms
(i.e., where the downbeat was located) had no eﬀect on either rhythm production or
phase correction.


2PM1-R03 : Music Listening II
Room 3, 13:30 – 15:30 Tuesday 26 August 2008, Oral session

How Music Touches: The Eﬀects of Pitch, Loudness, Timbre and
Vibrato on Listeners’ Audiotactile Metaphorical Mappings
Inbar Rothschild, Zohar Eitan; Tel Aviv University, Israel
2PM1-R03-1
Relationships of touch and sound are central to music performance, and audiotactile
metaphors are pertinent to musical discourse. Yet, few empirical studies have
investigated systematically how musical parameters such as pitch, loudness, and
timbre and their interactions aﬀect auditory-tactile metaphorical mappings. In
this study, 40 participants (20 musically trained) rated the appropriateness of six
dichotomous tactile metaphors (sharp-blunt, smooth-rough, soft-hard, light-heavy,
warm-cold and wet-dry) to 20 sounds, varying in pitch, loudness, instrumental timbre
(violin vs. ﬂute) and vibrato. Results (repeated measures ANOVAs) suggest that
tactile metaphors are strongly associated with all musical variables examined. For
instance, higher pitches were rated as signiﬁcantly sharper, rougher, harder, colder,
and lighter than lower pitches. We consider two complementary accounts for the
ﬁndings: psychophysical analogies of tactile and auditory sensory processing, and
experiential analogies, based on correlations between tactile and auditory qualities
of sound sources in daily experience.

The Eﬀect of Music Listening on Spatial Skills: The Role of
Processing Time
Doris Grillitsch, Richard Parncutt; University of Graz, Austria
2PM1-R03-2
Previous research on the eﬀect of music listening on spatial skills has not carefully
considered the time taken by participants to complete spatial tasks. We randomly
selected 4 groups of 10 participants. Each group listened to 3 minutes of music (or
silence) then worked on a cube rotation task (A3DW). Group 1 listened to happy,
fast music by Mozart, Group 2 to a favorite CD track, Group 3 to sad, slow music by
Mozart and Group 4 sat in silence. The groups did not diﬀer in spatial skills, but
there was a large diﬀerence in working time: the mean time spent on each rotation
task was 47s, 103s, 90s and 73s for Groups 1–4 respectively. We speculate on
possible reasons why listening to a favorite piece of music might cause participants
to spend more time on a subsequent spatial task. The results imply that music can
indirectly promote the acquisition of non-musical skills, since children who spend
more accumulated time on any task will become more skilful in that task.



                                        71
Tuesday 26 August 2008                                                   ICMPC 10

Continuous Measurement of Musical Impression by the Color
Image
Hiroshi Kawakami; Nihon University, Japan
2PM1-R03-3
Continuous change of the impression while listening to music was observed by
choosing the color. The possibility of observing continuous impression change was
veriﬁed using the color selection. Subjects were 17 volunteers and they clicked
the start button for playing music and selected the color, which they felt, on RGB
gradation. The value of each RGB parameters was recorded. Music was “Ah, vous
dirai-je Maman” by Mozart. Although how to choose a color for every subject was
diﬀerent, the similarity of change in 12 variations was seen. Warm colors were
selected in the dynamic parts, such as pink or orange, and contrast was deep. In the
8th variation that changes to minor, dark blue and purple were chosen. In the 7th
variation that modal interchanges on the parallel minor key were used frequently,
dark colors, such as purplish red, were also chosen. Moreover, in the 9th variation
that changed from the 8th minor to major, the yellowish green or orange were
chosen. Even if the color chosen by every subject was diﬀerent, it was possible to
record the impression change continuously by recording color and that the selected
color was related to the impression of a musical piece.

Aesthetic Reactions to Music in Elementary School Children:
Revisiting the Open-Earedness Hypothesis
Marco Lehmann, Reinhard Kopiez; Hannover University of Music and
Drama, Germany
2PM1-R03-4
The open-earedness hypothesis (Hargreaves, 1982) states that younger children
are more tolerant towards music that is regarded by adults as unconventional. To
understand the transition from more to less open-earedness, we had to examine
the precise interaction between the factors age and type of music. In line with
Hargreaves, a decline in open-earedness should not only result in less preference for
unconventional music in older children, but also in a stabilized level of preference
for conventional music. Based on the statement that pop music is a “badge of
identity” for adolescents (Hargreaves, Marshall, & North, 2003), we classiﬁed pop
music as conventional and examples of classical, avant-garde and ethnic music as
unconventional music. Elementary school children in the four grades (N=186) rated
their preferences for eight short musical examples taken from these genres. We
found a decline in open-earedness between grades one and two, in contrast to earlier
results by Schellberg & Gembris (2003). However, the decline in open-earedness was
due to a decreasing preference for classical music in older participants. Thus, while
children prefer pop music and devalue classical music within their acquired stylistic
sensitivity in western tonality (Hargreaves, North & Tarrant, 2006), with increasing
age, they possibly maintain open-earedness in regard to music which does not belong
to this idiom.


2PM1-R04 : Emotion in Music II
Room 4, 13:30 – 15:30 Tuesday 26 August 2008, Oral session

Cross-Cultural Investigation of Adolescents’ Use of Music for
Mood Regulation
Suvi Saarikallio; University of Jyväskylä, Finland
2PM1-R04-1
The current study was a cross-cultural exploration of diﬀerences and similarities
in how adolescents regulate their mood through music in Finland and in Kenya.
As a whole, the study was a qualitative exploration of cultural characteristics, but
it included also quantitative analysis of a pilot survey. The results demonstrated
the many-sidedness of music as a means of mood regulation, speciﬁed previous
understanding of the mood-regulatory mechanisms, established similarity between
the cultural groups in the basic nature of mood regulation, and revealed some
cross-cultural diﬀerences in how the regulation was realized and what aspects were
emphasized. The study increased knowledge of the role of cultural background in
the emotional experience of music.




                                        72
ICMPC 10                                               Tuesday 26 August 2008

Strong Emotional Experiences in Choir Singing — A Cross-Cultural
Approach
Jukka Louhivuori; University of Jyväskylä, Finland
2PM1-R04-2
The main reason for people to sing in a choir is according to previous studies music’s
social and emotional dimensions. In previous studies choir singers have reported
strong emotional experiences (SEM). Although SEM has been widely studied, the role
of cultural background in these experiences is not well understood. The study aims
to compare the quality and quantity of strong experiences of music experienced by
the singers with results reported in previous studies about SEM. It is argued that the
basic emotions are very similar in diﬀerent cultures because of the long evolutionary
history of emotions. The cultural diﬀerences are believed to appear more in the
realization of emotions in behavior (phenotype). The research material consists of
quantitative (N=713) and qualitative data (40 interviews), which has been collected
from choir singers in six countries (Finland, Estonia, Belgium, Rumania, South Africa,
Kenya). The main cultural diﬀerences concerned the role of social situation and
strength of emotions. Cultural background did not show a prominent role in the
frequency or quality of emotional experiences. Cultural background had an inﬂuence
on how emotions are expressed in musical performance and on speech about music.
Understanding underlying cognitive, emotional, social and cultural process behind
musical experiences asks for a global perspective. Deeper understanding of cultural
diﬀerences in musical experiences increases multicultural societies’ abilities to
manage complex socio-cultural processes.

Orientation Eﬀect in Continuous Emotional Response Tasks
Emery Schubert; University of New South Wales, Australia
2PM1-R04-3
This paper discusses identiﬁcation of initial-orientation-time based on the second-
order standard deviation (SD2) method, a simple time series technique that identiﬁes
signiﬁcant activity in multiple response time-series data. For each emotion dimension
of each piece investigated, the mean of the SD series, and the SD2 were calculated,
ignoring the ﬁrst 20 seconds. A one SD2 conﬁdence range was produced. Initial
orientation was then deﬁned as the time taken for the SD series to ﬁrst enter this
range. For the 2 dimensions by four pieces, the median orientation time was 7.5
seconds. Initial orientation time varies according to several factors. The ambiguity
of the emotional message and the tempo of the piece are two examples. Orientation
time can also be hidden in a ‘true’ response which happens to be close to the initial
response position. This raises the question of whether the start of a continuous
response task should be made at a neutral position (such as middle or zero points of
valence and arousal scales) or at random. The former falsely deﬂates the spread of
scores. However, there are pragmatic concerns about starting a continuous response
task in random positions. It therefore seems likely that there will be a typical lower
limit of initial orientation time where SD is initially less than the mean second order
SD, and then overshoots, and then settles into the ‘reliable’ conﬁdence region.

Evaluating Structure and Performance: Relationships Between
Judgments of Tension, Emotion, Expression, and Interest in
Diﬀerent Musical Performances
Richard D. Ashley; Northwestern University, USA
2PM1-R04-4
Numerous studies have explored the relationship between musical structure, expres-
sive performance, and listener responses; often continuous ratings are obtained for
emotionality (valence, intensity, or both), tension, expressiveness, and familiarity
of thematic materials. The relationship between these diﬀerent scales, and their
connection to speciﬁcs in musical structure and expressive performance, has been
little explored. This study aimed at more systematic exploration of the relationships
between continuous judgments of musical tension, emotion, expressiveness, and
interest; Two compositions were used: Mozart Piano Sonata K. 282, I (development
and recapitulation) and Scriabin Prelude Op. 11/4, in both expressive and mechanical
versions. Participants listened to these performances and gave continuous responses
to musical tension, emotionality, expressiveness, and interest. In addition, they
marked moments in the performances which were particularly expressive or interest-
ing and gave summary post-listening evaluations of the emotionality, expressiveness,
aesthetic quality, and interest of the performance. Results to date indicate that there
are both signiﬁcant correlations and signiﬁcant diﬀerences between the categories


                                         73
Tuesday 26 August 2008                                                    ICMPC 10

of continuous ratings, showing that these variables are tapping into diﬀerent aspects
of musical structure and performance. Individual musical moments marked as
expressive or interesting contribute in diﬀerent and complicated ways to these
judgments and have a strong eﬀect on not only momentary but overall responses.


2PM1-R05 : Education II
Room 5, 13:30 – 15:30 Tuesday 26 August 2008, Oral session

The Social Representations of Music, Musicality, Music Child and
General Teachers
Anna Rita Addessi, Felice Caurgati; University of Bologna, Italy
2PM1-R05-1
This paper deals with a research project currently being undertaken at University
of Bologna about the training of the university students studying to become music
teachers. The general hypothesis of the project is that “musical knowledge” (Olsson
1997, 2002), can be investigated as a social and psychological construction as
described by the theory of Social Representations (Moscovici 1981; Mugny-Carugati
1989), as well as social music values (Baroni 1993, Bourdieu 1983) aﬀecting music
education and teaching practice.

Mobile Music for Children — Experiences of MobiKid
Maija Fredrikson 1 , Pirkko A. Paananen 2 ; 1 University of Oulu, Finland;
2
  University of Jyväskylä, Finland
2PM1-R05-2
In the present study, mobile software applications for young children were devel-
oped and tested. The pedagogical design is based on IP-/Neo-Piagetian theory of
development. The research perspectives were focused on child-centered usability
and software development. MobiKid software was developed to allow children to
sing, record and listen to the songs, and forward the recorded songs to the server
independently at home. The song repertoire in the software was learned in advance
in music plays school; a group of nine girls worked on their personal mobile phones.
The music play school teacher worked also as the mobile teacher in the hardware.
Outside the music play school context, one boy used the device independently at
home. (UI) and the software design were investigated by parent questionnaires
and video-observations. Children were able to work independently on the device,
and the structure of decision-making was suitable for the users. However, some
UI-features were uncompleted. Children were highly motivated in singing, learning
and decision-making. MobiKid provided a natural, motivating context for musical
creativity and social sharing as well as a tool for research in the development of
singing.

Diﬀerences in Conceptions of Musical Ability
Susan Hallam; IOE University of London, UK
2PM1-R05-3
Historically, musical ability was largely conceptualised in relation to basic aural
skills. Recently, broader conceptions have identiﬁed a range of skills required for
reaching high levels of musical expertise. This study aims to explore the diﬀerent
conceptualisations of musical ability held by musicians, educators, other adults, and
children. An inventory with statements derived from previous qualitative research
exploring conceptions of music ability was used to assess the conceptions of musical
ability held by 650 individuals aged 14 to 90 including musicians, educators, adults
who were actively engaged in music making in an amateur capacity, adults were
not actively engaged in making music, children actively engaged in making music
in addition to their engagement with the school curriculum and children with no
engagement with music outside of the school curriculum. Factor analysis of the
responses revealed 6 factors related to: the skills required for playing an instrument
or singing; musical communication; listening and appreciating music; composition
and improvisation; personal commitment and motivation; and aural and analytic
skills. There were statistically signiﬁcant diﬀerences between each of the respondent
groups in relation to each of the factors. The ﬁndings suggest that the way that
musical ability is conceptualized by any individual depends on the nature of his or
her engagement with music.



                                         74
ICMPC 10                                               Tuesday 26 August 2008

The Impact of Formal and Informal Learning on Students’
Compositional Processes
Sylvana Augustyniak; University of New South Wales, Australia
2PM1-R05-4
Many students spend hours a day listening to music; from the moment students wake
up in the morning to the time they go to bed, students engage in listening to music.
The accessibility of various music technologies such as iPod, iTunes, compositional
software and the like have helped to increase the rate of memorized learning by
students.
Few studies have acknowledged the impact that these out-side inﬂuences have in the
classroom. One such inﬂuence is the impact of technology on students’ absorption
of conscious and unconscious knowledge regardless of whether students are trained
or untrained musically.
The two diﬀerent types of listening students engage in have been operationally
deﬁned in my PhD thesis as either conscious listening (purposeful listening/ explicit
learning) or unconscious listening (background listening/ implicit learning).


2PM2-R02 : Rhythm, Meter and Timing III
Room 2, 15:45 – 17:45 Tuesday 26 August 2008, Oral session

Selective Rhythmic Impairments in Music
William F. Thompson 1 , Linda Sigmundsdottir 1 , John R. Iversen 2 ,
Aniruddh D. Patel 2 ; 1 Macquarie University, Australia; 2 The
Neurosciences Institute, USA
2PM2-R02-1
Research has addressed tone deafness or “congenital amusia” (CA), but most of
this research concerns pitch-related deﬁcits. Rhythmic deﬁcits sometimes co-occur
in CA but may be secondary to pitch problems. The aim of this research was to
determine if there are individuals with normal musical pitch perception but impaired
musical rhythm perception. Another aim was to evaluate two novel tests of rhythm
perception. Participants were administered the MBEA (which includes 3 subtests
of pitch perception and 2 of rhythm perception) and two new rhythm tests: the
Monotonic Rhythm Test (MRT) and the Beat Alignment Test (BAT). The MRT involves
same/diﬀerent discrimination of rhythmic patterns produced on a single pitch. The
BAT involves listening to musical excerpts and judging whether a superimposed train
of beeps is “on the beat” or not. Five participants performed in the normal range on
pitch-related subtests of the MBEA but showed impairments on one or both rhythm
subtests. Individuals with impaired rhythm also showed reduced performance on the
BAT and MRT tests, suggesting a selective rhythmic impairment. However, variability
on these latter two tests was large, pointing to the need for larger samples in order
to determine if the tests can help diagnose rhythm impairments.

Does Amusic Mean Unmusical?
Jessica Phillips-Silver 1 , Isabelle Peretz 1 , Nathalie Gosselin 1 , Petri
Toiviainen 2 , Olivier Piché 1 ; 1 Université de Montréal, Canada;
2
  University of Jyväskylä, Finland
2PM2-R02-2
The severe pitch deﬁcit in congenital amusia can interfere with the ability to perceive
and produce a rhythmic beat in music. However, the extent of rhythm abilities in
the context of music without melodic pitch information is unknown. We tested the
rhythm abilities of a classic amusic subject, in the context of real music devoid of
melodic pitch : percussion music. Using perception and production of body move-
ment as an index of rhythm processing, we demonstrate that an amusic subject can
judge and synchronize movement to percussion music as well as to a metronome. We
suggest that in congenital amusia, performance on melodic music is not equivalent
to performance on percussion music. An amusic’s rhythmic ability is not limited to
the metronome : by removing the primary source of their musical deﬁcit, pitch, and
by using simple body movement, an amusic subject can be musical.




                                         75
Tuesday 26 August 2008                                                    ICMPC 10

Eﬀects of Marker Durations on the Perception of Inter-Onset Time
Intervals
Emi Hasuo, Yoshitaka Nakajima; Kyushu University, Japan
2PM2-R02-3
When people perceive rhythm in music, they are listening to the intervals between the
onsets of successive sounds. This study focuses on how the temporal length between
the onsets of two sounds is perceived. We varied the duration of the sound markers
systematically and conducted two experiments to examine how the durations of
the markers aﬀect the perception of time intervals. The results of the experiments
can be summarized as follows: 1) lengthening both the ﬁrst and the second marker
caused the inter-onset time interval to be judged longer, 2) lengthening only the ﬁrst
marker in some cases caused the interval to be judged longer, although not clearly,
and 3) lengthening only the second marker caused the interval to be judged longer in
a stable manner. These tendencies appeared both when the amplitude of the markers
of diﬀerent duration was made constant, and when the total energy of the markers
of diﬀerent duration was made constant. It was shown that the perceived length of
an inter-onset time interval increases as the duration of the markers lengthen, and
that the eﬀect of the second marker is more stable and more systematic.

A Multiresolution Model of Rhythmic Expectancy
Leigh M. Smith, Henkjan Honing; Universiteit van Amsterdam, The
Netherlands
2PM2-R02-4
We describe a computational model of rhythmic cognition that predicts expected on-
set times. A dynamic representation of musical rhythm, the multiresolution analysis
using the continuous wavelet transform is used. This representation decomposes
the temporal structure of a musical rhythm into time varying frequency components
in the rhythmic frequency range (sample rate of 200Hz). Both expressive timing and
temporal structure (score times) contribute in an integrated fashion to determine
the temporal expectancies. Future expected times are computed using peaks in
the accumulation of time-frequency ridges. This accumulation at the edge of the
analysed time window forms a dynamic expectancy. We evaluate this model using
data sets of expressively timed (or performed) and generated musical rhythms, by
its ability to produce expectancy proﬁles which correspond to metrical proﬁles. The
results show that rhythms of two diﬀerent meters are able to be distinguished. Such
a representation indicates that a bottom-up, data-oriented process (or a non-cognitive
model) is able to reveal durations which match metrical structure from realistic
musical examples. This then helps to clarify the role of schematic expectancy
(top-down) and it’s contribution to the formation of musical expectation.


2PM2-R03 : Performance III
Room 3, 15:45 – 17:45 Tuesday 26 August 2008, Oral session

Inﬂuences of Movement and Grip on Perceptual and Measured
Tone Quality in Drumming
Soﬁa Dahl, Michael Grossbach, Eckart Altenmüller; Hannover
University of Music and Drama, Germany
2PM2-R03-1
In drumming, the interaction between drumstick and drumhead is determining the
quality of the tone. It is believed that a richer spectrum is developed when the
stick freely strikes and bounces up from the drumhead, as compared to strokes
when the rebound of the stick is hindered. However, on occasion the rebound is too
strong for the next stroke and needs to be controlled, for instance by a muscular
hypertension of the grip. In order to investigate the movement-to-sound interaction
during drumming, audio, contact interaction between drumhead and drumstick,
and muscle activity in grip (EMG) were recorded. Furthermore, the movement of
drumstick and player were analyzed using a motion capture system.
Professional players were asked to play two types of mezzoforte strokes on a drum:
“normal” and “controlled”. “Normal” strokes were allowed to freely rebound from
the drumhead. For “controlled” strokes the player was asked to control the ending
position of the drumstick, stopping it as close as possible to the drumhead directly
after the stroke.



                                         76
ICMPC 10                                              Tuesday 26 August 2008

Results show that the two playing instructions inﬂuenced the interaction between
drumstick and drumhead. Compared to normal strokes, the controlled strokes
yielded a shorter contact duration and higher peak force. Listeners rated the normal
strokes as having a fuller timbre and harder attack.

Continuous Self-Report of Engagement to Live Solo Marimba
Performance
Mary Broughton 1 , Catherine Stevens 1 , Emery Schubert 2 ; 1 University
of Western Sydney, Australia; 2 University of New South Wales,
Australia
2PM2-R03-2
Laboratory-controlled experiments have demonstrated that expressive bodily move-
ment (or lack thereof) can contribute positively (or negatively) to assessments of
marimba performance. The experiment reported here investigates audience continu-
ous self-report engagement responses gathered via the portable Audience Response
Facility (pARF). The stimulus material was a solo marimba piece performed in a
live concert. A female musician performed two musically similar sections within
the piece in two diﬀerent performance manners (deadpan and projected). The
second-order standard deviation threshold method analysed signal reliability. As
hypothesised, mean engagement responses were greater in the projected sample
than the deadpan sample. Reliable signal was only observed in the projected sample.
Diﬀerence between deadpan and projected sample mean engagement responses may
be due to expressive bodily movement from the performance manner manipulation;
alternatively, an order eﬀect may be responsible. Experimentation in ecologically
valid settings enables understanding of audience perception of music performance
as it unfolds in time.

An Eﬀective Singing for Musical Expressions
Kiyomi Toyoda 1 , Tsutomu Fujinami 2 ; 1 Tokyo Nikikai Opera
Foundation, Japan; 2 JAIST, Japan
2PM2-R03-3
It is widely accepted among singers and voice trainers that singing in the same
manner for opening throat like yawn may result in increasing the resonance and
thus allow singers to express the content of music in detail. We carried out an
experiment to investigate whether the practice is eﬀective in improving vocal skills.
We employed a female singer, whose repertoire is mostly consisted of classical
music, and asked her to adopt the particular way of singing. We measured its eﬀect
in two aspects:1) Objectively, we measured the vibration of her body while she was
singing using a laser interferometer, an innovative optical sensing technology, and
2) Inter-subjectively, we measured how listeners perceived the content of music
composition with a questionnaire. As the result, we found the technique to be
eﬀective in increasing the resonance of the voice by opening the cavity of larynx and
pharynx. The FFT analysis of the vibration revealed that the spectrum values around
high tones(750–800Hz) were distinctively high when she opened the cavity of larynx
and pharynx, compared to the ones when she did not adopt such a technique. We
examined how thirty listeners perceived the content of a song by adopting the Music
appreciation evaluation experiment (Mood Adjective Check List) to observe its eﬀects
as we expected. We conclude that the singing, in which the throat opens like yawn,
resulted in increasing the resonance of voice. We believe that our ﬁnding will help
singers to improve his or her vocal-skill in musically meaningful ways.

Hype vs. Natural Tempo: A Long-Term Study of Dance Music
Tempi
Dirk Moelants; Ghent University, Belgium
2PM2-R03-4
Musical tempi found in dance music provide us with information on the relationship
between music and movement. Large databases of (dance) music tempi are available
as tools for DJs. In this paper, one speciﬁc database, the ‘Scandinavian Dance Charts’
is the main object of study. The database contains the tempi of the Top 40 in the
dance charts which is updated weekly since 1998. Analysis of the tempo distributions
thus provides us with details on the evolution in tempo over a period of 10 years. The
long-term study of dance music tempi may allow us to distinguish between fashion
trends in dance music (and dance movement) and the most preferred, natural tempo.
It shows that he main peak stays very consistent at 128 bpm, with a small range


                                         77
Tuesday 26 August 2008                                                    ICMPC 10

around it (125–130) representing about half of the total entries. Looking for variation
in the data throughout time has allowed us to identify the styles behind diﬀerent
tempo zones and to follow their evolution throughout time. Dance music found
outside the preferred range can be associated with diﬀerent types of movement
related to diﬀerent resonance phenomena.


2PM2-R04 : Education III
Room 4, 15:45 – 17:45 Tuesday 26 August 2008, Oral session

Exploring Children’s Understanding of Music Through the Use of
Drawings and Interviews
Tiija Rinta, Susan Hallam; IOE University of London, UK
2PM2-R04-1
Research on children’s understanding of music has generally considered children’s
improvisations, compositions, notations, or their understandings of the emotions
portrayed in music. The aim of this study was to investigate children’s overall
understanding of music through drawings and their explanations of those drawings.
18 10-year-olds participated in the study listening to music from three diﬀerent
genres, classical/ ﬁlm, jazz, and popular music. After listening to each piece the
children drew a picture reﬂecting how they understood the music and were then
interviewed and asked to explain the relationship between the picture and the music.
The drawings were analysed using content analysis, and consideration of colour
and size. The interviews were subject to thematic analysis. The themes emerging
related to musical elements, dynamics, images associated with the music, aﬀective
responses, events associated with the music, adopting a holist listening style, musical
structure, lyrics, familiarity with the musical style, and musical instruments. There
were diﬀerences in response relating to the genre of the music. The combination of
drawings and interviews was found to be a useful means for investigating children’s
musical understanding illustrating the range of diﬀerent types of understandings
which children may have in response to diﬀerent types of music.

Deﬁning Relationships Between Motivational Beliefs and
Self-Regulated Practising Behaviours Using a Structural Equation
Model
James M. Renwick 1 , Gary E. McPherson 2 , John McCormick 3 ;
1
  University of Sydney, Australia; 2 University of Illinois at
Urbana-Champaign, USA; 3 University of Wollongong, Australia
2PM2-R04-2
The amount of time musicians spend practising and their use of adaptive learning
strategies are crucial to skill development. Because practice is self-directed, mo-
tivational eﬀects are especially salient. This study of young people preparing for
a music examination investigated associations between practising behaviour and
motivational constructs derived from self-determination theory, which interprets
motivation as lying along a continuum of perceived autonomy. Relations between ﬁve
regulatory styles on this intrinsic–extrinsic continuum and three types of practising
behaviour were investigated through structural equation modelling. Factor analysis
revealed ﬁve motivational dimensions with partial correspondence to research in
academic and sport domains: internal, external, social, shame-related, and exam-
related motives. Three behavioural factors consistent with self-regulated learning
theory emerged: eﬀort management, monitoring, and strategy use. Structural
equation modelling showed a clearer picture than emerged from previous multiple
regression analyses: internal motivation accounted best for variance in each type
of practising behaviour, with only a small added eﬀect from competence beliefs
and exam-related motivation. Thus, for young musicians engaged in a demanding
assessment task, intrinsic motivation was not statistically distinguishable from
internalised extrinsic motivation, despite the distinction drawn in self-determination
theory. Self-regulated practising behaviour was explained more powerfully by this
unitary internal motivation construct than by perceived competence and other less
internalised forms of extrinsic motivation.




                                         78
ICMPC 10                                               Tuesday 26 August 2008

Developing a Music Aptitude Test for Schoolchildren in Asia
Yoko Ogawa 1 , Tadahiro Murao 2 , Esther Ho Shun Mang 3 ; 1 Tottori
University, Japan; 2 Aichi University of Education, Japan; 3 Hong Kong
Baptist University, China
2PM2-R04-3
This paper addressed a New Music Aptitude Test (NMAT) for Asian school children,
and the relationship between NMAT scores and children’s musical achievement as
rated by their teachers. Participants in the study were 135 male and 149 female
aged 8 to 13 years (3rd , 4th , 5th and 6th grades). In the ﬁrst part of the study, the
children were asked to respond to each item on paper while listening to audio stimuli
of NMAT, which consists of 5 categories: timbre, time, pitch, loudness and tonal
imagery-melody. In the second part of the study, music teachers also required to
rate whole children’s musical achievement using traditional musical tests. Children
performed best in the timbre and loudness categories while, weaker in the pitch and
tonal imagery-melody categories. Younger children were particularly weak in the
tonal imagery-melody category. Teachers’ assigned achievement scores did not have
strong correlations with NMAT scores. However, Pearson correlations (p<.001) were
insigniﬁcant among the younger children but signiﬁcant among the older children.
Therefore, the results of the NMAT were similar between those found in teachers’
evaluation in older children. This ﬁnding implies that NMAT could be a reliable
predictor of children’s musical aptitude.

The Professional Relevance of Music Psychology: An Internet
Survey
Richard Parncutt 1 , Nicola Dibben 2 , Margit Painsi 1 , Manuela Marin 1 ;
1
  University of Graz, Austria; 2 University of Sheﬃeld, UK
2PM2-R04-4
What careers are pursued by music psychology students? How professionally
useful are music psychology courses? Some 150 ex-students were recruited through
teacher/researchers and email lists. They completed an internet questionnaire
“Careers After Music Psychology”. Questions addressed the educational experience
and current profession of participants, and the usefulness and professional rele-
vance of music psychology courses. Most respondents were female and lived in
industrialized English-speaking countries. Their ages covered a wide range. Most
had successfully completed one or more unit/s or program/s in music psychology
at bachelors or masters level, as part of several (typically 5) years of post-secondary
study, mostly in the past 10–20 years. Respondents were mostly working full-time
in music therapy, performance, education, technology or research. Most had played
an instrument (often piano or voice) for many years (typically 25). Most rated music
psychology studies as “somewhat” or “quite” useful for later work but would like to
have seen more practical application (applied settings, experience/demonstrations,
industry/practitioner contact), career-relevant content (music therapy, neuroscience,
self-healing, performance, teaching), research methods/experience, new research,
intercultural topics, interdisciplinary content/skills, or professional communication
skills. Some wanted less (psycho-) acoustics and more historical/cultural context.
Our ﬁndings can help instructors orient course content towards careers, motivate
the study of music psychology, and promote music psychology among employers.


2PM2-R05 : Computational Models and Analyses II
Room 5, 15:45 – 17:45 Tuesday 26 August 2008, Oral session

Analysis and Automatic Detection of Breath Sounds in
Unaccompanied Singing Voice
Tomoyasu Nakano 1 , Jun Ogata 2 , Masataka Goto 2 , Yuzuru Hiraga 1 ;
1
  University of Tsukuba, Japan; 2 AIST, Japan
2PM2-R05-1
This paper presents a dual approach to the study of breath sounds in singing, con-
sisting of an acoustic analysis of breath sounds, and development of an automatic
breath detection system. Previous work on automatic breath detection were based
on relatively simple features that were postulated to be relevant to the detection. In
contrast, this study starts with a detailed acoustic analysis of breath sounds, with
the aim to explore novel characteristics. The obtained results can be used to enhance



                                         79
Tuesday 26 August 2008                                                      ICMPC 10

the capability of automatic breath detection.
The acoustic analysis used singing voice recordings of 18 singers with a total length
of 128 mins (1488 breath events). The results of the analysis show that the spectral
envelope of breath sounds remain similar within the same song, and their long-term
average spectra have a notable spectral peak at about 1.6kHz for male singers and
1.7kHz for female singers. A prototype version of a breath detection system was im-
plemented, using HMM based on MFCC, ΔMFCC, and Δpower as acoustic features. In
an evaluation experiment with 27 unaccompanied song samples, the system achieved
an overall recall/precision rate of 97.5%/77.7% for breath sound detection.

Composition Model of Modal Melody Based on the “Core Note”
Concept
Yuriko Hoteida, Yuichi Aizawa, Takeshi Takenaka, Kanji Ueda;
University of Tokyo, Japan
2PM2-R05-2
This study proposes a modal melody composition model based on the “Core Note”
concept. We deﬁne the Core Note as a hypothetical note sounding with the melody.
We then use it as a reference note to calculate sensory dissonance. Sensory dissonance
is used to evaluate composed melodies. First, we speciﬁcally examined Japanese chil-
dren’s folk songs. The proposed model uses reinforcement learning for decision-
making. The model includes an agent that perceives the preceding four notes, decides
the next note’s pitch and duration, and learns to compose better melodies based on
sensory dissonance and proximity evaluation. We introduce simulations of melody
composition and psychological experiments using the Semantic Diﬀerential method
to check impressions of composed melodies. For the simulations, we set conditions
using ﬁve diﬀerent Core Notes and a condition without setting Core Notes. Results of
the simulations reveal that Core Notes determine which notes are likely to be chosen
in the composed melodies. Results of psychological experiments show that impres-
sions of the composed melodies depend on the Core Notes: Core Notes such as E
and G make melodies brighter, whereas D and A make them darker. Results further
show that, given no Core Notes, the agent is forced to earn a reward from proximity
evaluation; consequently, the generated melodies become more stable.

A Spectral Timing Mechanism pour L’ART
Michael Connolly Brady; Indiana University, USA
2PM2-R05-3
In the late 1980s and early 1990s, Robert Gjerdingen presented a model called L’ART
pour l’art. It is an artiﬁcial neural network based on Adaptive Resonance Theory (ART)
that forms proactive memories for early Mozart melodies. Gjerdingen’s model pro-
vides an eloquent introduction to Adaptive Resonance Theory in terms of how the
theory may be applied to complex serial patterns. The model also helps to illustrate
a recognized limitation of ART concerning the problem of ‘when next’ as opposed
to ‘what next’ in serial pattern processing. The artiﬁcial neural network I have been
developing, called ART-PaC (Adaptive Resonance with Temporal Pattern Coordina-
tion), relates to Gjerdingen’s model and incorporates a spectral timing mechanism,
enabling the network to recognize the temporal relationships of an input pattern and
thus to make ‘when next’ predictions. The ART-PaC architecture is employed as a
control system for a saxophone-playing robot. The objective is for the robot to learn
to improvise melodies based on a bank of melodies it is trained on.

A Theory of ‘Four’ in Igbo Culture and Its Application in the
Harmonic Structures of Oral and Written Musical Compositions
Christian Onyeji; North-West University, South Africa
2PM2-R05-4
The centrality of the number ‘four’ in Igbo cultural milieu is quite signiﬁcant. While
ﬁgures designate statistical/numerical values in most cultures of the world, certain
ﬁgures have deep spiritual and social meaning in Igbo culture, such that understand-
ing the spiritual and physical worlds of the Igbo is, inextricably, tied to the knowledge
of the signiﬁcance of such ﬁgures. Central to the social, spiritual and general cultural
systems of Igbo people, the number ‘four’ manifests in the creative arts in some ways.
The dominance of the interval of ‘fourths’ in harmonic structures of many transcribed
Igbo oral compositions (indigenous music) constrained ﬁeld and literature research
that revealed interconnection of the number ‘four’ in various aspects of Igbo culture.
The inquiry covered the spiritual/belief system, social rites/rituals and the musical


                                          80
ICMPC 10                                               Tuesday 26 August 2008

arts enabling theoretical deductions presented in this discourse. Also discussed is ex-
ploitation of harmonic principles based on the intervallic structure of fourths found
in Igbo indigenous music in art music compositions by selected Igbo composers in
their search for a socio-cultural base for their compositions. This presentation spot-
lights the deep interconnection of musical practices of the Igbo with the social and
spiritual aspects of the culture manifested in the special use and meaning of the num-
ber ‘four’, revealing the cultural root of harmonic structures that feature dominance
of fourths in Igbo oral compositions.




                                         81
Wednesday 27 August 2008                                                   ICMPC 10


3AM1-R02 : Timbre I
Room 2, 8:30 – 10:30 Wednesday 27 August 2008, Oral session

Auditory Roughness in East Asian Hybrid Compositions
Steve Everett; Emory University, USA
3AM1-R02-2
Timbre is a primary structuring element in music and one of the most important and
relevant features of auditory events. The auditory sensation of roughness can be
described as a timbral attribute based on the sensation of rapid ﬂuctuations in the
amplitude envelope. It is involved in several aspects of sound evaluation. This paper
examines culturally-speciﬁc properties of timbral roughness in compositions of
several East Asian composers and proposes a method of interpreting cross-cultural
perceptions based upon ﬁndings in recent neuropsychological investigations of
timbre space and language. Comparisons will be made of the spectral analyses of
timbres in traditional Japanese and Korean musical forms with those found in select
compositions by modern Japanese and Korean composers. Also a survey of several
spectral analysis programs that use traditional FFT and STFT algorithms will be
discussed. Special prominence is given to compositions involving the male voice in
combination with an instrumental ensemble that mirrors, shadows, or anticipates
the timbral structures of the voice without ever playing in strict unison with it. This
paper concludes that the consequence of a culturally-speciﬁc process of timbral
recognition is an important dimension to ascertaining musical meaning.

Can Pianists Recognize and Consistently Label Gesture-Controlled
Timbre Nuances from Hearing Only the Sound?
Michel Bernays, Caroline Traube; Université de Montréal, Canada
3AM1-R02-3
When discussed amongst professionals, timbre is empirically described through
subjective descriptions. This study aims to determine the consensus and coherence
of such vocabulary among pianists, at ﬁrst by testing their ability to aurally recognize
timbre and then by examining their consistency in labelling gesture-controlled
timbre nuances. A professional pianist was asked to perform three short pieces with
eleven adjectives as successive timbre instructions (bright, round, etc.). The audio
recordings were used as stimuli, ﬁrst in an identiﬁcation task by the pianist himself,
then by 17 other pianists, who provided a verbal description of each timbre they
could recognize (in free form, then by forced choice). The control procedure showed
that the pianist himself could easily identify which timbre he had used. An analysis
of the listening test revealed the semantic proximity between the descriptors chosen
by the group of pianists and the actual verbal descriptors. The results of this study
indicate that the expressive intentions of a virtuosic pianist can be perceived by his
peers and can be verbally described in a way that concurs with a common ability
among pianists to identify and label timbre.

A Hybrid Model for Timbre Perception — Part 1: The Color of
Sound
Hiroko Terasawa, Jonathan Berger; Stanford University, USA
3AM1-R02-4
We propose a hybrid model of timbre integrating two complementary component
models, one of “color” and the other of “texture”. Previous studies on timbre
perception describe a multidimensional space, in which spectral centroid, spectral
ﬂuctuation, and temporal attack and decay characteristics constitute the principal
components. We propose that these factors can be eﬀectively described in terms of a
waveform’s instantaneous spectral envelope (the color), and instantaneous temporal
irregularity (its texture). The color model employs MFCC (Mel-Frequency Cepstral
Coeﬃcients) as the spectral envelope descriptor. Our previous study suggests that
MFCC is well correlated with human perception. In this study, we further investigated
the degree to which each of twelve MFCC coeﬃcients has a good association to human
perception of timbre, when a single coeﬃcient from MFCC is manipulated in an
isolated manner. Each of the Mel-cepstral coeﬃcients predicts the timbre perception
at the similar level for all the twelve coeﬃcients. At average, MFCC explains 85% of
the subjective judgments of the deviation in spectral envelope and thus suggests
itself as a good representation for the perceptual model of a sound’s color.



                                          82
ICMPC 10                                           Wednesday 27 August 2008


3AM1-R03 : Computational Models and Analyses III
Room 3, 8:30 – 10:30 Wednesday 27 August 2008, Oral session

Computational Model for Automatic Chord Voicing Based on
Bayesian Network
Tetsuro Kitahara 1 , Makiko Katsura 1 , Haruhiro Katayose 1 , Noriko
Nagata 2 ; 1 Kwansei Gakuin University, Japan; 2 JST, Japan
3AM1-R03-1
We developed a computational model for automatically voicing chords based on a
Bayesian network. Automatic chord voicing is diﬃcult because it is necessary to
choose extended notes and inversions by taking into account musical simultaneity
and sequentiality. We overcome this diﬃculty by inferring the most likely chord
voicing using a Bayesian network model where musical simultaneity and sequentiality
are modeled as probabilistic dependencies between nodes. The model represents
musical simultaneity as probabilistic dependencies between voicing and melody
nodes while it represents musical sequentiality as probabilistic dependencies be-
tween current-chord and previous- or following-chord voiding nodes. The model
makes it possible to take into account both simultaneity and sequentiality at a single
inference process. Experimental results of chord voicing for jazz musical pieces
showed that our system generated chord voicings that had appropriate simultaneity
and sequentiality.

An Experimental Comparison of Human and Automatic Music
Segmentation
Justin de Nooijer 1 , Frans Wiering 2 , Anja Volk 2 , Hermi J.M.
Tabachneck-Schijf 2 ; 1 Fortis ASR, The Netherlands; 2 Utrecht
University, The Netherlands
3AM1-R03-2
Music Information Retrieval (MIR) examines, among others, how to search musical
web content or databases. To make such content processable by retrieval methods,
complete works need to be decomposed into segments and voices. One would expect
that methods that model human performance of these tasks lead to better retrieval
output.
We designed two novel experiments in order to determine (1) to what extent humans
agree in their performance of these tasks and (2) which existing algorithms best
model human performance. Twenty novices and twenty experts participated in these.
The melody segmentation experiment presented participants with both audio and
visual versions of a monophonic melody. In real time, participants placed markers at
segment borders. The markers could be moved for ﬁne-tuning.
The voice separation experiment presented participants auditorily with a polyphonic
piece. They then listened to pairs of monophonic melodies and chose from these the
one that best resembled the polyphonic piece. All possible pairs were ranked.
We concluded that there is high intraclass coherence for both tasks. There is no
signiﬁcant diﬀerence in melody segmentation performance between experts and
novices, and three algorithms model human performance closely. For voice separa-
tion, none of the algorithms is close to human performance.

ACE: Autonomous Classiﬁcation Engine
Ichiro Fujinaga, Cory McKay; McGill University, Canada
3AM1-R03-3
Pattern recognition and automatic classiﬁcation techniques are currently being
used for a wide range of tasks in music research. Unfortunately, the variety and the
technical sophistication of machine learning techniques available can make it diﬃcult
to choose the best approach for a particular problem. The aim of this research is
to provide a tool to researchers in general, and in music researchers, in particular,
an easy-to-use software that can empirically determine suitable solutions for the
classiﬁcation tasks at hand.
ACE automatically ﬁnds optimal or near-optimal classiﬁcation methodologies for
arbitrary supervised classiﬁcation problems through experimentation. ACE includes
implementations of a wide variety of machine learning techniques and provides
interfaces intended speciﬁcally for the needs of music researchers. The eﬀectiveness


                                         83
Wednesday 27 August 2008                                                  ICMPC 10

of diﬀerent approaches is analyzed by ACE in terms of classiﬁcation accuracy,
training time, and classiﬁcation time. This allows users to experimentally determine
the best set of techniques to use for their particular priorities.
By using this software, music researchers can reduce the time for selecting the
suitable classiﬁcation approach for a given music-related problem and help in
terms of both increasing classiﬁcation accuracy and saving signiﬁcant amounts of
development time.

Computational Modelling of the Cognition of Harmonic Movement
Raymond Whorley, Marcus T. Pearce, Geraint Wiggins; Goldsmiths
University of London, UK
3AM1-R03-4
Our long-term aim is to investigate the extent to which statistical models of harmony
can provide insights into the cognitive processes involved in the harmonisation of
melodies and in harmonic expectancy. We have chosen to use multiple viewpoint
systems to model harmonic movement. We shall propose a number of ways in which
the multiple viewpoint framework can be developed, such that the complexities
of harmony can be adequately addressed; after all, four-part harmony consists of
four inter-related sequences. In this work, the linking of viewpoints is extended
to linking between parts or layers; and this inter-layer linking evolves into a more
ﬂexible form, which has a number of advantages over the strict viewpoint linking
of the original framework. For example, it allows context to be used which would
be unavailable if conventionally linked to an undeﬁned symbol. It also allows the
formation of relatively sparse contexts, which cover a longer length of sequence than
conventionally formed contexts containing the same number of symbols. Finally,
Prediction by Partial Match, which combines the predictions of Markov models of
diﬀerent order, is now also able to make use of models employing diﬀerently shaped
contexts of the same order.


3AM1-R04 : Memory and Imagery
Room 4, 8:30 – 10:30 Wednesday 27 August 2008, Oral session

Enculturation Eﬀects in Music Cognition: The Role of Age and
Music Complexity
Steven J. Morrison, Steven M. Demorest, Laura A. Stambaugh;
University of Washington, USA
3AM1-R04-1
The purpose of this study was to replicate and extend ﬁndings from previous studies
of music enculturation by comparing music memory performance of children to that
of adults when listening to culturally familiar and unfamiliar music. Forty-three
children and 50 adults, all born and raised in the United States, completed a music
memory test comprising unfamiliar excerpts of Western and Turkish classical music.
Examples were selected at two levels of diﬃculty — simple and complex — based on
texture, variety of instruments, presence of simultaneous musical lines and clarity of
internal repetition. All participants were signiﬁcantly better at remembering novel
music from their own culture than from an unfamiliar culture. Simple examples of
both cultures were remembered signiﬁcantly better than complex examples. Children
performed as well as adults when remembering simple music from both cultures,
while adults were better than children at remembering complex Western music. The
results provide evidence that enculturation aﬀects one’s understanding of music
structure prior to adulthood.

Beyond the Betts: Exploring Ecologically Valid Methods for
Assessing Musicians’ Imagery Abilities
Terry Clark, Aaron Williamon; Royal College of Music, UK
3AM1-R04-2
Research suggests that engaging in mental rehearsal can enhance performance
of a skill. However, an individual’s imagery ability can moderate its usefulness.
This study sought to explore the eﬃcacy of approaches for assessing imagery
ability in musicians that go beyond self-report measures. The participants were 25
performance students recruited at the Royal College of Music. They completed a
range of imagery questionnaires and gave two live and two mental performances
of a two-minute extract of their choice. The Inter Onset Intervals (IOIs; i.e. the

                                         84
ICMPC 10                                          Wednesday 27 August 2008

beat-by-beat tempo) for all four performances were extracted and then averaged
within each of the two conditions. Correlations between the averaged IOIs of the two
conditions were calculated. These correlations were compared to scores obtained
on the imagery questionnaires. A subset of 17 participants underwent a three-week
imagery training programme and completed post-testing identical to that used in
the pre-test. The participants’ ability to produce signiﬁcantly similar performances
between the two conditions is discussed, as well as how these correlations relate to
scores attained on the imagery ability questionnaires. Eﬀects from engagement in
the imagery training programme are reviewed.

Music in Everymind: Commonality of Involuntary Musical Imagery
Lassi A. Liikkanen; Helsinki Institute for Information Technology,
Finland
3AM1-R04-3
A new topic within the psychological discipline concerns involuntary semantic
memories. Initial research has suggested that musical memories are the dominant
type of memories that are remembered involuntarily. Interestingly, no reliable
information exists on how common this phenomenon of ‘earworms’, mental replay
of music, is among people living in a western culture. Present study intended to
investigate this issue by examining the topic in retrospect. Study conducted among
12,420 Finnish Internet users showed that 91.7% of people reported experiencing this
phenomenon at least once a week. Several statistical procedures were used to relate
the retrospectively reported frequency of the phenomenon to a set of background
variables. This revealed a positive connection to increased music practice and
listening, and sex (being a female). In contrast, with increasing aged, the frequency
of the incidents decreased. The results are discussed in the paper revealing the most
important factors underlying this non-volitional experience.

Music Engagement Predicts Verbal Memory as Eﬀectively as
Musicianship
T.C. Chin, N.S. Rickard; Monash University, Australia
3AM1-R04-4
Musicians have been reported to have signiﬁcantly better verbal memory abilities than
do non-musicians. In this study, we examined whether forms of music engagement
other than formal music training might also predict verbal memory performance,
after controlling for gender, educational level, and musicianship variables. We
present the ﬁrst evidence to date that music engagement (particularly aﬀective
engagement and listening frequency) predicts verbal memory measures to a similar
degree as reported for music training. The ﬁndings extend previous research by
suggesting that simply listening to or engaging emotionally with music may be
suﬃcient to enhance cognitive performance.


3AM1-R05 : Performance IV
Room 5, 8:30 – 10:30 Wednesday 27 August 2008, Oral session

Coordination of Body Movements and Sounds in Musical Ensemble
Performance
Peter E. Keller 1 , Mirjam Appel 2 ; 1 MPI CBS, Germany; 2 Eberhard Karls
Universität Tübingen, Germany
3AM1-R05-1
The current study tested the reliability and validity of interpersonal body-sway
coordination as an index of ensemble synchrony in piano duos. Seven pairs of
pianists performed two unfamiliar classical duets on a pair of MIDI pianos, with and
without visual contact. Pianists’ body movements were recorded using a motion
capture system. One-dimensional information about anterior-posterior body sway
was extracted from the normalized movement data associated with a single marker
positioned between each pianist’s shoulder blades. The diﬀerence in pianists’ body
positions across time was estimated by cross-recurrence analysis with a moving
window. Asynchronies between nominally synchronous sounds were computed
from the MIDI data. Analyses of the pianists’ movements revealed large inter-duo
diﬀerences in the coordination of body sway. These diﬀerences provided an index of
ensemble synchronization that was reliable (i.e., constant across contrasting musical
pieces and independent of whether or not pianists were in visual contact) and valid

                                        85
Wednesday 27 August 2008                                                  ICMPC 10

(i.e., body sway coordination was negatively correlated with the degree of asynchrony
between sounds). The ﬁnding that body-sway coordination provides a reliable and
valid index of ensemble synchrony has theoretical implications that bear upon the
notion of music as an embodied phenomenon, as well as practical advantages for
quantifying ensemble cohesion.

The Inﬂuence of Another’s Actions on One’s Own Synchronization
with Music
Lena Nowicki, Peter E. Keller; MPI CBS, Germany
3AM1-R05-2
Despite of the wealth of insights into the mechanisms underlying sensorimotor
synchronization with computer controlled sequences, little is known about interper-
sonal sensorimotor synchronization. In the present study, pairs of musically trained
participants tapped the beat of tonal sequences either on their own or alternating
with another person. In the latter ‘joint’ condition, the role of auditory feedback was
studied by manipulating whether participants could hear percussive sound triggered
by their own and/or their partners’ taps. Autocorrelations of intertap intervals (ITIs)
as well as means, variance and autocorrelations of asynchronies were analyzed as
measures of the mutual inﬂuence of tap timing on sensorimotor synchronization.
The lag-1 autocorrelations of ITIs were most negative in the joint tapping condition,
which is suggestive of timekeeper coupling between the co-actors. Such coupling
could be necessary to correct for the other person’s errors as precisely as possible.
The results of the asynchronies also show that acting with another person has an
impact on one’s own synchronization performance. Mean asynchronies and variance
of asynchronies were highest in the joint condition and the autocorrelations revealed
mutual error correction when acting with a partner. Further, auditory feedback was
found to play an important role in interpersonal sensorimotor synchronization.

Plus Ça Change: Analyzing Performances of Chopin’s Mazurka Op.
24 No. 2
Neta Spiro 1 , Nicolas Gold 2 , John Rink 1 ; 1 Royal Holloway University
of London, UK; 2 King’s College London, UK
3AM1-R05-3
Performances of the same piece can diﬀer from one another in innumerable ways
and for many diﬀerent reasons. Such diﬀerences are of considerable interest to mu-
sicologists and psychologists. The aim of the current study is to analyze the timing
and dynamic patterns of numerous performances in order to explore the musical
reasons for use as well as diﬀerences in use of those patterns. More speciﬁcally it
investigates the relationship between 1) structural and thematic characteristics of
a piece and the timing and dynamic characteristics of performances of that work
and 2) the relationship between patterns of timing and those of dynamics. A new
methodology is developed and applied which enables the systematic analysis and
comparison of diﬀerent performances by identifying patterns of performance, or
performance motives, and their location in performance. The results show that,
in general, the structure of a piece emerges from the performance patterns. The
relationship between timing and dynamics is not direct and the sources for use of
particular patterns seem to be many and varied, including structural and thematic
considerations. However, the performance patterns at section ends seem to be most
closely related to the surrounding long-term structural characteristics, while those
within some sections seem to be closely related to the motivic patterns driven by
genre-speciﬁc characteristics of the piece.

Spatio-Temporal Cues for Synchronization with Conductors’
Gestures
Geoﬀ Luck; University of Jyväskylä, Finland
3AM1-R05-4
The present study investigated the spatio-temporal characteristics of human move-
ment that oﬀer cues for between-person synchronization. The context chosen for
the study was that of conductor-musician synchronization. Participants tapped in
time with dynamic point-light representations of traditional conducting gestures
in which the clarity of the beat and overall tempo was manipulated. A number
of spatio-temporal features were computationally extracted from the movement
data, and the relationship between the timing of participants’ synchronizations and
these features examined. A series of linear regression analyses identiﬁed absolute


                                         86
ICMPC 10                                            Wednesday 27 August 2008

acceleration along the trajectory as the main cue for synchronization, while beat
clarity and tempo inﬂuenced the precise make-up of the emergent models. Overall,
the regression models accounted for 48% – 73% of the variance in participants’
responses. These results support previous work that has identiﬁed acceleration
along the trajectory of a movement as a ‘beat inducer’ using simpler stimuli, and
suggest that temporal features of a movement trajectory may oﬀer more reliable and
stable cues for synchronization than spatial features.


3AM2-R07 : Musical Scales and Melody / Harmony and
Tonality / Timbre / Psychophysics and Music Acoustics
Room 7, 10:40 – 12:30 Wednesday 27 August 2008, Poster session

Musical Parameters and the Classiﬁcation of Melodic Motives
Zohar Eitan 1 , Roni Y. Granot 2 ; 1 Tel Aviv University, Israel; 2 Hebrew
University, Israel
3AM2-R07-02
Music theorists often maintain that the “primary” musical parameters of pitch
and rhythm determine motivic categorization, while “secondary” parameters like
loudness or timbre do not establish motivic categories. We examined systematically
the eﬀects of contrasts in “primary” vis-à-vis “secondary” musical parameters on
listeners’ melodic classiﬁcation. Matrices of melodic patterns, each presenting 8
motives, were created by all interactions of two contrasting conditions in three
musical features. In Experiment 1, four matrices manipulated pitch contour, pitch-
interval class, and a compound feature involving the “secondary” parameters of
dynamics, pitch register and articulation. In Experiment 2, four diﬀerent matrices
manipulated rhythmic structure (metrical and durational accent), pitch intervals,
and the compound feature used in Exp1. Participants classiﬁed stimuli in each
matrix into two equal-numbered (4-4) groups of motives “belonging together.” In
both experiments, most participants used contrast in “secondary” parameters as
a basis for classiﬁcation, while few classiﬁcations relied on diﬀerences in pitch
interval (Exp2) or interval class (Exp2). Musically trained participants’ classiﬁcations
also applied melodic contour and rhythm. Results suggest a hierarchy of musical
parameters that reverses that suggested by most academic music theory. We discuss
the ramiﬁcations of this reversal on perceived musical structure and consequentially
on cognitively-informed music analysis.

Key Perception in Encountering a Non-Scale Tone
Rie Matsunaga 1 , Jun-ichi Abe 2 ; 1 JSPS, Japan; 2 Hokkaido University,
Japan
3AM2-R07-04
The present study investigated how a listener perceives a key when hearing a series
of tones followed by an additional non-scale tone of the current perceived key.
Here, we examined whether listeners always shifted their key responses in this
circumstance. To accomplish this aim, we developed two cases: One case presented
an additional tone that was not interpretable as a scale tone of the current key,
but which did ﬁt into the diatonic set of other keys. The other case, presented
an additional, non-scalar tone, which did not ﬁt into either the original key or any
other diatonic key. Musicians and non-musicians had to respond by selecting a key
(tonal center) category or an atonal judgment along with a conﬁdence rating for
given tone sequences. Responses made by the musicians and the non-musicians
yielded essentially equivalent results. In both cases, participants did not always shift
their responses to diatonic-congruent key categories (and atonal category), when a
tone sequence was followed by an additional non-scale tone. This suggests that a
non-scale tone is not always counted against evidence the current perceived key.

Ignore the First Chord, Just Appreciate the Second: Can Listeners
Control the Predisposition to Process the Interrelationship
Between Chords?
Hiroshi Arao; Hiroshima International University, Japan
3AM2-R07-06
The automaticity of processing is a crucial concept for understanding human cogni-
tion. Interference paradigms (e.g., the well-known Stroop and other tasks) generally


                                          87
Wednesday 27 August 2008                                                   ICMPC 10

provided a good insight into the nature of such processing. In the ﬁeld of music
cognition, however, there are limited studies that directly examine the automaticity
of processing. In the present study, the author examined the processing automaticity
of the relationship between chords. A selective appreciation task, a time-domain
extension of interference paradigms, was utilized. In each trial, listeners were
presented with a ﬁrst chord, a silence, and then a second chord. They were asked to
ignore the ﬁrst chord, to selectively attend to the second, and to rate the pleasantness
of this chord alone. The rating regarding the second chord, which was inﬂuenced by
its own consonance, was aﬀected by the presence of the to-be-ignored ﬁrst chord, or
more precisely, by the relationship between the ﬁrst and second chords. The results
showed that listeners have diﬃculty in inhibiting the predisposition to process
chordal relationships, even in a situation where a deﬁnite empty time interval is
inserted between the chords. In music cognition, such sequential processing appears
to be highly automatic and to exert an unavoidable inﬂuence on the appreciation of
basic aﬀective properties.

Using Kolmogorov-Smirnov to Determine the Eﬀect of Interval
Cycles on the Formation of Krumhansl & Kessler’s (1982) Tonal
Hierarchies
Matthew Woolhouse, Ian Cross; University of Cambridge, UK
3AM2-R07-08
To date, a number of statistical and psychoacoustic models have been proposed that
attempt to account for the pitch hierarchy measurements of Krumhansl et al. in
the 1980s. However, as Krumhansl has pointed out, none oﬀers a complete or fully
plausible explanation of the origin of Krumhansl’s tonal hierarchies (or variations
thereof), leaving open the possibility that, in addition to distributional and psychoa-
coustic factors, a further cognitive process may be involved in their formation. The
analyses presented in this paper demonstrate that there is a strong statistical link
between interval cycles and the perceptual ratings used by Krumhansl and Kessler to
form their major and minor-key tonal hierarchies. (An interval cycle is the minimum
number of iterations of an interval that are required for the original pitch classes
to be restated — a property that is hypothesized to lead to abstract grouping and,
subsequently, pitch attraction). Although correlations are not conclusive proof of
causality, given the size of the data sets and the objective nature of the method, the
evidence supporting a link is compelling. The results of the analyses are discussed
with respect to Brown, Butler and Jones’s rare-interval hypothesis. The important
role played by the interval cycles of dominant harmonies is also be explored.

Sequential Grouping Based on Timbre: Eﬀects of Diﬀerences in
Size of Resonant Bodies
Chihiro Takeshima 1 , Minoru Tsuzaki 1 , Toshio Irino 2 ; 1 Kyoto City
University of Arts, Japan; 2 Wakayama University, Japan
3AM2-R07-10
Natural sounds contain information about the shape and size of the resonator, and
humans perceive the sounds as timbre variation. We performed psychoacoustic
experiments to measure the ability of auditory stream segregation (sequential
grouping) based on the timbre change caused by size variation. In the ﬁrst and
second experiments, eight normal-hearing listeners were presented a six-vowel
sequence, and were required to identify the vowel sequences in the correct order.
Each vowel in the sequence was alternated in the speaker size (Experiment 1), or
ﬁxed to a constant speaker size (either large or small) (Experiment 2). The results
of Experiment 1 showed that it became diﬃcult to identify the sequences correctly
as the size-alternation became faster and larger, while the results of Experiment 2
showed no signiﬁcant deterioration due to the speaker size. In the third experiment,
listeners were presented a target vowel sound in advance on visual display and
asked to judge whether the size-alternated vowel sequence (as used in Experiment
1) contains the target. The results showed that the degree of deterioration caused
by size-alternation was smaller than that in Experiment 1. These ﬁndings demon-
strate that the size-alternation aﬀects the temporal order judgment and does not
signiﬁcantly aﬀect the individual vowel identiﬁcation. Since this situation is typically
observed when sequential grouping takes place, fast temporal alternation in size
information would induce sequential grouping.




                                          88
ICMPC 10                                            Wednesday 27 August 2008

Inharmonicity of Sounds from Electric Guitars: Physical Flaw or
Musical Asset?
Hugo Fastl, Florian Völk; Technische Universität München, Germany
3AM2-R07-12
Because of bending waves, strings of electric guitars produce (slightly) inharmonic
spectra. One aim of the study was to ﬁnd out — also in view of synthesized musical
instruments — whether sounds of electric guitars should preferably produce strictly
harmonic or slightly inharmonic spectra. Inharmonicities of typical sounds from
electric guitars were analyzed and studied in psychoacoustic experiments. Strictly
harmonic as well as slightly inharmonic spectra were realized and evaluated with
respect to audibility of beats, possible diﬀerences in pitch height, and overall pref-
erence. Strictly harmonic as well as slightly inharmonic spectra produce essentially
the same pitch height. Depending on the magnitude of the inharmonicity, beats are
clearly audible. Low, wound strings (e.g. E2, A2) usually produce larger inharmonic-
ities than high strings (e.g. E4), and hence beats of low notes are easily detected.
The inharmonicity increases with the diameter of the respective string’s kernel.
Therefore, an unwound G3 string can show larger inharmonicity and more prominent
beats than a wound D3 string. In a musical context, i.e. when playing short melodies
with synthesized strictly harmonic versus slightly inharmonic string sounds, the
latter are somewhat preferred, in particular by players of string instruments. The
slight inharmonicity of sounds from electric guitars is not a physical ﬂaw which
should be avoided by synthesizing perfectly harmonic sounds, but can be regarded
as a musical asset.


3AM2-R08 : Rhythm and Timing / Body Movement /
Memory
Room 8, 10:40 – 12:30 Wednesday 27 August 2008, Poster session

Automatic Classiﬁcation of Drum-Rhythm Patterns Employed in
Popular Music
Yuki Murakami, Masanobu Miura; Ryukoku University, Japan
3AM2-R08-02
The various styles of drum-rhythm patterns that are often employed in popular music
can roughly be classiﬁed into “basic rhythm” and “ﬁll-in” patterns. Compared to the
basic rhythm pattern, the ﬁll-in is an improvised pattern and is usually employed to
ﬁll in blanks in accompanying melodies. However, no criteria for objectively classify-
ing these patterns have been reported to date. This study describes a classiﬁcation
method based on similarity in neighborhood. We propose a method of classifying
drum-rhythm patterns by automatically adjusting the criteria based on the context of
excerpts. The method deals with symbolic representations, as provided by MIDI data.
Each sequence of one-measure-length contained in the given excerpt is evaluated as
corresponding to either a “basic-rhythm” or “ﬁll-in” pattern. The characteristics of
sequences for drum-rhythm patterns are obtained by classifying them. In addition,
we implemented the method as a visual application system that plays back MIDI data
and indicates ﬁll-in patterns.

Longitudinal Study of Rhythm Tapping with Visual Feedback
Makiko Sadakata, Alex Brandmeyer, Renee Timmers, Akvile
Miezlaiskyte, Peter Desain; Radboud University Nijmegen, The
Netherlands
3AM2-R08-04
The aim of this study is to investigate the beneﬁt of the visual feedback (VFB) on the
imitation and discrimination of three-interval rhythms. A previous study indicated
that short-term use of the VFB system facilitated learning to imitate loudness
patterns, while it was not for timing patterns or for discriminating loudness and
timing patterns. This study examines the eﬀect of long-term use of the system
which depicts timing information as the shape of a visual object. The experiment
employed a pretest-posttest between-subject design. 24 non-musicians participated
in 6–9 sessions of an imitation task, which took place in 3 weeks. During the test,
participants imitated aurally presented rhythms on a touch sensitive pad. During
the training, participants practiced imitating a subset of test rhythms that were
presented aurally and visually (VFB group). The perceptual discrimination task took
place at the beginning and end of the series of sessions. The analyses revealed (1) that


                                          89
Wednesday 27 August 2008                                                  ICMPC 10

participants signiﬁcantly beneﬁted from the long-term use of the VFB system and (2)
that the use of the VFB system did not result in superior perceptual sensitivity.

A Psychophysiological Approach to Auditory Temporal
Assimilation
Takako Mitsudo 1 , Yoshitaka Nakajima 1 , Gerard B. Remijn 2 ,
Yoshinobu Goto 3 , Shozo Tobimatsu 1 ; 1 Kyushu University, Japan;
2
  Kanazawa University, Japan; 3 International University of Health and
Welfare, Japan
3AM2-R08-06
We recorded event-related brain potentials (ERPs) to reveal the mechanisms of human
auditory temporal perception utilizing phenomena of temporal assimilation. Stimu-
lus patterns consisted of two neighboring time intervals marked by three successive
tone bursts (20 ms, 1000 Hz). There were eleven stimuli: seven standard stimuli,
where the ﬁrst time interval (T1) varied from 80 to 320 ms, and the second time
interval (T2) was ﬁxed at 200 ms, and four dummy stimuli, where the combinations of
T1 and T2 diﬀered from those of the standard stimuli. Participants judged whether
T1 and T2 had the same duration or not by pressing a button as quickly as possible.
Behavioral data showed a clear asymmetrical temporal assimilation; the participants
judged the two neighboring time intervals as the same when -80 ≤ (T1-T2) ≤ +40
[ms]. Electrophysiological data showed the following results: (a) A slow component
in the frontal area, that was similar to contingent negative variation (CNV), emerged
at approximately 300 ms after the 1st marker, and lasted until 300–400 ms after the
3rd marker; (b) the responses recorded from the right frontal electrodes were greater
than those from the left frontal electrodes, but (c) within a range of 200 ms after the
3rd marker, the frontal areas’ activation was smaller when the temporal assimilation
took place. Our results suggest that the attenuation of the frontal negativity that
took place immediately after the stimuli correlates with the perceptual assimilation
of auditory temporal patterns.

Measuring Co-Operability in Tapping Dyads
Tommi Himberg; University of Jyväskylä, Finland
3AM2-R08-08
Co-operability between two drummers/tappers engaged in synchronised rhythm
performance can be measured by looking at their phase error correction, and specif-
ically the error correction coeﬃcient (α). This coeﬃcient refers to the proportion of
the synchronisation error (SE) at beat tn that is corrected at the next beat tn+1 . The
model suggests that in co-operative correction, the tappers would correct the error
by meeting each other half-way; in other words, error-correct by approximately equal
amounts but in opposing directions. Vorberg (2005) and Repp & Keller (in press) have
tested this model by asking people to synchronise with simulated computer tappers.
In the current study, the model was applied post-hoc to co-operative tapping data,
where two people were synchronising with each other, or with computer metronome.
The αh was estimated, and the premises of the model discussed in light of two-tapper
data. It was then suggested that instead of the roles of leader and follower being
ﬁxed, the mutual adaptation to each other might change in the course of the trial. A
method of calculating and visualizing the change in the dependency between tappers
using windowed cross-correlation was then introduced.

The Beat Alignment Test (BAT): Surveying Beat Processing
Abilities in the General Population
John R. Iversen, Aniruddh D. Patel; The Neurosciences Institute, USA
3AM2-R08-10
The ability to perceive a musical beat (and move in synchrony with it) seems
widespread, but we currently lack normative data on the distribution of this ability
in musically untrained individuals. To aid in the survey of beat processing abili-
ties in the general population, as well as to attempt to identify and diﬀerentiate
impairments in beat processing, we have developed a psychophysical test called
the Beat Alignment Test (BAT). The BAT is intended to complement existing tests
of rhythm processing by directly examining beat perception in isolation from beat
synchronization. The goals of the BAT are 1) to study the distribution of beat-based
processing abilities in the normal population and 2) to provide a way to search for
“rhythm deaf” individuals, who have trouble with beat processing in music though
they are not tone deaf. The BAT is easily implemented and it is our hope that it is
widely adopted. Data from a pilot study of 30 individuals is presented.
                                         90
ICMPC 10                                             Wednesday 27 August 2008

The Development of Coordination and Expression in Duo
Performance: Body Movements in Focus
Jane Davidson; University of Western Australia, Australia
3AM2-R08-12
This poster reports how a ﬂautist and clarinettist came together in a rehearsal
process to produce an agreed and coordinated timing and interpretation of an
unfamiliar work. The current study builds on the foundations of previous studies
to assess how musicians in a duo coordinate. Two professional musicians were
asked to rehearse individually and then together a piece composed for the task.
All was recorded on video and motion capture system. Results indicated that solo
rehearsal oﬀered a diﬀerent core speed for each player: the ﬂautist remaining close
to the tempo and types of expressive musical eﬀects she had set in her ﬁrst solo
run-through; the clarinettist experimenting extensively with diﬀerent tempi and a
range of eﬀects. Each used their bodies very diﬀerently, the clarinettist using much
more expansive movement than the ﬂautist. The clarinettist explored the musical
material in order to ﬁnd an interpretation, whereas the ﬂautist used the run-throughs
to consolidate her interpretation. In duo, it was as if the clarinettist who tried to ﬁnd
how to “dance” (her words) with her duet partner, exploring the musical and physical
space and noticing the ﬂautist’s responses. The ﬂautist’s movements became much
more like those of the clarinettist, adapting and coming to focus on the tempo and
expressive eﬀects of the clarinettist.


3AM2-R09 : Music Listening and Preferences /
Development / Performance / Audio-Visual Interactions
/ Psychoacoustics
Room 9, 10:40 – 12:30 Wednesday 27 August 2008, Poster session

Peak Experience in Music Performance: An Investigation of the
Precursors and Nature Among Choral Performers
John Whaley; Keele University, UK
3AM2-R09-02
The study of Peak Experiences (Maslow, 1968) has received signiﬁcant attention
within music psychology as music is regarded as a reliable trigger of such experi-
ences. Most literature, however, has focused on music listening and disregarded
the potential of music performance to elicit the phenomenon. This study investi-
gates both the causes and nature of peak experiences in music performance, with
three fundamental aims: to demonstrate that peaks occur as a result of music
performance, to investigate speciﬁc internal/personal and external/circumstantial
precursors to these experiences, and to analyze features common to these experi-
ences. Using a single concert performance of a collegiate Choir, the study pre and
post-tested participants using Likert-scale items derived from both music and non
music-related peak literature. Findings indicate that music performance can trigger
peak experiences and the data reveal three distinct “types” of performance peaks:
lost/carried away, perfection, and physical reaction. Findings also identify a number
of precursors signiﬁcantly associated with these experiences, including self-rated
quality of performance, degree to which music is liked, perception of audience,
attempts to ‘send a message’ to audience, and previous peaks in performance and
during other activities. Discussion includes assessment of the quantitative approach
to the Peak Experience and the implications of this research for both performers and
future research.

The Musical Dimension of Daily Routines with Under-Four
Children
Anna Rita Addessi; University of Bologna, Italy
3AM2-R09-04
This paper deals with a research project currently being undertaken about the
observation of young children musical behaviours during the daily routine: changing
the diaper, before sleeping, the lunch, free game.




                                          91
Wednesday 27 August 2008                                                   ICMPC 10

The Impact of Severe Visual Impairment on Musical Development:
A Comparison of Two Syndromes — Retinopathy of Prematurity
and Septo-Optic Dysplasia
Adam Ockelford; Roehampton University, UK
3AM2-R09-06
Accounts from teachers have suggested that children born with little or no sight
tend to exhibit more highly-developed levels of musicality than their fully sighted
counterparts, as evidenced, for example, in the possession of ‘absolute pitch’, and
precocious performing skills. However, until recently, no systematic research has
been undertaken in this area. This paper reports on studies that examine the impact
of severe visual impairment in the context of two syndromes. Questionnaires were
distributed to three groups of parents, largely in the UK and the US, whose children
had (a) retinopathy of prematurity; (b) septo-optic dysplasia; and (c) no known
long-standing medical conditions. These were supplemented with visits to observe
the children in action. The principal topics covered were parents’ perceptions of: (a)
their children’s interest in everyday sounds and in music; (b) the importance of music
to their children in diﬀerent contexts; (c) the time they devoted to musical activity;
(d) their musical abilities (including AP); and (e) learning, education and therapy. The
results suggested that both level of vision and type of syndrome had an impact on
developing musicality, as evidenced by the reported levels of AP and the levels of
achievement in terms of performance skills and creative output, expressed through
improvisation and composition.

A Longitudinal Study of the Process of Acquiring Absolute Pitch
Ayako Sakakibara; Ichi-onkai Music School, Japan
3AM2-R09-08
In the present paper, the process of acquiring absolute pitch was described. 24
young children (2∼6 years old) were trained by a “Chords identiﬁcation method”
with an established success rate of greater than 90%. This method mainly consists of
tasks for identifying some chords [Eguchi,1991; Sakakibara,1999,2004]. They need
to remember 9 kinds of chords for white-piano key tones and 5–15 kinds of chords
for black-piano key tones.
The purpose of this study was to investigate the transition of cognitive strategies
longitudinally in this training process. According to the view that the attributes
of tones have two components : “tone height” and “tone chroma”, absolute pitch
possessors were supposed to have a strategy depending on “chroma” in identiﬁcation
of pitch.
Results showed that 2 strategies were observed in the training process: one depend-
ing on “height” and the other depending on “chroma”. A typical change proﬁle over
time was evident in virtually all subjects. Initially, subjects tried to identify chords
only by pitch “height”, but later they came to rely upon “chroma”, especially as the
number of chords increased. The tendency to depend on “chroma” increased over
time in training. Finally subjects succeeded in correctly identifying chords by relying
upon both “height” and “chroma”. After the training in which chords change into
single tones, every subject could identify single tones. Average length of the time it
took to reach 100% accuracy in identifying single tones was 4 years and 5 months.

Towards an Ecological Evolutionary Psychoacoustics of Music
Richard Parncutt; University of Graz, Austria
3AM2-R09-10
Psychoacoustical principles shape and limit musical experience. Why do many music
theorists and musicologists regard psychoacoustics as irrelevant? Here, I situate
psychoacoustics and music in broader ecological and evolutionary contexts. What
plausible evolutionary-ecological explanations for music-psychoacoustic phenomena
can be developed, tested and applied in music education? Hearing is adaptive
when it promotes recognition and evaluation of sound sources. Typical human
environments contain multiple reﬂectors. The signal reaching the ear is often an
unrecognizable superposition of direct and reﬂected/delayed sound. Frequency
analysis enables source recognition: partial frequencies (and rhythmic patterns)
are unaﬀected by reﬂection/superposition, amplitudes are somewhat aﬀected, and
phase relations are jumbled (at middle to high frequencies). Ecological-evolutionary
theory thus explains why the ear is more sensitive to frequency (there are roughly
1000 JNDs in the audible range) than intensity (roughly 100 JNDs), and almost phase
deaf. Thus, frequency and time are the most reliable auditory parameters for source


                                          92
ICMPC 10                                           Wednesday 27 August 2008

recognition (Terhardt) and the main axes of both the auditory scene and most music
notation. Ecological-environmental arguments can also explain why a harmonic
complex tone usually has one pitch although several partials are audible, why the
pitch corresponds to the fundamental, and why critical bandwidth is around 2–3
semitones in the mid-high range and increases at lower frequencies — with important
consequences for musical structure.

Eﬀect of Music on the Performance and Impression in a Slot Game
Yuma Sakabe, Toshimitsu Katsuzaki, Masashi Yamada; Kanazawa
Institute of Technology, Japan
3AM2-R09-12
It is often said that music aﬀects the impression of our environment and also our
behavior. This was evidenced in the context of the virtual environment of a video
racing game. In the present study, we examined the eﬀect of music on the perfor-
mance and impression of a slot-machine game. Eight players were trained in a fairly
simple mode called “avoiding the alignment” for at least 40 hours, then participated
in the experiment. In the experiment, ten musical excerpts were prepared. One
session consisted of 100 trials of playing the game in the “avoiding the alignment”
mode with listening to a musical stimulus or without music. Each participant played
two sessions for each condition. After each session the participants evaluated the
impression of the game using semantic diﬀerential scales. They also evaluated the
impression of each excerpt without playing the game. Results showed that there
were no excerpts that increased the success rate from the no music condition.
Moreover, principle component analyses and multi-regression analyses showed that
an “agitated” musical excerpt provides “unpleasantness” to the game and, in turn,
results in a negative eﬀect on the performance of the game. The results in the
present study were consistent with the results for the video racing game.


3AM2-R10 : Emotional Aspects / Rhythm and Timing
Room 10, 10:40 – 12:30 Wednesday 27 August 2008, Poster session

Emotional Connotations of Isolated Instruments Sounds
Tuomas Eerola, Vinoo Alluri, Rafael Ferrer; University of Jyväskylä,
Finland
3AM2-R10-02
Considerable eﬀort has been exerted towards understanding how performance
of music and its psychoacoustic and structural features contribute to emotional
expression, but relatively little attention has been paid to the role of timbre in com-
municating emotions in music. The aim was to investigate the role of timbre in the
perception of emotions in music. Are the sounds played from regular instruments
in isolation enough to convey a sense of emotional attribution to a listener? And
if so, which acoustic correlates of timbre are responsible for these attributions?
The role of timbre to emotional connotations of musical sounds was investigated in
two behavioural experiments. In Experiment 1, musically trained participants rated
valence, energy and tension of isolated instrument sounds. In Experiment 2, another
set of sounds was rated to extend the results. Acoustic descriptors of the stimuli
such as spectral measures (e.g. Brightness, Spectral ﬂux, Irregularity, with separate
descriptors for diﬀerent parts of the envelope), and onset and decay measures were
extracted in order to explain the perceived emotional dimensions. High agreement
among the emotion ratings of the participants was observed, suggesting that even
isolated instrument sounds carry cues about the emotional expression that are
easily recognized by the listeners. These associations could be explained in acoustic
features as strong correlations between the ratings and acoustic features were
observed.

Japanese College Students’ Emotional Responses to J-POP-Like
Songs on Physiological and Psychological Measures
Kaori Iwai, Mayumi Adachi; Hokkaido University, Japan
3AM2-R10-04
We examined Japanese college student’s emotional responses to J-POP-like songs by
physiological and psychological measure. We had them listen to either an original
song consisting of the same emotional values of a tune and lyrics or that consisting
of contrasting emotional values of a tune and lyrics. First, we recorded each partic-


                                         93
Wednesday 27 August 2008                                                  ICMPC 10

ipant’s heart rate and blood pressure while listening to a song, and obtained their
judgment for an emotional value of the song when the song ended. 1–2 days later,
each participant recorded emotional impressions of the same song continuously
while listening for the second time. Results showed that the overall emotional values
of the songs had no signiﬁcant eﬀects on heart rate or blood pressure. However, the
continuously recorded valence showed that happy and sad tunes (regardless of lyrics
types) were clearly diﬀerentiated from each other regardless of the time location of
a song. This indicates that Japanese college students tend to rely on the emotional
value of a tune rather than that of lyrics. In the poster presentation, we also explore
relations between physiological and psychological responses by mapping the two
continuous data.

Aﬀective and Perceptual Responses to Very Brief Musical Stimuli
Richard D. Ashley; Northwestern University, USA
3AM2-R10-06
The nature and time course of emotional responses to music is an open question
(cf. Peretz et al 1998, Bigand et al 2005), particularly in the very earliest moments
of a sound. This study aimed to (a) examine the time course of aﬀective and other
responses to very brief musical stimuli, and (b) investigate the inﬂuence of timbre
and other acoustic features in such responses. Stimuli were a subset of those from
Peretz et al 1998, with both positively and negatively valenced excerpts, in solo piano
and orchestral textures, with durations of 25, 50, 100, 250, 500, and 1000 msec.
Participants rated each excerpt on a number of 1–10 scales: sad/happy, slow/fast,
dark/bright, bass/treble, heavy/light, thick/thin, and also as minor or major. The
data indicate main eﬀects of excerpt. In many instances, accurate identiﬁcation
of valence and mode were made below 100 msec. For some excerpts, judgments
of valence reached the asymptote at 100 msec, but others showed no signiﬁcant
diﬀerence from 50 msec on. There was remarkable consistency as well in the other
judgments, which again tended to resolve at or before 100 msec. Acoustic features,
including register, spectral centroid, and attack time appear to be the underlying
sources of many of these judgments.

Psycho-Physiological Patterns of Musical Emotions and Their
Relation with Music Structure
Eduardo Coutinho, Angelo Cangelosi; University of Plymouth, UK
3AM2-R10-08
This study investigates the dynamics of psychological and physiological reactions
during music listening to determine whether diﬀerentiated psychological and physi-
ological patterns could be related with diﬀerentiated patterns of music variables.
We asked 39 participants to give continuous self-reports of the intensity (Arousal)
and hedonic value (Valence) of the emotions felt while listening to 9 pieces of western
instrumental art (classical) music. Simultaneously we recorded their Heart Rate (HR)
and Skin Conductance Response (SCR) for the full length of the pieces.
Arousal was found to increase for higher levels of tempo, loudness, mean pitch,
sharpness and timbral width, and Valence was correlated with variations in tempo
and (tonal) dissonance. Psychological and physiological reports showed that in-
creased Arousal and Valence are related with increased SCR, while increased HR
related with higher Arousal. We also found a negative relationship between Valence
and Heart Rate.
Our study supports the idea that signiﬁcant patterns of interactions between music
structure and diﬀerentiated levels of intensity and hedonic value do exist in musical
emotions. Our study also shows diﬀerentiable physiological patterns across diﬀerent
emotions, supporting the claim that physiological arousal is also a component of
musical emotions. We are currently creating a neural network model to analyse in
detail such interactions.

Psychological Study of Strong Experiences in Listening to Music:
Relationship Between Physical Reactions and Acoustic Features of
the Music
Shoko Yasuda 1 , Toshie Nakamura 1 , Maria Raluca Draguna 1 , Satoshi
Kawase 1 , Kenji Katahira 1 , Haruka Shoda 2 ; 1 Osaka University,
Japan; 2 Hokkaido University, Japan
3AM2-R10-10


                                         94
ICMPC 10                                           Wednesday 27 August 2008

Many people have strong experiences with music (SEM), although the mechanism of
SEM is still obscure. Our previous work that examined SEM quantitatively showed
that at the conclusion of the music, ratings of SEM strongly related to ratings of
ﬁve physical reactions: goose pimples, lump in the throat, shivers down the spine,
being close to tears, and arousal. Other studies also suggested that ratings of such
physical reactions for the entire music related to ratings for each part of the music.
In this light, the present study aimed at clarifying the relationship between physical
reactions and acoustic features, particularly dynamics, for each part of the music.
53 students participated in this study. They were required to listen to two musical
pieces and evaluated how strongly they experienced the ﬁve physical reactions
during each part of the music. As a result, the A-weighted sound pressure level
strongly related to goose pimples, shivers down the spine, and arousal. Additionally,
the ratings of these physical reactions during crescendo were signiﬁcantly higher
than during diminuendo. In addition, the dynamic range and magnitude of change
in loudness within crescendo aﬀected the ratings of these physical reactions. As the
ﬁve physical reactions showed a relationship with SEM, this study contributes to a
further quantitative clariﬁcation of SEM.

The Eﬀect of Repetitive Music Listening for Days on Anxiety
Reduction
Chiharu Araki 1 , Hiroshi Shimazaki 2 , Akira Imai 3 , Yoshimi Ito 1 ;
1
  Nagoya University, Japan; 2 Taisei Gakuin University, Japan;
3
  Shinshu University, Japan
3AM2-R10-12
This study investigated whether the diﬀerent eﬀect of repetitive music listening
for three days on reducing state anxiety can be seen between high and low trait
anxiety groups. Sixty undergraduate students classiﬁed into high and low trait
anxiety groups were randomly assigned to one of three conditions (preferred mu-
sic condition, selected music condition and white noise condition). One session
consisted of 3 steps: (1) completing state anxiety scale of STAI (i.e., pre-test), (2)
repetitive music listening for 10 minutes, (3) completing state anxiety scale of STAI
(i.e., post-test). This session was repeated for three days. Results showed that only
in preferred music condition, the state anxiety score in pre-tests reduced day by day.
However, no diﬀerent eﬀect between high and low trait anxiety groups was found in
all conditions. This study demonstrated that listening to preferred music repetitively
for three days reduced state anxiety, regardless of the level of trait anxiety. One
possible explanation is that the good eﬀect of listening to preferred music was
gradually accumulated as the days went on.


3AM2-R11 : Education / Performance
Room 11, 10:40 – 12:30 Wednesday 27 August 2008, Poster session

VEMUS: An Interactive Practicing Environment for Beginning Wind
Instrument Students — First Year of Evaluations
Petri Laukka 1 , Anders Askenfelt 2 , Kjetil Falkenberg Hansen 2 , Svante
Granqvist 2 , Kahl Hellmer 2 ; 1 Uppsala University, Sweden; 2 KTH,
Sweden
3AM2-R11-02
Virtual European Music School (VEMUS) is an EU-FP6 project aiming to develop
and evaluate an interactive learning environment for wind instruments. Vemus
addresses three complementary learning settings for students at beginning to
intermediate levels: self practicing, classroom environment, and distance learning.
The interactive self-practicing environment is based on a ‘virtual teacher’ interface
where the ‘teacher’ listens to students’ performances and gives feedback (including
praise and tips on how to correct errors) based on automatic acoustic analyses
of the performances. Here, we present results from the ﬁrst evaluations of the
eﬀectiveness of self-practice using Vemus with regard to a) motivation for practicing,
and b) uses of eﬃcient practice strategies. Beginning music students (12 per instru-
ment) were randomly assigned to either practicing with or without Vemus during a
four-week period. The students’ motivation and uses of various practice strategies
were assessed at baseline and weekly throughout the study period using specially
designed questionnaires. Performance improvement was assessed by observing the
frequencies of performance errors detected by Vemus. Preliminary results from


                                         95
Wednesday 27 August 2008                                                   ICMPC 10

recorder and ﬂute students showed that Vemus-students reported signiﬁcantly
increased motivation compared to the controls, and also reported increased use of
some eﬃcient practice strategies. The students further found Vemus easy to use
in everyday practicing. Analyses of the distribution of various performance errors
are under way. By introducing educational technology, Vemus has the potential to
facilitate practicing by making it more enjoyable and eﬃcient during the ﬁrst years
of playing.

Teacher-Parent-Pupil Trios: The Impact of Interpersonal
Interaction on Pupil Satisfaction, Enjoyment and Musical
Attainment
Andrea Creech; IOE University of London, UK
3AM2-R11-04
Accounts of pupil-parent-teacher relationships within context of one-to-one instru-
mental lessons suggest that human interaction has the power to shape the musical
perception and cognition of pupils. This paper tests the hypothesis that teacher-
pupil-parent interpersonal interaction, in the context of musical instrument learning,
has an impact on learning outcomes. 337 teacher-pupil-parent trios completed
quantitative measures of control and responsiveness, pupil satisfaction with lessons,
enjoyment of violin and musical attainment. A cluster analysis, using dimensions of
control and responsiveness as predictors of cluster membership, revealed six distinct
interaction types. Analyses of covariance, controlling for pupil age and testing for
diﬀerences in satisfaction, enjoyment of violin and musical attainment between the
six interaction types, revealed that where interaction was characterized by three-way
collaboration pupils experienced the greatest enjoyment and satisfaction with
lessons. Where pupils were in a subordinate relationship to a strong teacher-parent
dyad relatively higher musical attainment was found. This research suggests that
interpersonal interaction within instrumental learning contexts is a complex process
that may have implications for musical attainment as well as for enjoyment and
satisfaction experienced by pupils. The ﬁndings point to the need for future research
that investigates whether interpersonal interaction may be re-framed by teachers in
order to enhance musical perception and cognition amongst their pupils.

Increased Quality and Frequency of School-Based Music Program
Has Limited Beneﬁts on Self-Esteem and Social Skills
N.S. Rickard, P. Appelman, R. James; Monash University, Australia
3AM2-R11-06
Private music tuition has been demonstrated to have non-musical beneﬁts for
children, such as enhancing self-esteem and general intelligence. However, there has
been limited experimental investigation of the eﬀects of school-based music training.
This research explored eﬀects of a classroom based music programs on primary
school children’s self-esteem (Culture Free Self-Esteem Inventory-III) and social skills
(Social Skills Rating Scales). A Kodaly program was introduced at Preparatory level
(treatment n=52, control=25), while a string-based instrumental program was intro-
duced at Grade 3 (treatment n=55, control=37). Control schools continued to receive
their normal music curriculum. Once the eﬀects of age and school were controlled,
ANCOVAs revealed signiﬁcant time by treatment interactions for both social skills
(F2,186 =4.07, p<.05) and self-esteem (F2,140 =3.05, p=.05) in the younger cohort after
two years of the program. These eﬀects were largely attributable to improvements
in the ﬁrst year of the program, and did not persist into the second year. In older
children, MT also countered a decrease in general self-esteem experienced by the
control group in the ﬁrst year (p=.055). Interestingly, the beneﬁts observed after one
year of the music program were not evident following introduction of an alternative
novel arts-based program (juggling) into the older cohort of several schools for one
year. Therefore, while transient, the music program had beneﬁts that cannot be
attributed to the novelty of introducing a new arts-based program. Music education
may however need to be more intensive (private tuition) than is possible in large
group situations to yield more persistent beneﬁts.

Diﬀerences and Common Features of Performance Anxiety During
a Musical and a Non-Musical, Social Performance
Susanne Gorges, Georg W. Alpers, Paul Pauli; University of Würzburg,
Germany
3AM2-R11-08


                                          96
ICMPC 10                                           Wednesday 27 August 2008

According to questionnaire studies, social anxiety seems to be an important part of
musical performance anxiety, but there is a lack of studies investigating this in an
experimental design outside of the laboratory. In the current study we compared
self-reported and physiological responses in 31 music students while playing in a
concert or giving a speech in front of an audience, and during two baselines without
an audience. Within a period of 90 minutes, participants repeatedly rated their
performance anxiety as well as further positive and negative emotions. In addition,
heart rate (HR) was measured. HR and anxiety ratings were signiﬁcantly higher
in the public performances compared to the baselines, and similar in both public
performances. Performance anxiety during the speech and the concert correlated
signiﬁcantly. Whereas HR in the speech condition only correlated with anxiety
ratings, HR changes in the concert condition were also associated with positive
emotions. Results seem to conﬁrm a classiﬁcation of musical performance anxiety
as a subtype of social anxiety, but some special features should be considered,
especially with respect to possible interventions.

Eﬀects of the Relationship Between Co-Performers’ Body
Movement on Coordination in Ensembles
Kenji Katahira 1 , Toshie Nakamura 1 , Satoshi Kawase 1 , Shoko
Yasuda 1 , Haruka Shoda 2 , Maria Raluca Draguna 1 ; 1 Osaka
University, Japan; 2 Hokkaido University, Japan
3AM2-R11-10
Performers often display similar body movements with their co-performers in
ensembles. The occurrence of such distinctive behaviour may be explained by
examining the role of body movement in the coordination of ensemble performances,
which has been previously pointed out in research on body movement in musical
performances (Davidson, 2002). The aim of this study was to investigate whether
coordination during performances depends on the way in which performers use
their body movement; our focus was on ensembles, in which information about
co-performers’ body movements is available to performers. Two participants were
paired and instructed to synchronize their equal interval tapping on electronic drums
in a face-to-face situation. 5 trials were applied. The synchronization score was
examined based on their tapping timing, and body movements were investigated
based on time serial data of their body movements. We examined the relationship
between their synchronization score and the way they used their body movement
throughout all trials. The results showed that within one pair, the relationship be-
tween the participants’ body movements developed into two types, and a diﬀerence
in the synchronization scores associated with these types was observed. Based on
these results, the role of body movement was discussed.

The Relationship Between Body Types and Singers’ Breathing in
Classical Singing
Hyunhi Kim 1 , Jane Davidson 1 , Viggo Pettersen 2 ; 1 University of
Sheﬃeld, UK; 2 University of Stavanger, Norway
3AM2-R11-12
This study aimed to examine the relationship between body types and singers’
breathing in classical singing. Electromyographic activity (EMG) from the intercostals
(INT1 & INT2) and in the lateral abdominal (OBL1 & OBL2) muscles was obtained
from twenty four experienced singers (19 undergraduate students, 2 post graduate
students, 2 singing teachers who had an international solo performing career) and 1
amateur singer with two years of private singing lessons. The INT and OBL muscles
on the right side of the body were recorded by surface electromyography (EMG). The
circumference of the lower thorax (LTX) and the abdominal region (ABD) (at the navel
level) were sensed with two strain gauge sensors. The singers’ bodies were mapped
concerning: height, weight, lower thorax, mid-shoulder to mid point, mid point to
hip distance and nipple to lower rib distance. This was done in order to calculate
body type, height to weight ratio and lung volume to weight ratio of the upper
body. Three diﬀerent vocalization tasks were performed: (1) an aria, (2) a sustained
tone, and (3) a glissando. The twenty four singers sang “Voi che sa pete”, chosen
from Mozart opera, The marriage of Figaro, by the author. The muscle activity was
recorded in the performances of the repeated tasks. The hypothesis of this study
was that classical singers’ body types would aﬀect the breathing apparatus, and
that the diﬀerent body types would cause diﬀerences in the use of muscles when
breathing for classical singing. The results showed that singers with normal weight
tend to use the intercostal muscles more than those with overweight or obesity.

                                         97
Wednesday 27 August 2008                                                   ICMPC 10


3AM2-R12 : Performance / Neuroscience
Room 12, 10:40 – 12:30 Wednesday 27 August 2008, Poster session

Analysis of the Tuning Process in the Accompaniments
Noriyuki Takahashi 1 , Yoko Oura 1 , Minoru Tsuzaki 2 ; 1 Niigata
University, Japan; 2 Kyoto City University of Arts, Japan
3AM2-R12-02
In musical ensembles, it is necessary for players to anticipate their co-players’
performances and accordingly coordinate their own performances in real time. This
study was conducted to investigate real-time coordination processes in the piano
accompaniments to a solo cello performance. Two university students (pianists A
and B), majoring in piano music and experienced in accompaniment, were asked to
thoroughly practice the accompaniment part of ‘Le cygne’ composed by Saint-Saëns.
The melody played by a professional cellist was recorded in advance. In the experi-
ment, the prerecorded cello performance was presented ﬁve times through the use of
a monitor display and speakers, and upon each presentation, the pianists were asked
to play the accompaniment part of the piece on a MIDI piano. The MIDI and sound
data of each performance was recorded on a computer. An analysis of the onset
timing error of the cello and piano revealed that the total timing error of pianist
A was low, even in the ﬁrst trial, while that of pianist B was considerably higher,
and with every subsequent trial, it gradually decreased to end at same level as that
of pianist A. Moreover, pianist A demonstrated relatively lower onset timing errors
except at the beginning of the phrase, where the errors were found to be greater; in
contrast, pianist B showed a higher errors not only at the beginning but also in the
middle of the phrase, suggesting that it was not easy for her to ﬂexibly coordinate
her performance with that of the cello. Correlation analysis of the dynamics revealed
that pianist A was signiﬁcantly better than pianist B at harmonizing with the cello.
In terms of the smoothness of the tempo change, pianist A was signiﬁcantly better
than pianist B. These results suggested that the soloist’s skill is unrelated to his/her
ensemble ability, and that playing in a musical ensemble would involve some unique
skill.

Emotional Communication of a Pianist’s Intended Expression via
Acoustical and Visual Information
Haruka Shoda 1 , Toshie Nakamura 2 , Maria Raluca Draguna 2 ,
Satoshi Kawase 2 , Kenji Katahira 2 , Shoko Yasuda 2 , Mayumi Adachi 1 ;
1
  Hokkaido University, Japan; 2 Osaka University, Japan
3AM2-R12-04
We often experience diﬀerent impressions from the same piece of music when
performers change their expressions. Psychological studies suggest that performers’
intentions are communicated via acoustical and visual cues. In this light, we aimed
at clarifying whether visual information can communicate a performer’s emotional
intention. We conducted an experiment in which a pianist and 106 listeners evaluated
their intended and perceived emotional expressions of two renditions (deadpan and
artistic) for two musical pieces (“Etude Tableaux Op.39-1” and “Prelude Op.32-5”
by Rachmaninoﬀ). As a result, the core emotional components intended by the
pianist were perceived accordingly by the audience in the sound only, and in the
matched sound and vision modes. Although visual information could communicate
such components, there were no visual eﬀects on emotional communication of the
performer’s intentions in the matched sound and vision mode. In contrast, visual
information aﬀected the perception of mechanical performance.

Auditory Feedback-Based Error Monitoring Processes During
Musical Performance: An ERP Study
Kentaro Katahira, Dilshat Abla, Sayaka Masuda, Kazuo Okanoya;
RIKEN Brain Science Institute, Japan
3AM2-R12-06
Auditory feedback is important in detecting and correcting errors during sound
production behaviors. In the context of vocal production, a forward model, in
which a prediction of action consequence (corollary discharge) is created, has been
proposed to explain the dampened activity of the auditory cortex while producing
self-generated vocal sounds. It is unclear how auditory feedback is processed and


                                          98
ICMPC 10                                           Wednesday 27 August 2008

what neural substrate underlies the process during musical performances. Clarifying
the similarities and diﬀerences between the modules for vocal production and
music performance can help clarify the brain mechanisms involved in both cognitive
functions. We investigated the neural correlates of human auditory feedback-based
error detection using event-related potentials (ERPs) recorded during musical perfor-
mances. Keyboard players of two diﬀerent skill levels played simple melodies using
a musical score. During the performance, the auditory feedback was occasionally
altered. Subjects with early and extensive piano training evoked a negative ERP
component N210, which was absent in non-trained players. When subjects listened
to music that deviated from a corresponding score without playing the piece, N210
did not emerge but the imaginary mismatch negativity (iMMN) did. N210 may reﬂect a
process of mismatch between the intended auditory image evoked by motor activity,
and actual auditory feedback.

Neuromagnetic Alpha Desynchronization Reﬂects Auditory
Processing to Musical and Non-Musical Sound Diﬀerently in
Children
Takako Fujioka, Bernhard Ross; Rotman Research Institute, Canada
3AM2-R12-08
It is known that alpha-band (8–13 Hz) activities in auditory cortex are modulated by
auditory stimulus in children, as in adults. The current study aims to retrospectively
investigate alpha-band activities in our previous magnetoencephalographic (MEG)
data which have shown that non-oscillatory brain response in children matures
diﬀerently according to the stimulus-type (musical and non-musical sound) and
child’s musical experience in Suzuki music lessons (Fujioka, Ross, Kakigi, Pantev,
& Trainor, 2006). Time-frequency analysis using Wavelet Transform was applied to
single-trials of source waveforms observed from left and right auditory cortices in
response to the violin tone and noise stimulus. The results show prominent alpha
power decrease with a clear dissociation between the upper (12 Hz) and lower (8 Hz)
range for both stimuli. Interestingly the alpha desynchronization lasted longer in the
upper than the lower alpha band only for the violin tone, although the maturational
change and group diﬀerence were absent. In general the time courses of alpha
(onset around 300 ms, peak at 500 ms, oﬀset after 1500 ms) were similar to those
previously found for older children and adults with auditory memory related tasks.

Segmentation of Musical Sequence with Statistical Regularities: An
Event-Related Potentials Study
Dilshat Abla, Kentaro Katahira, Kazuo Okanoya; RIKEN Brain Science
Institute, Japan
3AM2-R12-10
Language and music share much in common structurally. To extract chunk as well as
their organization from the speech stream or music, humans must possess eﬃcient
computational procedures. Several behavioral experiments have reported evidence
that infants and adults readily learn statistically deﬁned patterns in auditory input
sequences. To investigate the neural processes involved in online segmentation
and statistical learning, we recorded ERPs while adult participants were exposed to
continuous, auditory sequences, the elements of which were organized into “tritone
words” that were sequenced in random order, with no silent spaces between them.
Participants were listening to three 6.6-minute sessions of sequences. Results showed
that word onset (initial tone, the less-predictable position) elicited the largest N400
in the early learning session of high learners, but in middle learners, the word-onset
eﬀect was elicited in a later session, and there was no eﬀect in low learners. The
N400 amplitudes were correlated with the transitional probability of continuous tone
streams. The results suggest that the N400 eﬀect indicates not only online word
segmentation but also the degree of statistical learning. This study provides insight
into the neural mechanisms underlying online statistical learning processes.

The Brain Activation of Absolute Pitch Possessors: A
Near-Infrared Spectroscopy Study
Shiho Miyazawa 1 , Shozo Kojima 2 ; 1 Waseda University, Japan; 2 Keio
University, Japan
3AM2-R12-12
This study examined the characteristics of the cerebral hemodynamic responses to
musical sound. We measured absolute pitch (AP) and non-absolute pitch (NAP) by

                                         99
Wednesday 27 August 2008                                                  ICMPC 10

near-infrared spectroscopy (NRS). Absolute pitch (AP) is the ability to identify the
pitch without any external reference. Previous studies, from neuroimaging results,
has revealed that Broca’s area is associated with phonetic and verbal process and,
right medial temporal lobe is associated with melody retrieval. However, behavioural
studies have shown that when they retain musical pitch, NAP group use sensory
encoding, whereas AP group use verbal encoding. By using pitch memory task, we
examined hemodynamic response during the task, and we focused Broca’s area and
the homologue in the right hemisphere. 23 volunteers participated this experiment
and were classiﬁed by pitch naming test. The result is that AP’s score of pitch
memory test is greater than NAP’s. Furthermore, a larger response to the AP group
than to the NAP group changes occurred in Broca’s area. These results demonstrated
that the diﬀerence strategy was used between AP group and NAP group in pitch
memory test by using neuroimaging method. We suggested that a new method of
investigating music perception using NIRS.


3AM2-R13 : Neuroscience
Room 13, 10:40 – 12:30 Wednesday 27 August 2008, Poster session

Cortical Processing of Consonance of Pure-Tone Dyads: An
Evoked Potential Study
Kosuke Itoh 1 , Shugo Suwazono 2 , Tsutomu Nakada 1 ; 1 University of
Niigata, Japan; 2 National Hospital Organization Okinawa Hospital,
Japan
3AM2-R13-02
Multiple levels of auditory processing are believed to contribute to perception
of consonance in music. Regarding peripheral processing, consonance has been
successfully formalized as the absence of “roughness” perceived when two slightly
mistuned simple tones are unresolved in the cochlea. By contrast, little is known
about the later central processing. To investigate this issue, long-latency components
of auditory evoked potentials (EPs) were recorded while listeners who had received
ﬁve to sixteen years of training in Western music listened to dyads comprising
two pure tones paired at various intervals. Symmetrically over frontal scalp, the
amplitude of auditory EP was signiﬁcantly more negative for dissonance than for
perfect consonance at around the N2 latency, and the EP amplitude for imperfect
consonance was intermediate. This eﬀect was not signiﬁcant at the N1 and P2
latencies, by contrast. Thus, the EP was modulated by the interval of pure-tone dyads
in a manner that could not be accounted for by the cochlear “roughness theory”
of consonance, which states that dissonance (or “roughness”) of simple-tone dyads
monotonically decreases with frequency separation. In addition to peripheral mech-
anisms, cerebral cortical processing likely contributes to perception of consonance
in listeners who had received training in Western music.

Brain Activities Evoked by Musical Intervals in an Octave:
Dissonant or Consonant
Reiko Shiba 1 , Kazuhiro Hirai 2 , Iku Nemoto 2 ; 1 RIKEN Brain Science
Institute, Japan; 2 Tokyo Denki University, Japan
3AM2-R13-04
The unpleasantness brought by dissonant cords is common to many people. This
feeling is considered due to the beat of sounds. We suppose some innate structures
in our brain to cause this feeling. In this study, we investigate the basic auditory
processing mechanisms of the sensory dissonance perception of musical chords.
(Exp. 1) Intervals synthesized of two pure tones ranging from unison to octave
were presented to 13 adult musically naïve volunteers. Evoked magnetic ﬁelds for
the intervals were recorded by whole head MEG system (Neuromag122). Eighty
responses to each interval were averaged and low-pass ﬁltered and then analyzed.
(Exp. 2) The evoked magnetic ﬁelds for 3-tone musical chords (dissonant; C-C# D,
C-C#-B, consonant; C-E-G, C-F-A) were recorded and analyzed similarly. A signiﬁcant
large response to the dissonant interval (minor second) was observed in the both
auditory cortex from 200 to 400 ms after sound presented in experiment 1. The
signal source of this activity existed in the vicinity of that of N1m. A relative large
response to dissonant 3-tone musical chord (C-C#-D) was observed from 300 to 400
ms in experiment 2. These responses were supposed to be speciﬁc to the dissonant
musical intervals.


                                         100
ICMPC 10                                              Wednesday 27 August 2008

The Time Needed to Make Decision for Musical Preference and
EEG Activities
Tatsuya Iwaki, Tomohiko Makimori; Hiroshima International
University, Japan
3AM2-R13-06
Likes and dislikes can be judged only from listening to the beginning of a music
piece. Presented study tried to understand the question that how long did listeners
need to listen to music to make decision for musical preference. Furthermore,
there was a possibility that the brain activities during the time taken to decide the
preference diﬀered from that after the period. Twenty participants were asked to
push the button when they could judge the preference for a music piece. Presented
pieces were all classical music that were presented for each 100 s. Typical 21 channel
EEGs were recorded through the experimental session. Mean time to need to judge
the musical preferences were 25 s. This means listeners are able to anticipate their
preference for each music piece at the beginning. Analyzing the correlation between
EEG data and the score of musical preference, alpha band amplitude at frontal area
during 25 s from the presentation of music pieces was lateralized with the greater
score ratings of preference. Relating this diﬀerence of EEG amplitude, EEG coherence
between left and right frontal-temporal sites was also increased. EEG changes implied
the activation of frontal brain area reﬂecting the analytic state of musical preference.

Frontal Brain Activation During Listening to Negative vs. Positive
and High vs. Low Arousal Music: The Moderating Eﬀects of
Impulsivity and Neuroticism-Anxiety Dimensions of Personality
Kari Kallinen; Helsinki School of Economics, Finland
3AM2-R13-08
A considerable body of research has found a relationship between emotions and
frontal cortical activity. Some studies suggest that left and right frontal alpha
activation is related to the experience of positive and negative emotions, respectively
(“valence hypothesis”), while others have suggested that the right hemisphere
processes both positive and negative information (“right hemisphere hypothesis”).
In addition to emotional characteristics of music, it has also been suggested that
there may be individual diﬀerence variables that aﬀect the nature and magnitude
of responses to music, although only a few studies have examined this issue in
connection with music and brain activity. The purpose of the present study was to
test the valence and right hemisphere hypothesis by examining the frontal brain
activity during positive (i.e., joyful) vs. negative (i.e., sad and fearful) and high (i.e.,
joyful and fearful) vs. low arousal (i.e., sad) music. In addition we wanted to study
whether the personality dimensions (i.e., impulsivity [Imp] and neuroticism-anxiety
[N-Anx]) moderates the activity. The present study gave only partial support for the
valence theory of frontal brain asymmetry, and demonstrated that the individual
diﬀerences (such as personality) may have very fundamental eﬀects on the brain
activity observed in individuals. The results will be discussed from the valence and
right hemisphere hypothesis point of view and the role of individual diﬀerences in
brain responses to emotional music will be further explored.

Anterior Portion of Temporal Lobes Participates in the Perception
of Chords: A PET Study
Masayuki Satoh; Mie St. Cross Hospital, Japan
3AM2-R13-10
I had experienced a case of amusia who showed the impairment of the discrimination
of isolated chords (Satoh 2005). Using positron emission tomography (PET), I ascer-
tained the activation of the anterior portion of the temporal lobes in the perception
of chords in nonmusicians. Eleven right-handed male volunteers performed two
kinds of musical tasks of identical musical pieces: harmony-listening and soprano
part-listening. Six cerebral blood ﬂow (CBF) measurements were determined for
each subject — three during the harmony-listening and three during the soprano
part-listening task. Employing the 15 O-labeled water (H2 15 O) intravenous bolus
technique, I collected emission data for 90 seconds in each measurement following
the intravenous bolus injection of 15∼17 mL (40 mCi) of H2 15 O. PET data analysis
was performed by using an automated PET activation analysis package (Minoshima
1993). Subtraction of soprano-part listening from harmony-listening task showed
that signiﬁcant increases in regional CBF were found in the anterior portion of the
bilateral temporal lobes, bilateral cingulated gyri, and the cerebellum. Based on the


                                           101
Wednesday 27 August 2008                                                  ICMPC 10

results of case and activation study, it is reasonably concluded that the anterior
portion of the bilateral temporal lobes is vital in the perception of chords.


3AM2-R16 : Demonstration III
Room 16, 10:40 – 12:30 Wednesday 27 August 2008,

“Seeing Harmony”: A Computer Graphics System for Visualizing
Harmony
Chikashi Fujimoto, Hiroo Konaka, Takefumi Hayashi, Norman D.
Cook; Kansai University, Japan
3AM2-R16-1
Two independent acoustical features (dissonance and tension) contribute to the
overall sonority of musical triads. These features can be calculated from the relative
size of the intervals among the fundamental frequencies and their upper partials in
any chord, and then plotted as 3D graphs. The strength of the major/minor “modal-
ity” of chords is also determined by the relative size of the two intervals in each
three-tone combination. Precise, quantitative evaluation of these harmonic features
allows for the visualization of chords both in terms of their resolved/unresolved
character (“instability”, deﬁned as the sum of interval dissonance and triad tension)
and in terms of their aﬀective valence (the relative size of the upper and lower
intervals among partial triads). Based on this model of harmony perception, we
have developed an OpenGL visualization software system which displays harmonies
as dynamic 3D patterns in response to the input of musical chords. The software
has both visual and auditory output, allowing the user to perceive the relationship
between auditory features and a visual analog that is constructed on the basis of
the acoustical properties of the chords. Unlike familiar “screensaver” software that
presents visual patterns in response to music, the Seeing Harmony software has a
rigorous psychoacoustical foundation.


3AM2-R17 : Demonstration IV
Room 17, 10:40 – 12:30 Wednesday 27 August 2008,

A Psychoacoustical Toolbox for Sound/Music Analysis and Sound
Design/Music Creation
Alexandre Torres Porres; University of São Paulo, Brazil
3AM2-R17-1
This demo session is about a Puredata Library of Patches. This project is still at its
initial phase on a doctorate degree program at USP, under the supervision of Prof. Dr.
Fernando Iazzetta and Prof. Dr. Marcelo Queiroz. The ﬁrst Patch of this library was
developed during a Master’s degree, and was centred on the Psychoacoustical Model
of Roughness. The software was adopted to analyse spectrum of sounds and derive a
scale of consonant and dissonant steps. Such a procedure enabled the analysis behind
the Poster presentation from the same author at ICMPC 10. Thus, more information
about it can be found on the full paper submitted to the Proceedings. The software
and user manual can be downloaded from http://porres.googlepages.com/home.
Besides the Scale derivation from the Spectrum, this tool also analyses Roughness of
audio signal over time. Because of its creative possibilities regarding the perception
of tuning, the software has a section of Adaptive Tuning modules that work as
pitch correctors, and were used with theremin-like sensors. The current state of
development aims to include other Psychoacoustical Analysis of sound/music, as
well as creative interactive tools for sound design and music creation.


3AM2-R18 : Demonstration V
Room 18, 10:40 – 12:30 Wednesday 27 August 2008,

Creating the Expression of Cantablile with Software
Tomoyasu Taguti; Taguti Laboratory of Computation and Analysis,
Japan
3AM2-R18-1
Cantabile, or singing, is one of directions concerning the general character of a


                                        102
ICMPC 10                                           Wednesday 27 August 2008

piece or section of music, which necessarily directs the player to convey the implied
emotion to the audience in his/her performance. Like other directions such as
dolce, amoroso, agitato, appassionato, and others, the meaning is totally holistic as
compared with the performance marks indicating the tempo, dynamics, articulation,
etc. that are more concrete and directly related to the performance practice. The
author will present a descriptive method to realize the desired musical expression
with software. The essential point is a hierarchical superposition of performance
variables corresponding to the dynamics, agogics, articulation, onset-shift, and
pedaling. These performance variables, deﬁned in the form of envelope functions in
the beat variable, can be given to any musically meaningful units as well as to their
subgroups, and sub-subgroups . . . at arbitrary depths, of the entire note structure.
The demonstration will be focused on the cantabile expression for Andante cantabile
con espressione, Second Movement of Mozart’s Piano Sonata No. 9 in A minor,
KV 310, by making use of several simple envelop forms, or envelop elements, of
the performance variables. Note that the set of the instances of the performance
variables in hierarchical structure is viewed as a decomposition of the desired
(complex) musical expression.


3PM1-R02 : Timing and Performance
Room 2, 13:30 – 15:30 Wednesday 27 August 2008, Oral session

Accuracy in Time-Interval Production with Contextual Sequences :
A Comparison Between Trained and Untrained People
Tomoko Hashida, Takao Sato; University of Tokyo, Japan
3PM1-R02-1
The present study aimed at investigating whether musical training experiences
eﬀect on the accuracy of time-interval production when contextual sequences are
presented, which may provide clues for the prediction of the beat timing and mental
subdivision. We are interested in whether the abilities to use contextual sequences
for producing time-intervals are diﬀerent between trained and untrained people or
not. In the experiment, participants were required to produce 4000ms inter-onset
of tapping-intervals (ITIs) by using key. Seven types of contextual sequences were
generated depending on the length of key press. Standard deviations of ITIs were
calculated as a measure of production accuracy. The results indicate that people can
produce time-intervals with contextual sequences accurately regardless of musical
proﬁciencies and the types of contextual sequences don’t eﬀect on production
accuracies. In other words, the abilities to produce time-interval by making use
of hints for the prediction of beat timing and the mental subdivision provided by
sequences are not diﬀerent between trained and untrained people.

Multi-Feature Modeling of Pulse Clarity from Audio
Olivier Lartillot, Tuomas Eerola, Petri Toiviainen, Jose Fornari;
University of Jyväskylä, Finland
3PM1-R02-2
This paper deﬁnes pulse clarity as an introspective understanding of the quality of
the rhythmic structures not merely reduced to questions related to synchronization
capabilities. The objective of this study is to establish a composite model explaining
pulse clarity judgments from the analysis of audio recordings, and to ground the
validity of the model through listening tests. The models used in this study are
based on a range of musical features usually regarded as important in the perception
of pulse clarity. Rhythmic periodicity is estimated via the autocorrelation of the
amplitude envelope of the audio waveform. Statistical characterization of the
autocorrelation function indicates the prominence of the main pulsation. Harmonic
relations between the main pulsation and the secondary periodicities may also
contribute to the rhythmic clarity. Besides periodicity, descriptors related to the
amplitude envelope itself are also considered. The models have been written in
Matlab using MIRtoolbox. To evaluate the models, 25 participants rated the pulse
clarity of one hundred excerpts from movie soundtracks. The mapping between the
model predictions and the ratings is carried out via regressions. Almost a half of the
listeners’ rating variance can be explained with a combination of periodicity-based
and non-periodicity-based factors.




                                        103
Wednesday 27 August 2008                                                  ICMPC 10

Eﬀects of Physiological Arousal on Performing Tempo and Artistic
Expression in Pianists
Michiko Yoshie, Takeshi Hirano, Akito Miura, Kazutoshi Kudo,
Tatsuyuki Ohtsuki; University of Tokyo, Japan
3PM1-R02-3
The present study examined whether pianists’ heightened physiological arousal
in stressful performance situations can alter their performing tempo and artistic
expressions. Seven music majors aged 21–37 years (M = 28.4, SD = 5.9) performed
Chopin’s Etude Op. 25-5 on a grand piano under two conditions: (a) In the rehearsal
condition, participants played the piece alone in a practice room. (b) In the competi-
tion condition, participants performed in front of ﬁve judges and a large audience.
During their performances, we recorded the mean heart rate (HR), sweat rate (SR),
and electromyographic (EMG) activity from four muscles in the left arm and shoul-
der. The eﬀectiveness of arousal manipulation was conﬁrmed by the signiﬁcantly
increased HR, SR, and EMG activity in the competition condition. The total duration
of the piece signiﬁcantly decreased from the rehearsal to competition condition.
Additionally, participants tended to hold the note under a fermata relatively longer
in the competition condition, presumably in an attempt to relieve muscle fatigue.
We conclude that the heightened physiological arousal due to psychological stress
can accelerate the internal tempo (Boltz, 1994) and hence performing tempo in
musicians, leading to the impairment of performance quality. Furthermore, the
associated increases in muscle activity can alter the artistic expressions.

Eﬀect of Pianists’ Expressive Intention on Amount and Type of
Body Movement
Marc R. Thompson, Geoﬀ Luck; University of Jyväskylä, Finland
3PM1-R02-4
Body movement displayed in music performance is said to be an overt manifestation
of the musician’s expressive intentions and goals regarding the music being played.
We are interested in studying if diﬀerent levels of expression result in diﬀerent
amounts of body movement and gestures. For this study, musicians were asked in
multiple sessions to play an excerpt from the same piece using three diﬀerent levels
of expression while their movements were recorded using an optical motion capture
system. Statistical tests show that an increasing amount of expression resulted in
more body movement and that the amount of expression and physical movement
were inter-twined. Also, we present data suggesting that the head and shoulders
travelled a further distance overall, and showed bigger diﬀerences between perfor-
mance manners, compared to the ﬁngers, wrists and lower back. We hypothesize
that this is related to the contrasting roles these parts of the body play in piano
performance.


3PM1-R03 : Music Listening III
Room 3, 13:30 – 15:30 Wednesday 27 August 2008, Oral session

Musical Preferences of Secondary and Post-Secondary Students in
Singapore
Sun-Hee Chang 1 , Eddy Chong 2 ; 1 Seoul National University, Korea;
2
  Nanyang Technological University, Singapore
3PM1-R03-1
This study seeks to investigate the eﬀect of familiarity, age, gender, ethnicity, and
academic group as well as their interactions on the musical preferences of secondary
and post-secondary students in Singapore. The students whose responses were
analyzed (n=1529) ranged from age 12 to 23. Five music categories were used
in the listening survey — Jazz, Rap, Rock, Traditional music (comprising African,
Chinese, Indian, Indonesian, Japanese, and Malay) and Western classical music.
Mean preference results showed that western classical music was the most familiar
(M=3.58, SD= .86) and most preferred (M=3.47, SD=.80) whereas traditional music
was the least familiar (M= 2.80, SD= .87) and least preferred (M=2.41, SD= .66). The
correlation between familiarity and preference is signiﬁcant for all music categories;
ANCOVA results further showed that familiarity of the corresponding music category
was the most inﬂuential factor aﬀecting preferences in all cases. Of the other four
factors examined, interaction eﬀects between gender and academic group, ethnicity
and academic group, as well as gender and ethnicity were found to be signiﬁcant,


                                        104
ICMPC 10                                           Wednesday 27 August 2008

aﬀecting the diﬀerent music categories in diﬀerent ways. In terms of main eﬀects,
age was the only factor that did not signiﬁcantly interact with the other factors in
inﬂuencing preferences; on its own, it seemed to aﬀect only students’ preference for
Jazz. Of the ﬁve music categories, Rap which was ranked second in terms of pref-
erence was more strongly preferred by the lower academic ability secondary-school
groups (unlike with all the other music categories), and by both Indians and Malays
(as with Traditional music but not with the others).

Relations Between Music Preferences, Personality Characteristics
and Values in a Turkish Sample
Hasan Gurkan Tekman, Gaye Goklu, Vuslat Saglam; Uludag
University, Turkey
3PM1-R03-2
Rentfrow and Gosling (2003) factor-analyzed reported preferences for musical genres
and found that four dimensions (upbeat and conventional, intense and rebellious,
energetic and rhythmic, reﬂective and complex) could summarize musical prefer-
ences of US samples. Preference for each one of these dimensions also had reliable
correlations with some person variables including personality, political, and cognitive
characteristics. We investigated the generality of the four dimensions discovered by
Rentfrow and Gosling (2003) in a diﬀerent culture and also aimed to see whether
musical genres speciﬁc to this culture would ﬁt into the same classiﬁcation. Further-
more, we examined the relations between music preferences and person variables
including value orientations. The four factors that were observed by Rentfrow and
Gosling (2003) emerged in the data. However, some of the musical genres speciﬁc
to Turkey combined in a ﬁfth factor. Not only personality characteristics but also
value orientations were related to music preferences. The correlations of music
preferences and person variables showed that there may be diﬀerences as well as
commonalities in the functions the same genres serve in diﬀerent cultures.

Music Preference and Sensation Seeking Tendency in Various Age
Groups
Marek Franek; University of Hradec Králové, Czech Republic
3PM1-R03-3
Several studies reported that individuals, who score high in the sensation-seeking
tend to prefer certain musical genres, in particular hard rock, punk, heavy metal,
and reggae music. The association between sensation seeking and music preference
was studied only in adolescents or young adults. There is a question, whether this
tendency is also manifested in another age groups. Participants (N=521) were asked
to complete the Sensation Seeking Scale V and the test of musical preference. The
multiple regression analysis revealed that jazz, electronica/dance music (dance,
techno, jungle etc.) and rock music was positively related to sensation seeking (SS).
It was shown that in (1) adolescents and younger adults (16–25 years) preference
of jazz and electronica/dance music was positively associated to SS, in (2) adults
from 26 to 39 the preference of rock and electronica/dance music was positively
associated to SS, and in (3) adults from 40 to 57 the preference of jazz and rock was
positively associated to SS. The results showed that in adolescent and young adults
group the preference of electronica/dance music and jazz was related to the high
level of SS, while in middle age individuals SS was associated with the preference of
rock. It seems that for middle age individuals rock could have a similar arousing
eﬀect as electronica/dance music for younger people.

Relating Personality to Reported Music Preferences and Listening
Behaviour
Greg Dunn, Boris de Ruyter; Philips Research, The Netherlands
3PM1-R03-4
Though music is a ubiquitous and ingrained aspect of our daily lives, little is known
about why people become fond of one music genre versus another. We aim to sup-
port previous results relating reported music preferences to personality, and further
extend its validity to listening behaviour. To accomplish this, 395 participants (335
males) completed measures for music preferences and personality. Their listening
behaviour was tracked using a music database for a minimum period of 3 months.
Results indicated participants’ reported music preferences were positively correlated
to listening behaviour for 13 of 14 genres. Correlations between music preferences
and personality were not consistent with previous research, however; neither for


                                         105
Wednesday 27 August 2008                                                  ICMPC 10

reported music preferences nor listening behaviour. Thus, while reported music
preferences reliably indicated listening behaviour, further study is required to fully
establish how music preferences and listening behaviour are related to personality.


3PM1-R04 : Emotion in Music III
Room 4, 13:30 – 15:30 Wednesday 27 August 2008, Oral session

Dynamic Auditory Parameters and Perceived Musical Tension
Roni Y. Granot 1 , Zohar Eitan 2 ; 1 Hebrew University, Israel; 2 Tel Aviv
University, Israel
3PM1-R04-1
Though the perception of musical tension has recently received considerable atten-
tion, the eﬀect of interactions among auditory parameters on perceived tension has
hardly been examined systematically. In this study, 72 participants (30 musically
trained) listened to short melodic sequences which combined manipulations of
pitch direction, pitch register, loudness change and tempo change, and rated each
sequence for overall tension level and for direction of tension change (increasing or
decreasing). For overall tension, repeated measures ANOVAs show main eﬀects of
loudness, pitch direction, and pitch register (lower more tensional), and several sig-
niﬁcant interactions (e.g., tempo and loudness, pitch direction and register). Tempo
had only a weak eﬀect on overall tension ratings but signiﬁcantly aﬀected ratings
of tension change. Results indicate that the mutual eﬀect of auditory parameters
on perceived tension is often strongly interactive, rather than additive. In addition,
pitch register is shown as a strong determinant of perceived tension; in contrast to
previous ﬁndings, lower register enhances perceived tension. We discuss results in
light of an ecological model, bearing on cross-modal and aﬀective connotations of
auditory qualities, in which the degree of tension is aﬀected by auditory cues for
impending threat.

Aﬀective Response to Tonal Modulation
Marina Korsakova-Kreyn, W. Jay Dowling, Joseph Dunlop; University
of Texas at Dallas, USA
3PM1-R04-2
This study investigated aﬀective response to modulations to all 12 major and minor
steps of the Western tonal schema. The participants were asked to listen to 48
brief chordal progressions and to indicate the intensity of their aﬀective response to
the concluding part of each progression on six bipolar adjective scales: Happy-sad,
Pleasant-unpleasant, Strong-weak, Firm-wavering, Bright-dark, Warm-cold. These
pairs represent three dimensions: the two semantic diﬀerential scales, Evaluation
and Potency, and a category involving synaesthetic metaphors often involved in
music perception. There were diﬀerentiated aﬀective responses to the diﬀerent
modulations that depend on key proximity and mode, and, perhaps, stylistic famil-
iarity. Listeners were sensitive to modulation to the Subdominant and Dominant,
recognized the “negativity” of the tritone, and sensed as “positive” the popular
modulations into major keys that ascend a half step or descend a major third. This
investigation agrees with other studies on aﬀective response to the major and minor
modes. In addition, the study provides evidence that the Lerdahl-type system of
tonal relationships involved in functional harmony needs to be supplemented with
considerations of the importance of a semitone approaching the root of the tonic
triad or the mode-deﬁning third.

Music Emotion Classiﬁcation by Audio Signal Analysis: Analysis
of Self-Selected Music During Game Play
Don Knox, Gianna Cassidy, Scott Beveridge, Raymond MacDonald;
Glasgow Caledonian University, UK
3PM1-R04-3
Music emotion classiﬁcation algorithms seek to classify music ﬁles automatically
by means of audio signal analysis. An overview of these methods is given, and an
emotion classiﬁcation algorithm is applied to the preferred music choices made
by test subjects during a game play experiment. Results from the experiment are
presented, in which test subjects were exposed to 3 sound conditions: preferred
music, game soundtrack and experimenter-selected music. Obtained measures
are heart rate, pedometer rate, game score, completion time and enjoyment. The

                                        106
ICMPC 10                                           Wednesday 27 August 2008

preferred music choices from the experiment are analysed and classiﬁed according to
mood cluster, valence and arousal. Obtained measures for these music classiﬁcations
are discussed, as are the implications for automatic mood classiﬁcation in choosing
music for future experiments. It is noted that such mood classiﬁcation schemes are
nascent. A means by which these schemes may be made more robust is proposed,
and initial results toward this goal are presented.

Emotional Cues in Knocking Sounds
Renzo Vitale 1 , Roberto Bresin 2 ; 1 RWTH Aachen University, Germany;
2
  KTH, Sweden
3PM1-R04-4
The object of this research is to describe how temporal and dynamic cues in knocking
sounds can communicate emotions, just like in expressive musical performances.
An experiment has been conducted where several emotions were supposed to be
expressed by diﬀerent performers. Participants were asked to knock on a wooden
door according to instructions. Knocking sounds have been recorded both outside
and inside the room, and afterwards they were rated in listening tests.
Together with acoustic measurements, arm movements during the knocking action
were detected through a motion capture system, so that the body behaviour (visual
component) could be correlated to the sound evaluation (acoustical component).
Based on previous research on arm movements and music performance, ten diﬀerent
emotions were selected for investigation.
Results conﬁrm the use of the same strategies in both expressive everyday body ges-
tures and expressive music performance. Listeners were able to perceive emotions to
a large extent. Strong similarities between the use of acoustical features in knocking
and music performance were found.
The intended emotions were generally perceived correctly. Among the relevant
acoustical features extracted from the recordings, rhythm and IOI as well as loudness
revealed to be strong cues.


3PM2-R02 : Development II
Room 2, 15:45 – 17:15 Wednesday 27 August 2008, Oral session

Young Children’s Rhythmic Behaviour in Singing: The Inﬂuence of
Mother Tongue on Their Development
Nozomi Azechi; IOE University of London, UK
3PM2-R02-1
Diﬀerent rhythmic behaviour in musical performances was reported in some cross-
cultural studies. Dotted rhythm is one of the ‘need to be trained’ musical skills among
Japanese music students, though it is a favorite rhythm for use in composition and
improvisation among Japanese children. A 3:1 ratio dotted rhythm and a 2:1 ratio
were found to be confused in Japanese students and children’s musical performances.
It has been believed that this phenomenon was caused by the eﬀect of the Japanese
language phonetic system and its rhythm. However, the developmental process
of this rhythmic behaviour and inﬂuence of language rhythm has not been examined.
Japanese and English children aged 3–6 singing: “Twinkle, twinkle” (song1) and “If
You’re Happy and You Know It” (song2) were analyzed by nPVI. Preliminary analysis
of song1 indicates that there is a diﬀerence between the two groups at age 4 and
above. Further diﬀerence is expected on dotted rhythm in song2. However, the
diﬀerence was found only between the age 5 groups. The nPVI value of the English
3-year-old group was lower than that of the Japanese. The results from the song1
suggest that there is a tendency for language eﬀects to become more evident as the
children grow up. However, many of singers from the English 3-year-old group sung
dotted rhythm of the same length, which is diﬀerent from the rhythm of mother
language. There is a contradiction between results from two songs.

Development of Pitch Processing in Auditory Cortex Between 2
and 4 Months of Age
Laurel J. Trainor, Chao He; McMaster University, Canada
3PM2-R02-2
In the ﬁrst months after birth, infants attend to the pitch patterns of music and


                                         107
Wednesday 27 August 2008                                                     ICMPC 10

speech. While it is known that auditory cortex undergoes large maturational changes
between 2 and 4 months of age, little is known about how this aﬀects pitch processing.
We are using event-related potentials (ERPs) derived from EEG recordings to track the
development of the brain’s response to changes in pitch and pitch patterns. We have
measured ERP responses to (a) changes in a repeating tone of one pitch, (b) changes
in a pitch pattern (order of the tones), (c) changes in a melody that is transposed
to a new key, and (d) the pitch of the missing fundamental. Responses to a change
in the pitch of a single tone change dramatically between 2 and 4 months of age,
with 2-month-olds showing an increase in a positive slow wave and 4-month-olds a
negative mismatch (MMN) response similar to adults. By 4 months, robust responses
to changes in pitch pattern and to the pitch of the missing fundamental can be seen.
We conclude that pitch processing in auditory cortex changes dramatically between
2 and 4 months, with adult-like responses emerging around 4 months.

A Day in the Life Project: Everyday Musical Experiences Among
Two Year Old Girls in Seven Diﬀerent Locations
Susan Young; University of Exeter, UK
3PM2-R02-3
Interest in the everyday musical experiences of children is growing, receiving impetus
from recent activity in the social and anthropological study of childhood. In this
study, a single day of home-care for seven two-and-a-half-year-old girls living in dif-
ferent countries (Canada, Italy, Peru, Thailand, Turkey, UK, USA) was video-recorded
by project researchers. In an ecological approach to research other data was also
collected including preliminary interviews, ﬁeld notes and mappings of the home
environment. The project has been long-term and the music strand [one of many
strands being researched by the project team] has evolved into several lines of interest
including: technology assisted musical activity; music as parenting practice; and the
home as a site for young children’s entertainment through music. This presentation
will give an overview of these lines of interest and illustrate them with examples from
the video data. Developmental psychology has tended to be insuﬃciently interested
in the wider cultural processes which contribute to children’s musical development.
Ethnomusicology and the sociology of music have been insuﬃciently interested in
children’s musical activities. It will be argued that the integration of interdisciplinary
accounts of young children’s musical experiences is essential if we are to acquire
fuller understandings of their musicality, the diversity of musical practices and how
children develop musically within heterogeneous contexts.


3PM2-R03 : Audio-Visual Interaction
Room 3, 15:45 – 17:15 Wednesday 27 August 2008, Oral session

Experiencing Musical Performance: The Eﬀect of a Visual
Component on Appreciation of Complex Musical Sound
Scott D. Lipscomb, Guerino Mazzola; University of Minnesota, USA
3PM2-R03-1
The purpose of this study was to determine whether the presence of the visual
aspect of music performance enhances to a signiﬁcant degree the experience of the
listener-viewer in response to a variety of musical styles.
Participants in the study were assigned to one of three conditions (audio only, audio
with visualization, or audio with visual performance) and were then presented with
examples selected from each musical genre. Musical examples were selected from
both traditional (relatively “simple”) and modern (“complex”) musical contexts. The
stimuli were presented in a random order to each group of participants. After
experiencing each stimulus, every participant responded on a series of 8-point verbal
scales. Selection of these verbal scales was determined by a preliminary study, the
results of which were presented at SMPC 2007.
Based on the results of the present study, the authors will propose potential ap-
plications for visual analogues of musical examples from a wide array of musical
repertoires to facilitate the music learning process and inﬂuence the evolution of
musical preferences.




                                          108
ICMPC 10                                           Wednesday 27 August 2008

The Eﬀects of Diegetic and Non-Diegetic Music on Viewers’
Interpretations of Film
Siu-Lan Tan 1 , Matthew P. Spackman 2 , Elizabeth Wakeﬁeld 1 ;
1
  Kalamazoo College, USA; 2 Brigham Young University, USA
3PM2-R03-2
245 participants viewed a ﬁlm excerpt, accompanied by diegetic music (presented
as if originating from within the world of the ﬁlm characters) or the same music
presented as if an external dramatic score. Diegetic and non-diegetic presentations of
the same music led to diﬀerent interpretations with respect to characters’ emotions,
relationship between characters, and the overall mood of the scene.

The Eﬀect of Music on the Fear Emotion in the Context of a
Survival-Horror Video Game
Masashi Yamada; Kanazawa Institute of Technology, Japan
3PM2-R03-3
The eﬀect of music on the fear emotion is investigated using a survival-horror video
game, in the present study. Eight male and two female players participated in the
experiment. Two scenes without music were selected from the game, and eight
musical excerpts composed for the series of the survival-horror game were selected.
In the ﬁrst part of the experiment, the participants listened to each of the excerpts
only and then evaluated the impression of it using semantic diﬀerential scales. In
the second part, each combination of scenes and excerpts was presented to the
participants. Then they evaluated the impression of the audio-visual stimulus and
also rated their fear emotion. Principle component analyses and multi-regression
analyses showed that the impression of the game is largely determined by music, not
visual information. Moreover, a “heavy” music provides the “potency” and “darkness”
to the game and, in turn, evokes a strong fear emotion for the players. These results
are consistent with a previous study which used a roll-playing game. However, there
were no signiﬁcant diﬀerences between male and female players in the fear emotion
and impression of the game, in the present study. This was inconsistent with the
results for the roll-playing game, in which males were more deeply aﬀected by music
than females.


3PM2-R04 : Music and Language
Room 4, 15:45 – 17:15 Wednesday 27 August 2008, Oral session

Language, Music, and Modularity: Evidence for Shared Processing
of Linguistic and Musical Syntax
L. Robert Slevc 1 , Jason C. Rosenberg 2 , Aniruddh D. Patel 3 ; 1 Rice
University, USA; 2 University of California at San Diego, USA; 3 The
Neurosciences Institute, USA
3PM2-R04-1
The extent to which syntactic processing relies on special-purpose cognitive modules
has attracted considerable debate. The current experiments address this issue
by simultaneously manipulating syntactic processing demands in language and in
music. Participants performed self-paced reading of garden-path sentences in which
a structurally unexpected word caused temporary syntactic processing diﬃculty. As
participants read, each button press triggered a musical chord, with the resulting
sequence forming a coherent Bach-style chord progression. When a harmonically
unexpected chord was paired with a structurally unexpected word, participants
showed substantially enhanced garden-path eﬀects (as measured by reading times),
suggesting that language and music were competing for similar processing resources.
No such interaction was observed when the critical word violated semantic, rather
than syntactic, expectancy, nor when the critical chord violated timbral, rather than
harmonic, expectancy. These results support a prediction of the shared syntactic
integration resource hypothesis (SSIRH, Patel, 2003), which suggests that music and
language draw on a common pool of limited processing resources for integrating
incoming elements (such as words and chords) into syntactic structures.

Towards an Ecological Theory of Musical Semantics
Ghofur Eliot Woodruﬀ; University of Cambridge, UK
3PM2-R04-2


                                        109
Wednesday 27 August 2008                                                  ICMPC 10

Despite the intimate link between music and language, and the conviction voiced
by many that music has meaning, its semantic content continues to elude scholars
in all ﬁelds. A preliminary aim of this paper is to survey current theories of lin-
guistic and musical meaning, and thereby consider how meaning may be sought
more proﬁtably. One solution is to examine non-verbal communication within
evolutionary paradigms. The recent work of Ian Cross on motivational structures in
music, derived from Eugene Morton’s model of animal communication, is cited as
exemplary in this respect. The central theme of this paper is dedicated to a theory
of representation developed by Ruth Millikan in which sound is simultaneously
descriptive and directive. It is argued that the semantic mapping functions for
this representation derive from evolutionary, biological, and physical principles as
outlined by Morton and others. Finally, it is proposed that Millikan’s theory accounts
for simple satisfaction conditions in music. These conditions are explored in relation
to three forms of musical engagement: music as cinematic experience, music as
social interaction, and music as autonomous activity.

Cognitive Musicology, Automata Theory, and the Empirical
Testability of the Language and Music Faculty Hypothesis
Uwe Seifert; University of Cologne, Germany
3PM2-R04-3
Like for cognitive science the epistemological framework of cognitive musicology is
automata theory. Concerning language and music there are at present two research
programmes within this cognitive science framework: the generative approach
inaugurated by Noam Chomsky and the approach based on ﬁndings on mirror
neurons and Michael A. Arbib’s Mirror System Hypothesis. Empirical research on the
evolution of the language and music faculty or on language readiness within these
programmes relies on the idea of recursion, discrete inﬁnity, and self-embedding.
In this paper, the results of this empirical research are discussed and it is argued
that recursion based on a discrete inﬁnity assumption for cognitive domains is a
theoretical concept which is empirically not directly observable and testable. This
claim is made evident by showing that the main assumption made by empirical
researchers — natural languages are not ﬁnite state languages — can only be
established by proof. Furthermore, it is argued that only computational modeling
— especially with robots — might bridge the gap between automata theory and
empirical research. The argument is based on the result that computers are ﬁnite
automata and the assumption that within the epistemological framework of cognitive
science computational systems like humans are at present best considered of as
ﬁnite automata.


3PM2-R05 : Harmony and Tonality I
Room 5, 15:45 – 17:15 Wednesday 27 August 2008, Oral session

Revisiting Local versus Global Processing of Cadences in the
Solution of Musical Puzzles
Benjamin Anderson, Richard D. Ashley; Northwestern University, USA
3PM2-R05-1
Many recent studies have used musical puzzles to evaluate the local or global
importance of cadence structure. In one study (Tillmann, Bigand, & Madurell 1998),
participants chose two puzzle pieces to assemble a musical piece that conformed
to what normally occurs in music. The authors concluded that participants (1)
were sensitive to temporary modulations, (2) perceived the syntactic function of
half and authentic cadences, and (3) had more diﬃculty with modulating minuets
than minuets with half cadences. Looking at their results in terms of surface cues
and associational structures yields clues into how listeners make order judgments.
Lerdahl and Jackendoﬀ (1983) brieﬂy acknowledge the importance of associational
structures in grouping structure, but otherwise deal little with the notion. Using
discourse theory (Wolf & Florian 2005) as a guide, the model introduced here ﬁlls
this gap in theoretical understanding using network hierarchies rather than strict
hierarchies to model listener perception. Relationships between themes and motives
can be established and then adapted throughout a piece even if the two statements
are not adjacent. These relationships can demonstrate the diﬀerence between primed
expectations and their later fulﬁlments and oﬀer new insight into puzzle solving
tasks.



                                        110
ICMPC 10                                           Wednesday 27 August 2008

Statistical Properties of Tonal Harmony in Bach’s Chorales
Martin Rohrmeier, Ian Cross; University of Cambridge, UK
3PM2-R05-2
This study aims to contribute empirical computational results to the understanding
of tonality and harmonic structure. It analyses aspects of tonal harmony and
harmonic patterns based on a statistical, computational corpus analysis of Bach’s
chorales. This is carried out using a novel heuristic method of segmentation de-
veloped speciﬁcally for that purpose. Analyses of distributions of single pc sets,
chord classes and pc set transitions reveal very diﬀerent structural patterns in both
modes, many, but not all of which accord with standard music theory. In addition,
most frequent chord transitions are found to exhibit a large degree of asymmetry,
or, directedness, in way that for two pc sets A,B the transition frequencies f(A→B)
and f(B→A) may diﬀer to a large extent. Distributions of unigrams and bigrams are
found to follow a Zipf distribution, i.e. decay in frequency roughly according to
1/x2 which implies that the majority of the musical structure is governed by a few
frequent elements. The ﬁndings provide evidence for an underlying harmonic syntax
which results in distinct statistical patterns.
A subsequent hierarchical cluster analysis of pc sets based on respective antecedent
and consequent patterns ﬁnds that this information suﬃces to group chords into
meaningful functional groups solely on intrinsic statistical grounds without regard
to pitch content.

Is There a Relationship Between Pitch Attraction and Generative
Grammar in Western Tonal Music?
Matthew Woolhouse, Martin Rohrmeier; University of Cambridge, UK
3PM2-R05-3
Despite recent theoretical and empirical advances linking language and music, an
important question remains unanswered: how is a particular chord and/or tone al-
lotted to a particular position within a grammatically structured musical phrase? For
example, in a statistical study by Rohrmeier of the frequencies of diatonic chord pro-
gressions in Bach chorales, chord II was ﬁve times more likely to follow chord IV than
to precede it; chord I followed chord V twice as often as preceding it, and so on. This
research draws together two existing lines of research with the aim of showing how
local and possibly global dependency relationships speciﬁed in Rohrmeier’s genera-
tive grammar may be linked to and supported by chord attraction levels as speciﬁed
in Woolhouse’s pitch attraction model. It is argued that, to a signiﬁcant degree, the
transitional regularities of chords in tonal music can be explained and understood in
terms of generative rules of tonal harmony and pitch attraction. Empirical evidence
is provided which supports the notion that chord transition asymmetries in Bach
chorales are linked to chord attraction, mediated by an underlying syntactic process.




                                         111
Thursday 28 August 2008                                                   ICMPC 10


4AM1-R02 : Timbre II
Room 2, 8:30 – 10:30 Thursday 28 August 2008, Oral session

Investigating English Violin Timbre Descriptors
C. Fritz, A.F. Blackwell, Ian Cross, B.C.J. Moore, J. Woodhouse;
University of Cambridge, UK
4AM1-R02-1
Performers often discuss the sound quality of a violin or the sound obtained by
particular playing techniques, calling upon a diverse vocabulary. But how do those
words relate to each other? How consistent are they between players? How reliably
can they be used by teachers, or performers explaining to violin makers what they
want? This study explores the verbal description of the distinctive timbres of
diﬀerent violins: what descriptors are used by performers to characterise violins?
61 common descriptors were collected and then arranged by violinists on a map,
so that words with similar meanings lay close together, and diﬀerent meanings far
apart. The results of multidimensional scaling demonstrate consistent use among
violinists of a previously informal vocabulary and highlight which words are used
for similar purposes. These terms and their relations will be useful for violin makers
and luthiers, especially specialists in setting up and adjusting instruments during
discussions with performers. They provide a tool for acoustical research into the
quality of instrumental sound. Furthermore, identifying word consistency between
players can contribute to development of pedagogical and directorial methods, as
well as ways of annotating music scores for composers and arrangers.

Aggressiveness of the Growl-Like Timbre: Acoustical Features and
Biomechanical Mechanisms
Chen-Gia Tsai, Shwu-Fen Wang, Yio-Wha Shau, Tzu-Yu Hsiao;
National Taiwan University, Taiwan
4AM1-R02-2
The term growl is originally referred to as low-pitched, rough sounds uttered by
animals. Humans occasionally use growl-like voice to express excessive emotions.
Growl can be found in singing styles of ethnic and pop musics such as Chinese
opera and death metal. Some rockmusicians believed that death growl is distorted
by use of the guts, although the abdominal function has been unclear. The present
study showed that the deepest abdominal muscle, transversus abdominis, was
signiﬁcantly activated during growling. We proposed a biomechanical model to
relate the aggressive characteristic of growl-like timbre to the motor mechanisms
underlying growl production in humans.

Perception of Wind Instrument Vibrato Sounds
Michael Oehler 1 , Christoph Reuter 2 ; 1 IAMP, Germany; 2 University of
Cologne, Germany
4AM1-R02-3
In several experiments it has been shown, that vibrato is an important factor for the
perceived naturalness in wind instrument sounds. In the presented study vibrato
and micromodulations were investigated from a source oriented perspective along
the natural sound generating process. This approach seems to be promising, since
a deeper insight into the typical behavior of modulations and the relevance of each
individual vibrato parameter can be provided. Modulated bassoon and oboe sounds
were synthesized by means of a currently developed synthesis and analysis frame-
work. Based on the pulse forming principle, realistic source-oriented modulations
as well as often used AM and FM-modulations were produced and subsequently
rated in a listening test. A conducted ANOVA showed (p<.01) that the diﬀerent
types of modulation signiﬁcantly aﬀect the perceived naturalness. The stimuli with
combined pulse width and cycle duration modulation were perceived as natural
as the original sounds, whereas all other stimuli were perceived signiﬁcantly less
natural. The results support the hypothesis, that source-aﬀected timbre modulation
is an important factor for the perceived naturalness of double reed woodwind
vibrato sounds. Further investigations may as well be useful for exploring new sound
synthesis algorithms as for other experiments in the ﬁeld of timbre research.




                                        112
ICMPC 10                                              Thursday 28 August 2008

Do Key-Bottom Sounds Distinguish Piano Tones?
Werner Goebl, Ichiro Fujinaga; McGill University, Canada
4AM1-R02-4
The timbre of a single piano tone as well as its loudness is primarily determined by
the speed at which the hammer hits the strings (ﬁnal hammer velocity). However, the
overall sound may also be inﬂuenced by impact sounds such as the hammer-string
or the ﬁnger-key impact sounds. Especially the latter can be varied with playing
technique (touch) and is easily perceptible. Little is known about the nature of
sounds that emerge from the interaction of key and keybed. In this study, we
investigate whether the absence or presence of a key hitting the keybed makes two
otherwise identical piano tones distinguishable by expert listeners. A skilled pianist
produced a number of isolated tones on a computer-monitored Boesendorfer grand
piano (“CEUS”) that measures the loudness and onset timing of the tones as well as
the continuous position of the keys. 19 musically trained participants rated tone
pairs that were identical in pitch, loudness, and tone length, but with or with out
a key-keybed contact with regard to their identity (same or diﬀerent). Overall, the
participants performed the task very well, signiﬁcantly better than chance. Even
though the investigated key-bottom sounds are subtle compared to other sound
components, our results conﬁrm that they can audibly inﬂuence the timbre of a
piano tone. The investigated eﬀect may indeed have ecological relevance, as many
important listening situations occur in the vicinity of the piano keyboard (e.g., piano
practicing and piano lessons).


4AM1-R03 : Performance V
Room 3, 8:30 – 10:30 Thursday 28 August 2008, Oral session

It’s All in the Timing: Interpersonal Synchrony Increases
Aﬃliation
Michael J. Hove 1 , Jane L. Risen 2 ; 1 Cornell University, USA; 2 University
of Chicago, USA
4AM1-R03-1
The tendency to mimic and synchronize with others is well established. Although
mimicry has been shown to lead to aﬃliation between co-actors, the eﬀect of inter-
personal synchrony on aﬃliation remains an open question. The authors investigated
the relationship by having participants match ﬁnger movements with a visual moving
metronome. In Experiment 1, aﬃliation ratings were examined based on the extent
to which participants tapped in synchrony with the experimenter. In Experiment 2,
synchrony was manipulated. Aﬃliation ratings were compared for an experimenter
who either a) tapped to a metronome that was synchronous to the participant’s
metronome, b) tapped to a metronome that was asynchronous, or c) did not tap.
As hypothesized, in both studies, the degree of synchrony predicted subsequent
aﬃliation ratings. Experiment 3 found that the aﬃliative eﬀects were unique to
interpersonal synchrony. We interpret these results in terms of perception/action
links and self/other overlap and suggest that music and dance evolved as a means
for social bonding.

The Visual Feedback System with Interactive Contrast Training
for Fluent Finger Piano Exercises
Makiko Sadakata, Alex Brandmeyer, Renee Timmers, Peter Desain;
Radboud University Nijmegen, The Netherlands
4AM1-R03-2
Finger exercises are important for piano students at all skill levels for developing
and maintaining their motor skills. Many exercises aim at improving ﬂuency; that is,
to play materials evenly in timing and loudness. However, both the materials and
the training procedures are often monotonous. To make them more interesting, we
developed and evaluated a VFB system for two-ﬁnger trill exercises with interactive
contrast training. 24 conservatory piano students participated in the experiment.
Fluency in each trill was tested 5 times with 4 short imitation trainings in between.
The training conditions included 2 within-subject factors: 1) the presence of VFB
and 2) the type of target performance (contrast/even). The system visualized ﬂuency
in timing and loudness as the orientation and size of visual objects. Timing and
loudness of target trills were unequal for contrast conditions while they were equal
for even conditions. The preliminary analyses showed that even advanced piano stu-


                                         113
Thursday 28 August 2008                                                   ICMPC 10

dents improved their ﬂuency in trills over multiple tests and trainings. Furthermore,
both VFB and contrast training seemed to help this improvement. Qualitative data
indicated that most participants considered the VFB system and training useful.

Fingering Forces in Violin Playing
Hiroshi Kinoshita, Satoshi Obata; Osaka University, Japan
4AM1-R03-3
When playing the violin, the left ﬁngers press the string against the ﬁngerboard and
make a temporary termination of the string to control the pitch of the sound. The
temporal and spatial features of the string-pressing force (ﬁngering force) can be an
important source of information for understanding sound control by the violinists.
However, the nature of ﬁngering force is totally unknown. A violin that permitted
the online measurement of 3D ﬁngering forces was developed, and the eﬀects of
musical tempo, sound dynamics, and the ﬁngers used on the nature of ﬁngering
force were studied. Trained violinists performed repetitive A-tone (open) and D-tone
production (force measurement) using the ring ﬁnger at the tempi of 1, 2, 4, 8, and 16
Hz with p, mf , and f dynamics. The force proﬁle was clearly tempo-dependent. At
slow tempi, the proﬁles were characterized by an initial pulse-like force, followed by
leveled force during the ﬁnger contact period. At tempi higher than 2 Hz, only pulsed
proﬁles were observed. The maximum ﬁnger force exceeded 4.5 N at 1 and 2 Hz and
decreased with increases in tempo. The minimum ﬁngering force required for sound
production was assessed to be less than 0.7 N, indicating that the violinists use an
attack force well above the minimum force, most likely reﬂecting their intention to
secure rapid and steady production of a target sound. The sound dynamics as well
as the ﬁnger used also inﬂuenced the magnitude of ﬁngering force.

The Inﬂuence of the Stage Show on the Evaluation of Rock Guitar
Performance
Reinhard Kopiez, Marco Lehmann, Christian Kopp; Hannover
University of Music and Drama, Germany
 4AM1-R03-4
Acting on stage (posing) plays an important role in the live performance of popular
music. However, there is only little knowledge about the inﬂuence of stage per-
formance on the spectators’ evaluation. In our exploratory study we assume that
there is a positive correlation between the rated degree of posing and the perceived
instrumental virtuosity. It is also assumed that the rating will be inﬂuenced by a
spectator’s own instrumental expertise. A selection of 15 video samples (duration:
15–20 s) of outstanding rock guitarists were used as stimuli. Independent variables
were “instrumental expertise” (music students vs. pupils) and “presentation mode”
(audiovisual vs. audio only). 18 music students from a German university of music
and 41 high school pupils with low or no instrumental education (mean age: 16.0)
participated in the group experiment. Evaluation was conducted by means of an
answer sheet, using 11 items on a 6 point likert scale. Stimuli were presented on a
large scale screen (2 × 2 m). Ratings of the show factor signiﬁcantly correlated with
nearly all averaged items related to “virtuosity”, such as “challenging” (r = .69, p <
.01) and “impressive” (r = .84, p < .01). Music students were inﬂuenced more strongly
by the audiovisual information than were the pupils. This mechanism is independent
of a rater’s own instrument.


4AM1-R04 : Neuroscience II
Room 4, 8:30 – 10:30 Thursday 28 August 2008, Oral session

Does Program Music Induce Visual Imagery Better Than Absolute
Music? — An EEG Study
Jun-ok Kim, Moo Kyoung Han, Dongil Chung, Yeojeong Choi,
Jaeseung Jeong; KAIST, Korea
4AM1-R04-2
Program music is a form of Western classical music that carries extra-musical mean-
ings. Compared with absolute music in which artistic interest is supposedly conﬁned
to abstract constructions in sound, it is well known that program music elicits more
visual imagery in listeners than does absolute music. Despite the signiﬁcance of
this long belief in music aesthetics, it has not been proved yet and its underlying
neurophysiological mechanism is unclear. The aim of this study was to examine

                                        114
ICMPC 10                                            Thursday 28 August 2008

whether the program music indeed evokes more activations of cortical regions
involved in visual imagery than does absolute music using electroencephalograms
(EEG). 20 participants of non musician were asked to listen 4 program music and 4
absolute music pieces. And their EEGs recorded. Signiﬁcant activation of cortical
regions was estimated using sLORETA. After the EEG recording session, participants
reported their appreciation of visual images if they have. We found that program
music pieces tended to evoke more visual imagery than absolute music (p<.01).
sLORETA analysis revealed that beta band (13 – 30Hz) activation was signiﬁcant
in the cuneus and occipital lobe. Self-report analysis showed that absolute music
also evoked visual imagery in listeners which is associated with their own previous
experiences, whereas visual images in listeners during perception of program music
are associated with the theme of music.

Musical Experience Inﬂuences Subcortical Encoding of Pitch,
Timing and Timbre in Vocal Expressions of Emotion
Dana Strait, Nina Kraus, Erika Skoe, Richard D. Ashley; Northwestern
University, USA
4AM1-R04-3
Musicians exhibit enhanced perception of emotion in speech, although the biological
foundations for this advantage remain unconﬁrmed. In order to gain a better under-
standing for the inﬂuences of musical experience on neural processing of emotionally
salient sounds, we recorded brainstem potentials to aﬀective human vocal sounds.
Musicians showed enhanced time-domain response magnitude to the most spectrally
complex portions of the stimulus and decreased magnitude to more periodic, less
complex portions. Enhanced phase-locking to stimulus periodicity was likewise
seen in musicians’ responses to highly complex portions. These results suggest that
musical training engenders both enhancement and eﬃciency of neural response that
is intricately connected with acoustic features important for the communication of
emotional states. Our ﬁndings provide the ﬁrst biological evidence for behavioral
observations indicating that musical training enhances the perception of vocally
expressed emotion in addition to establishing a subcortical role in the auditory
processing of emotional cues.

Automatic Movie Themes Playlist Generation Through Gaps
Across Emotion Loci and Curve of GAEL versus Preference
Tien-Lin Wu, Yuna-Pin Lin, Shyh-Kang Jeng, Jyh-Horng Chen;
National Taiwan University, Taiwan
4AM1-R04-4
An automatic movie-theme playlist generation approach is proposed to achieve
enjoyment of listeners through integration of music-expressed (musical features)
and user-felt emotions (physiological signals) to create a varying music-expressed
playlist based on two music-psychology theories as recommendation criterion:
“Gap across emotion loci (GAEL)” and “Ω-shaped emotional similarity-preference
curve)”. The music-expressed emotion ground-truth is constructed by 328 users’
8-emotion categories’ online taggings on two hundred 30-s movie-themes and the
user-felt one is collected by playing sixteen representative themes to 26 subjects
and simultaneously and continuously measuring their Electroencephalography (EEG)
signals. Other social and cultural factors on judging the corresponding movies are
extracted by collecting IMDb’s metadata. Then, these three heterogeneous feature
sets (musical acoustics and psychoacoustics, EEG asymmetry indices, and moviegoer’
tastes) are selected and integrated to reach the best recognition accuracies for both
user-felt (97.5%) and music-expressed (93.2%) emotions based on the proposed latent
music emotion taxonomy. The combined GAEL-Ω function grasps three qualities
of music-listening, especially on emotion gaps, musical familiarity, and preference.
Finally, the two users’ subjective evaluations all show a promising result on experi-
encing the automatic generated playlists.




                                        115
Thursday 28 August 2008                                                   ICMPC 10


4AM1-R05 : Body Movement
Room 5, 8:30 – 10:30 Thursday 28 August 2008, Oral session

Quantifying Children’s Embodiment of Musical Rhythm in
Individual and Group Settings
L. De Bruyn, M. Leman, Dirk Moelants; Ghent University, Belgium
4AM1-R05-1
We empirically quantiﬁed the impact of social interaction on movements made by
children while listening and responding to music. The methodology was based on
wireless motion capturing, using Wii Nintendo Remote sensors, and subsequent
statistical analysis. We investigated intensity of movement and the amount of
synchronization with the beat in two conditions: individual, separated by screens,
and social, moving together in groups of four encouraging social interaction. Data
analysis showed that there is a social embodiment factor which can be measured
and quantiﬁed. Furthermore there is also an eﬀect found of the type of music on the
gesture response, both in the individual and social context of the experiment.

Spontaneous Movement with Music: Searching for Common
Kinetic Patterns
Petri Toiviainen, Geoﬀ Luck, Marc R. Thompson; University of
Jyväskylä, Finland
4AM1-R05-2
Music listening is often associated with spontaneous body movements, frequently
synchronized with the musical beat. Premised on the notions of embodied cog-
nition and action-perception coupling, one could postulate that these movements
facilitate the perception of the temporal structure of music. We investigated the
kinematic and kinetic aspects of spontaneous movements using a high-resolution
motion-capture system. Various kinematic variables were estimated from the data,
while body-segment modeling was utilized to obtain estimates of kinetic variables.
Although the participants produced a wide variety of movement patterns, some
commonalities between them were found. On the kinematic level, it was found that
musical beat was most clearly represented by movements in the vertical direction,
with the points of beat being associated with high downward velocity of the torso.
Movements in the horizontal direction displayed longer periodicities, typically two
beats. On the kinetic level, the instantaneous internal power of the body showed
clear peaks at the instants of musical beat. The results indicate that, regardless
of the wide variety of spontaneous movement patterns, musical beat tends to be
associated with bursts of instantaneous muscular power. This could suggest that
the perception of the temporal structure of music is associated with imitation-based
corporeal representations.

Analysis of Contemporary Dance Movement in the Presence and
Absence of a Musical Soundscape
Catherine Stevens 1 , Christian Kroos 1 , Shaun Halovic 1 , Johnson
Chen 1 , Emery Schubert 2 , Shuai Wang 2 , Kim Vincs 3 , Julien Tardieu 1 ,
Garth Paine 1 ; 1 University of Western Sydney, Australia; 2 University of
New South Wales, Australia; 3 Deakin University, Australia
4AM1-R05-3
Perception of dance, like live music, is multimodal. Participants and/or beholders, at
the very least, respond to visual, temporal, and auditory cues. An experiment was
conducted during a live performance and under controlled conditions to investigate
the eﬀect of the presence and absence of a musical soundscape on the kinematics
and dynamics of a male contemporary dancer. In addition to the recording of dance
movement using 10 Vicon cameras, 20 audience members recorded their emotional
reaction to the work in real-time under visual only, auditory only and auditory-visual
conditions. We asked: does dance movement vary as a function of the presence or
absence of a musical soundscape and what is the eﬀect of the presence/absence
of music on audience response? As anticipated, the synchronization of the three
dancers in the visual only condition was good although there was some 5% time com-
pression. Judged arousal recorded from audience members was comparable across
conditions while valence diverged. Arousal reﬂected the choreographic structure of
a generally more dynamic ﬁrst half compared with the second half. The ultimate goal


                                        116
ICMPC 10                                               Thursday 28 August 2008

is to use motion capture data to predict and aid interpretation of audience response.
Further strategies for analysis of this rich data set — analysis of speciﬁc sections and
contraction-expansion segments, individual points and distances between speciﬁc
points — are discussed.

The Role of the Body Movement in Listening to a Musical
Composition
Keiko Asakura; Teikyo University of Science & Technology, Japan
4AM1-R05-4
Cross-modal functions have attracted interest. The interest has focused on re-
searches into interaction between auditory and visual sensation. We have the sixth
sensation, proprioception that gathers information from our body. The addition
of proprioception increases the amount of sensory information, which leads to the
reliable musical perception. The aim of this research is to examine the hypothesis
that in listening to a musical composition, body movements representing its structure
would help us to feel and understand it profoundly. Participants were asked to write
down their feelings, the title they gave in listening to a piece. After practicing body
movements they were asked to do the same again. Comparing the two descriptions,
I examined how the proprioception could inﬂuence upon the audition. The data
showed certain characteristics. Eighty-six percent of descriptions changed from
the images related to water, quiet scenes and something moving, to the images of
small and mysterious creatures, pretty children and someone moving or cooperative
movements, and from consolatory and tender feelings to cheerful and pleasant
feelings. The number of participants who spotted some musical structure increased
from 60 percent to 86. These ﬁndings that body movements play such an important
role in listening to music should lead to body involvement in music education.


4AM2-R02 : Harmony and Tonality II
Room 2, 10:40 – 12:40 Thursday 28 August 2008, Oral session

Virtual Pitch and the Classiﬁcation of Chords in Minor and Major
Keys
Ludger J. Hofmann-Engl; Croydon Family Groups, UK
4AM2-R02-1
This paper makes use of the virtual pitch model as developed by Hofmann-Engl
in order to demonstrate that Hugo Riemann’s functional harmonic system has a
psychological basis and that is can be considered to be superior to the Roman
numeral system.
It will be shown that the comparison of tonal chords within a given key, produces
high similarities between the set of virtual pitches of chord I to VI, of chord IV to
II and of chord V to III. Additionally, the tension between chord I and V can be
explained by its high degree of dissimilarity. This adds momentum to Riemann’s
terminology by relating the tonic (I) to the parallel tonic (VI), the subdominant (IV) to
the subdominant parallel (II) and the dominant (V) to the dominant parallel (III). The
closing eﬀect of the perfect cadence can be seen as an eﬀect of tension and resolution.
These results act in a dual fashion. They add further support to the validity of the
Hofmann-Engl pitch (virtual pitch according to Hofmann-Engl), and suggests at the
same time that the international community might proﬁt from adapting Riemann’s
system.

Key-Proﬁle Comparisons in Key-Finding by Correlation
Craig Stuart Sapp; Stanford University, USA
4AM2-R02-2
Several improvements to the key-proﬁle weightings used in the Krumhansl-
Schmuckler key-ﬁnding algorithm have been proposed over the past decade. This
paper describes several tests which can be used to compare the performance of
these new weightings amongst themselves and against the original weights derived
from probe-tone experiments. The proposed tests may also be use to compare
diﬀerent key-ﬁnding algorithms to each other. Weightings are then evaluated using
ﬁve composition cycles in all 24 major and minor keys: Chopin’s op. 28 preludes,
and J.S. Bach’s Well-tempered Clavier preludes and fugues (books 1 & 2).




                                          117
Thursday 28 August 2008                                                   ICMPC 10

Nonlinear Time-Frequency Transformation: Implications for Pitch
& Tonality
Marc J. Velasco, Edward W. Large; Florida Atlantic University, USA
4AM2-R02-3
A growing body of evidence is consistent with the possibility of nonlinear oscillation
in both the peripheral and central auditory nervous systems. We aim to understand
the implications of nonlinear resonance for the perception of pitch and tonality in
music. Our goals are 1) to introduce a network of neural oscillators, each tuned to a
distinct frequency, arrayed along a frequency gradient; 2) to describe how gradient-
frequency networks of nonlinear oscillators transform sound; and 3) to spell out the
predictions of nonlinear resonance for the perception of pitch and tonality in music.
We show how a network of neural oscillators transforms sound stimuli. We derive
predictions about general properties of nonlinear time-frequency transformation,
including amplitude saturation, frequency detuning and higher-order resonances
(harmonics, sub-harmonics, integer ratios and combination tones). The perception of
pitch is predicted to arise as a consequence of higher order resonances of nonlinear
oscillators. The perception of tonality is predicted as a global pattern of resonance
regions at frequency relationships of small integer ratios. Neural oscillation provides
a substantive, potentially universal principle underlying the basic materials of music,
namely pitch and tonality. This framework may ultimately be extended to provide
the building blocks of a universal grammar for music.

Can Statistical Language Models be Used for the Analysis of
Harmonic Progressions?
Matthias Mauch 1 , Daniel Müllensiefen 2 , Simon Dixon 1 , Geraint
Wiggins 2 ; 1 Queen Mary University of London, UK; 2 Goldsmiths
University of London, UK
4AM2-R02-4
The availability of large, electronically encoded text corpora and the use of comput-
ers in recent decades have made Natural Language Processing (NLP) a ﬂourishing
research area. A wealth of standard techniques has been developed to serve use
cases like document retrieval, identiﬁcation of a ﬁnite vocabulary and synonyms, and
the collocation of terms. Similarly, social networking among musicians in internet
forums and the advent of automatic chord extraction have led to the establishment
of chord databases, if on a smaller scale. Comparatively little research has been
carried out on these growing corpora of chords. We suspect that one reason for this
lack of research lies in the diﬃculty to decide if chords or other harmonic elements
can be treated like lexemes in a text corpus. More simply, the question is: What is
a word in terms of harmony? In this paper we propose a bottom-up approach. In
order to ﬁnd harmonic units whose distributions resemble distributions of words
we consider chord elements diﬀering in (a) length of chord sequence (counted in
chord symbols), and (b) chord alphabet. Using lengths from 1 to 4 and two diﬀerent
chord alphabets we obtain a parameter space of size 8. For each of the parameter
settings we compute statistical summaries of the resulting frequency distribution
of the harmonic unit. As results, we report the parameter settings for two diﬀerent
chord corpora (2500+ songs each) that generate a frequency model corresponding
most closely to the Brown Corpus, a general text corpus of American English.


4AM2-R03 : Music Listening IV
Room 3, 10:40 – 12:40 Thursday 28 August 2008, Oral session

Linguistic Description and Musical Experience
Elizabeth Hellmuth Margulis; University of Arkansas, USA
4AM2-R03-1
Perhaps no outreach eﬀort is as ubiquitous as the program note: a practice which
relies on the assumption that extramusical information aﬀects musical experience.
Yet the psychological mechanisms underlying the use of linguistic information
during musical listening remain under-examined. How can reading about a piece
aﬀect the meaning it seems to confer, the enjoyment it makes possible, and the trace
it leaves in memory? More broadly, how can experiences outside of musical listening
come to transform musical experience itself?
In this study, nonmusicians heard excerpts from Beethoven String Quartets prefaced
by either a dramatic description, a structural description, or no description. They

                                         118
ICMPC 10                                              Thursday 28 August 2008

were asked to rate their enjoyment of the music, and in a later stage, to recall
excerpts and descriptions.
Results showed a complex pattern of interaction between description, enjoyment,
and memory. Conceptualizing listening by connecting it to linguistically named
correlates (a practice fundamental to music training) may have more multifarious
(and not always straightforwardly beneﬁcial) eﬀects on musical experience than
commonly assumed.

A Global Model of Musical Tension
Morwaread Farbood; New York University, USA
4AM2-R03-2
Musical tension is a high-level concept that is diﬃcult to formalize due to its subjec-
tive and multi-dimensional nature. This paper presents a quantitative, parametric
model of tension based on empirical data gathered in two experiments. The ﬁrst
experiment is an online test with short musical excerpts and multiple choice answers.
The format of the test makes it possible to gather large amounts of data. The second
study requires fewer subjects and collects real-time responses to musical stimuli.
Both studies present test subjects with examples that take into account a number of
musical parameters including harmony, pitch height, melodic expectation, dynamics,
onset frequency, tempo, and rhythmic regularity. The goal of the ﬁrst experiment
is to conﬁrm that the individual musical parameters contribute directly to the
listener’s overall perception of tension. The goal of the second experiment is to
explore linear and nonlinear models for predicting tension given descriptions of the
musical parameters for each excerpt. The data from these two experiments are then
correlated to musical features and ﬁnally used to train and test linear and nonlinear
predictive models of tension.

The Nature of Stylish Expressiveness in Performing Baroque
versus 19th -Century Music
Dorottya Fabian, Emery Schubert, Richard Pulley; University of New
South Wales, Australia
4AM2-R03-3
There has been a growing emphasis on playing European art music according to
the performing traditions of the era when any given piece was composed. There
is a claim that compositions sound more stylish, when performed according to the
stylistic conventions of their time of origin.
We wanted to ﬁnd out whether baroque performing conventions can be applied to a
romantic piece, just like late 19th -century stylistic mannerisms are often used when
performing early 18th -century pieces. We also wanted to know the aesthetic response
of listeners to such renderings.
We asked a professional violinist to record Träumerei (by Schumann) played (a) ac-
cording to romantic and (b) baroque performing conventions. 28 participants (music
students and professional players) listened to the randomized stimuli and rated 20
parameters (e.g. vibrato, portamento, phrasing, tempo preference, expressiveness,
stylishness, etc.).
Results conﬁrmed the diﬀerent performance means that create baroque as opposed
to romantic expressiveness. A performance in the baroque style is characterized by
short phrases, detached articulation, frequent punctuation, added ornaments and
relatively strict metre. The romantic style is created by long phrases, prominent
vibrato, legato articulation and strong emotionality. Listeners are able to detect these
diﬀerences. Romantic pieces performed the romantic way were regarded stylish and
expressive.
Listeners prefer historically informed renderings and judge them as being more
stylish than those that employ a generic style of expressiveness. Mainstream
performers should take note, as well as psychologists and computer scientists
investigating and modelling expressiveness in music performance.

Bimusicality: A Dual Enculturation Eﬀect on Non-Musicians’
Musical Tension and Memory
Anil Kumar Roy 1 , Elizabeth Hellmuth Margulis 2 , Patrick C.M. Wong 1 ;
1
  Northwestern University, USA; 2 University of Arkansas, USA
4AM2-R03-4


                                         119
Thursday 28 August 2008                                                   ICMPC 10

We seek to investigate the eﬀects of mono- and bi-cultural musical exposures on
non-musicians’ responses to music from diﬀerent cultures, eﬀects that could be
analogous to bilingualism. We examined three groups of non-musicians on tasks
related to musical tension and memory (W: Exposure to Western music and minimal
exposure to Indian music, WI: Exposure to Western and Indian music, I: Exposure to
Indian music and minimal exposure to Western music). Musical stimuli consisted
of short unfamiliar Indian and Western melodies. We found signiﬁcant interaction
eﬀects for both tasks: non-native music was judged to be signiﬁcantly tenser, and
native music showed higher accuracy on the memory task. Interestingly, the WI
(bicultural group) did not diﬀerentiate the tension or show increased accuracy
for Indian or Western music, suggestive of a dual enculturation eﬀect in music
(“bimusicality”). These results suggest that dual perceptual expertise is not conﬁned
to experiences that include receptive and expressive components as in the case
of bilingualism, but can be observed in simple passive exposure as in the case of
non-musicians whose expressive musical abilities are relatively limited. [supported
by a grant from the Center for Interdisciplinary Research in the Arts at Northwestern
University, and by NIH]


4AM2-R04 : Emotion in Music IV
Room 4, 10:40 – 12:40 Thursday 28 August 2008, Oral session

Individual Diﬀerence in Music Perception: The Inﬂuence of
Personality and Cognitive Style on the Perception of Musical
Melodies
Kyungil Kim 1 , Sung Yub Kim 1 , Yang-Eon Kim 2 ; 1 Ajou University,
Korea; 2 Hochschule für Kirchenmusik, Germany
4AM2-R04-1
This study suggests that, ﬁrst, chronic individual diﬀerence factors strongly inﬂu-
ence music perception, and, second, these eﬀects are very speciﬁc and independent.
Further, personality and cognitive style can be used as markers of music perception.
To demonstrate these arguments, college students completed cognitive strategy-
related personality tests, language use test, state emotion scale. Then a 2 (mode:
major vs. minor) × 2 (texture: nonharmonized vs. harmonized) × 3 (tempo: 72,
108, 144 bpm) within participants experimental design was employed, in which the
participants rated subjective happiness.
The results show that the participants’ ratings for the three musical dimensions are
individually and independently correlated to diﬀerent personality traits and/or lin-
guistic variables in meaningful ways. For example, participants’ ratings of subjective
happiness for major mode were positively correlated with cognitive strategy whereas
those for minor mode showed signiﬁcant relationships mainly with anxiety-related
personality. More individual relationships were found between participants’ music
perception and their language use. Participants’ responses for tempo dimensions
were observed to be related with cognitive factors in language use rather than with
personality factors. People’s perception of harmonized melodies was correlated with
social anxiety whereas their responses to nonharmonized melodies were inﬂuenced
by cognitive strategy-related factors both in personality tests and language use.

An Intercultural Study of Ecstasy and Trance in Music
Anita Taschler, Richard Parncutt; University of Graz, Austria
4AM2-R04-2
Ecstacy and trance have diverse institutionalised religious and healing functions. In
many cultures, hallucinations, synaesthesia and intense emotions are deliberately
induced by music and dance. We investigated ecstatic experiences in musical
performance. Participants were twelve musicians with at least six years professional
experience. They were living and working in Graz, and had been born in Austria,
Germany, Italy, Senegal and Morocco. Their main instruments were voice, piano,
keyboard, bass, dance, drums or percussion, and they mainly performed jazz, world
music and African folk. Interviews were held after live performances, and content
analyses were compared with results of Gabrielsson and Lindstrom (2003). All par-
ticipants reported altered states of consciousness that involved intense and mainly
positive emotion (including subjective, physiological and behavioural correlates)
and supported the performance process, including thinking, time perception, and
physical control. Altered states generated conﬁdence, motivation, and personal,


                                        120
ICMPC 10                                                Thursday 28 August 2008

social and musical insights, and were promoted or inhibited by physical/mental
exertion, degree of preparation, stage fright, competition among musicians, setting,
individual mood, and identiﬁcation with music. Results suggest that altered states
induced by music promote recovery, healing, creative insight, spirituality, identity,
and motivation — and hence survival in dangerous situations.

The Minor Third Communicates Sadness in Speech and Music
Meagan E. Curtis, Jamshed J. Bharucha; Tufts University, USA
4AM2-R04-3
Musical intervals are associated with emotions, but the origin of these associations
hasn’t been suﬃciently explained by current theories. For instance, the minor third
is associated with sadness in Western cultures, but the origin of this association is
a matter of debate. The current experiments explore the intriguing possibility that
the associations between intervals and emotions are also present in the prosody of
human vocal expressions. Bi-syllabic speech samples conveying happiness, anger,
pleasantness, and sadness were recorded by nine actresses. The speech samples
were rated for perceived emotion. Acoustic analyses were conducted on the speech
samples, and the prosodic contours of the sad speech samples revealed that the
relationship between the two salient pitches tended to approximate a minor third,
which is consistent with the emotional associations in the domain of music. Other
patterns were observed for the other emotions. Regression analysis of the speech
sample emotional ratings revealed that the minor third was the most reliable acoustic
cue for identifying sadness. The results suggest that there are correspondences
across domains in the use of pitch contours to encode and decode emotion. These
ﬁndings support the theory that human vocal expressions and music share an
acoustic code for communicating emotion.

The Perception of Structure and Aﬀect in Contemporary
Sound-Based and Serial Compositions as a Function of Musical
Expertise, Liking and Familiarity
Freya Bailes, Roger T. Dean; University of Western Sydney, Australia
4AM2-R04-4
Electroacoustic sound-based music is stylistically unfamiliar to most, and it typically
contrasts diﬀerent timbres and textures rather than clear rhythms and discrete
pitches. This study extends our previous investigations of listener perceptions of
electroacoustic stimuli, to determine the relationship between continuous measures
of perceived structure and aﬀect (arousal, valence) in full-scale electroacoustic and
atonal compositions. We also examine the roles of musical expertise, liking, and
familiarity for the music. Participants listened to compositions by Dean, Webern,
Wishart and Xenakis. Each piece was presented once in each of a counterbalanced
structure task (continuous indication of perceived change in sound while listening)
and aﬀect task (continuous indication of perceived arousal and valence of the music
while listening). After each item, listeners rated their familiarity with the piece and/or
style of piece, and their liking of it. Early results suggest that changes in continuous
ratings of perceived aﬀect coincide with changes in perceived structure. Arousal
ratings are largely independent of expertise, liking or familiarity, and represent
a relatively stable dimension of aﬀect. Conversely, valence ratings are positively
related to prior experience with the sounds.


4AM2-R05 : Neuroscience III
Room 5, 10:40 – 12:40 Thursday 28 August 2008, Oral session

Aﬀective Responses to Music Performance: An fMRI Study
Heather L. Chapin, Edward W. Large; Florida Atlantic University, USA
4AM2-R05-1
Our goals were to determine the inﬂuence of performance expression on listeners’
emotional responses and neural activity. Ten musically trained and ten untrained
participants listened to two versions of a romantic piano composition. The expressive
performance included natural variations in timing and sound intensity. The mechan-
ical performance, generated by computer, maintained constant tempo and sound
intensity throughout. Participants reported emotional responses in a 2-dimensional
response space (emotional intensity and valence) before and after fMRI scanning. In
the fMRI scanner, participants listened to both versions without reporting emotional


                                          121
Thursday 28 August 2008                                                   ICMPC 10

responses. Ratings of emotional intensity correlated signiﬁcantly across trials and
were positively correlated with tempo in 19 subjects. The fMRI analysis revealed a
main eﬀect of performance, showing an increased BOLD response for the expressive
performance in an emotion-related network. A main eﬀect of training showed an
increased BOLD response for trained participants in brain areas associated with
emotion processing and reward. We also observed increases in BOLD responses
that were temporally correlated with tempo ﬂuctuations. In summary, performance
expression predicted listeners’ dynamic ratings of emotional intensity. Moreover,
limbic areas responded to expressive versus mechanical performances, and dynamic
BOLD changes tracked performance expression and reported emotional intensity
over time.

fMRI Study of Diatonic Triads
Norman D. Cook; Kansai University, Japan
4AM2-R05-2
The basic facts concerning the perception of diatonic harmonies are well established
from both laboratory experiments and the statistics on the use of chords in popular
and traditional classical music. What has remained unclear, however, is the diﬀerent
contribution to harmonic “stability” of so-called “sensory dissonance” — due primar-
ily to the presence of small intervals — and so-called “musical dissonance”, i.e., the
inherently unstable, unresolved quality of certain chords that do not contain small
intervals (notably, the diminished and augmented chords). In order to determine
the eﬀects of sensory dissonance and musical dissonance, we have run behavioral
experiments using isolated chords and 2- and 3-chord sequences and have developed
a psychophysical model of harmony perception that includes both a traditional
2-tone (dissonance) factor and a 3-tone (tension) factor. We have also studied the
brain response to isolated chords and 2-chord sequences using fMRI in order to
determine the brain localization of cortical regions involved in (1) the perception
of dissonance and harmonic tension, and (2) the perception of major and minor
chords. The ﬁrst fMRI experiment showed diﬀerential brain activation to (1) chords
containing dissonant intervals and (2) unresolved “tension” chords. We interpret
this dissociation as clear indication that sensory dissonance can be distinguished
from the musical dissonance produced by triads with two equal intervals (the two
3-semitone intervals in a diminished chord in root position and the two 4-semitone
intervals in an augmented chord). The second fMRI experiment showed that the
strongest response to 2-chord sequences was when the ﬁrst chord was an unresolved
tension chord and the second chord was a resolved major/minor chord that diﬀered
from the ﬁrst chord by one semitone. I conclude that the fundamentals of traditional
harmony theory have a clear acoustical basis that can be quantitatively modeled
provided that the eﬀects of 3-tone combinations are considered.

Focal Activations and Properties of Functional Brain Networks
Derived from MEG Data While Listening to Music
Andreas A. Ioannides, Armen Sargsyan, Marotesa Voultsidou, Mari
Aoki; RIKEN Brain Science Institute, Japan
4AM2-R05-3
In our earlier study of how “real” music engages the brain we extracted regional brain
activations from the magnetoencephalographic (MEG) signal recorded while subjects
listened to music as it unfolded over a few-second-long periods of time (Popescu
et al., NeuroImage, 2004). We compared the music score with the timecourses in a
small set of pre-deﬁned brain areas using rhythmogram analysis. Using the average
response of 20 music score presentations, we showed that motor related areas played
pivotal role during the music listening experience that depended on the musical
expression. Here, we use the rhythmogram analysis for every voxel in the brain and
carried out the analysis for each presentation of the music score. The new analysis
demonstrated that in areas usually associated with linguistic and music processing
like Broca’s area the pattern of activity as captured in the rhythmogram correlates
with the pattern of the music score as long as the music plays. Surprisingly, other
areas correlate with the pattern of activity of the music score immediately before the
onset of the music, while after the music has stopped, the activity in some areas of
the brain correlates with derivatives of the patterns in the music score.




                                        122
ICMPC 10                                              Thursday 28 August 2008

Detecting Imagined Music from EEG
Rebecca S. Schaefer 1 , Marcos Perreau Guimaraes 2 , Peter Desain 1 ,
Patrick Suppes 2 ; 1 Radboud University Nijmegen, The Netherlands;
2
  Stanford University, USA
4AM2-R05-4
Previous work has shown a varying, but well above chance result in detecting heard
words and sentences, internally spoken words, and internally imagined rhythms
from EEG [cf. PNAS 94, (1997), 14965; PNAS 95, (2005), 15861, Proc. ICA2004. Kyoto,
Japan, (2004)]. To extend these ﬁndings, we have carried out experiments that aim
to replicate the above results for both heard and imagined music, hypothesizing that
the temporal structure in music may increase the accuracy of detection.
Two common stimuli and two self-selected stimuli were used for each of six partic-
ipants, in a perception and an imagery condition, allowing classiﬁcation between 4
diﬀerent musical phrases. Preliminary results show classiﬁcation above chance level
(25%) for all participants in the perception condition (38–68%), and for 4 participants
in the imagery condition (33–43%).
Although promising (and still likely to be improved upon) these results are not as
striking as those found for auditory perception (spoken words, up to 97% with 7
classes) or imagination (rhythm, 37% with 5 classes). Further analyses will however
ﬁnetune these results and create a clearer image of the brain signature associated
with hearing and imagining music.




                                         123
Friday 29 August 2008                                                      ICMPC 10


5AM1-A01 : Ethnomusicology
Atrium 1, 9:15 – 10:45 Friday 29 August 2008, Oral session

Psychoacoustical and Cognitive Basis of Sutartines
Rytis Ambrazevcius; Kaunas University of Technology, Lithuania
5AM1-A01-1
The measurements of pitches and intervals in Sutartin˙    es (Lithuanian type of vocal
Schwebungsdiaphonie) reveal that this style of singing is based on the psychoacous-
tical principle of maximum roughness. This principle is at work, ﬁrst of all, for the
central pitches (the “double tonic” composed of two colliding voices) which form
the nucleus of the scale. The stability and salience of the pitches decreases towards
the margins of the scale. The sequential movements of the vocal dyads create the
impression of melodic contours that are akin, in a general sense, to the melodic
contours found in monophony. Thus the element of linear thinking in Sutartin˙       es
is revealed. On the other hand, the tension-resolution patterns analogous, in deep
structure, to the patterns known in the Western music, can be envisaged. The further
comparisons of Sutartin˙ es with other musics in the Baltics might be quite promising
in the discussion on their origins.

Experimental Investigation of Relative Pitch Salience in Northern
Mozambican Damba
Lydia Slobodian, Ian Cross; University of Cambridge, UK
5AM1-A01-2
A series of experiments conducted in Cabo Delgado Province, Northern Mozambique,
investigate perception of pitch change by students of Damba, a musical form taught
in madrassas along the coast. To determine relative salience of pitches in short
Damba melodies, we altered the pitches of diﬀerent tones in each melody by +/- 1.0
semitone. Participants listened to the original followed by an altered or unaltered
version and had to determine whether the two melodies were the same or diﬀerent.
Responses show that in most cases participants were better able to hear pitch change
in higher tones and ﬁnal tones, regardless of tone length. Analysis also suggests
consistent hierarchies within certain melodies, as well as the possibility of an overall
tonal hierarchy. These ﬁndings can inform analyses of individual Damba pieces and
performances in the absence of an explicit music theory. This project illustrates both
the constraints and potential inherent in cross-cultural experimentation.

What Emotions do Raags Evoke? An Internet-Based Survey of
Listener Responses
Parag Chordia, Alex Rae; Georgia Institute of Technology, USA
5AM1-A01-3
Raag is the melodic framework that forms the basis of Indian music. Raags are
thought to consistently elicit certain emotions, with diﬀerent raags creating diﬀerent
moods. We investigate whether this correspondence truly exists cross-culturally.
We undertook a web-based survey (http://paragchordia.com/survey/raagemotion2)
in which participants were asked to listen to ten raag excerpts and rate the extent to
which twelve emotions were evoked. A total of 553 subjects participated.
ANOVA analysis showed that raag was a signiﬁcant factor in emotion responses (p
< .001). Using conﬁdence intervals adjusted for multiple comparisons (p < .05), we
found that a clear distinction could be made between many raags. Across many
emotions, raags clustered into groups with positive (Khamaj, Desh and Bageshri),
medium (Bhimpalasi and Yaman), and negative valence (Gujari Todi, Marwa, Shree
and Darbari).
Familiarity with Indian music was a signiﬁcant main eﬀect for many emotions.
Experienced listeners tended to rate the excerpts as more intensely emotional than
did listeners with little or no prior exposure.
There is a clear correspondence between valence and scale degrees used. Raags using
‘minor’ notes are considered negatively valenced, while raags using ‘major’ notes tend
to be positively valenced, giving further evidence for this cross-cultural phenomenon.
Detailed data and graphs can be found at
http://paragchordia.com/research/raagEmotion/.




                                         124
ICMPC 10                                                  Friday 29 August 2008


5AM2-S01 : Psychoacoustics II
Space 1, 11:00 – 13:00 Friday 29 August 2008, Oral session

They Were Playing Our Song: A Psycho-Acoustic Explanation of
Why Non-Vocal Musical Instruments Determined How We Sing
Joe Wolfe, Emery Schubert; University of New South Wales, Australia
5AM2-S01-1
In the last few centuries, musical instruments have inﬂuenced the styles of singing.
This paper uses acoustical arguments to suggest that, even in prehistory, musical
instruments may have inﬂuenced singing. The codings of speech and music are
diﬀerent and in some ways complementary. The voice operates on acoustical prin-
ciples distinctly diﬀerent from those of (other) musical instruments. We conjecture
that the pitch stability and the independence of pitch and loudness — features very
important in much music — may have come from imitation of or performing with
musical instruments. Lomax proposed that aspects of a culture are reﬂected in the
music they make. We suggest that peoples exposed to manually played instruments
exhibiting discrete, stable pitches are more likely to sing using categorical pitch.
Peoples who are exposed only to less resource rich, non-musical instrument bearing
regions are more likely to sing in a manner that suits their biological speech appara-
tus and therefore to sing with more portamento and with correlations between pitch
and loudness. Evidence is presented though sound recording made by Australian
Aboriginal people who had varied access to physical resources in their diﬀering
native environments.

Evaluations of Proﬁciency of Fluctuating Musical Sounds Using
Fluctuation Strength
Nozomiko Yasui, Masafumi Kinou, Masanobu Miura; Ryukoku
University, Japan
5AM2-S01-2
The tremolo played on the mandolin is a continuous sound produced by the rep-
etition of attenuating sounds. The amount of acoustic amplitude of tremolo is
usually ﬂuctuated in terms of time, so it is said to be a ﬂuctuating continuous sound.
Therefore, the tremolo is assumed to give a listener the feeling of ﬂuctuation, which
is thought to be concerned with subjective evaluation for performance proﬁciency.
Introduced here is an evaluation method that strongly depends on the “Fluctuation
Strength (FS)”, which has been suggested as an index for evaluating the hearing
sensation produced by ﬂuctuating sounds at low frequencies. Past studies have
investigated the index for modulated pure tones and broadband noise. In this
study, tremolo ﬂuctuations are described by FS, and the relation between them and
evaluations of tremolo proﬁciency was investigated through evaluation experiments.
Speciﬁcally, a relation between factors of ﬂuctuation feeling and subjective evaluation
of performance proﬁciency is investigated using simulated sounds. Highly rated
tremolos had a relatively low physical FS, which suggests that the physical FS can be
used as an index to evaluate tremolos played on the mandolin. It is thought that a
skillfully played tremolo satisﬁes both a playing restriction and a psychoacoustical
criterion.

Dynamic Changes of Intensity, Duration and Timbre: Eﬀects on
Judged Loudness and Emotional Arousal
Kirk N. Olsen, Catherine Stevens, Julien Tardieu; University of Western
Sydney, Australia
5AM2-S01-3
Two experiments investigate loudness, intensity change and emotional arousal
in a simple musical context and builds on recent ﬁndings of a perceptual bias to
tones of increasing intensity. This bias involves an overestimation of loudness
change for increasing intensities (ramps), relative to decreasing intensities (damps)
of identical spectral content. Experiment 1 (n=32) tested this hypothesis by system-
atically manipulating independent variables of stimulus timbre (vowel; violin), layer
(monotone; chord) and duration (1.8 s; 3.6 s), presented as counterbalanced pairs of
items over linear intensity sweeps of 60–90dB and 90–60dB. In Experiment 2 (n=17),
Galvanic Skin Response (GSR) was measured as an indicator of emotional arousal
as participants heard singular ramped or damped items while judging loudness
change. In Experiment 1, the bias for rising intensities was recovered and extended


                                         125
Friday 29 August 2008                                                     ICMPC 10

using musical timbres over various durations. The magnitude of this diﬀerence
signiﬁcantly increased as a function of stimulus duration. In Experiment 2, GSR
indicated that both ramped and damped musical stimuli elicit increases in physio-
logical arousal. Our results support the claim that an illusory perceptual response is
elicited by simple musical stimuli of gradually increasing intensity. This response is
correlated with physiological change, namely increased arousal. The implications of
this fundamental, and possibly adaptive, perceptual and physiological response to
simple musical stimuli are discussed.


5AM2-S02 : Music Therapy / Evolutional Perspective /
Development / Neuroscience and Disorders /
Computational Models and Analyses
Space 2, 11:00 – 13:00 Friday 29 August 2008, Poster session

The Eﬀect on Cognition and Ability by Learning to Play the Piano
in Senior Year
Jie Ren 1 , Xiaoping Luo 2 ; 1 South China Normal University, China;
2
  Xinghai Conservatory of Music, China
5AM2-S02-01
There are 47 subjects who come from Guangzhou Elder College in this study. They
have diﬀerent degrees on playing the piano: never played(Group I), have played for
less than 2 years(Group II) and have played for 2 or more than 2 years(Group III).
We measured all subjects cognitive processing speed, working memory capacity, and
ﬂuid intelligence by experimental and testing methods. The results are: Subjects
in each group have faster VPS than APS (d=941.58ms, d=917.86ms,d=866.97ms;
respectively) and subjects in Group II have larger AWMC than VWMC(d=1.10).
Subjects in Group II and Group III have faster VPS (562.33ms,567.74ms respectively)
and larger AWMC (7.60, 7.47; respectively) than those in Group I (638.94ms; 5.17,
respectively). Subjects in Group III have faster APS than those in Group I (1434.71;
1580.52; respectively). There is no signiﬁcant diﬀerence between VWMC and CRT-IQ
in three groups. CRT-IQ closely correlated with VPS and VWMC(r =-.294,r =.289;
respectively). We concluded that learning to play the piano have positive eﬀects on
the development of cognition abilities and a limited eﬀect on Fluid intelligence of
the elderly people.

A Comparison of the Psychosocial Eﬀects of Music Therapy,
Animal-Assisted Therapy and a ‘Discussion Group’ in Cognitively
Intact, Elderly Participants
S.R. Toukhsati, G. King, L. Greenﬁeld; Monash University, Australia
5AM2-S02-03
The aim of this study was to evaluate the psychosocial eﬀects of exposure to music
therapy in cognitively intact (MMSE ≥ 24), elderly individuals. The eﬀects of music
therapy were compared with an alternative therapeutic (animal-assisted therapy)
and ‘non-therapeutic’ (discussion group) intervention in 31 participants (Mean Age
= 86.16 years, SD = 5.64). Each treatment was randomly assigned to two Aged Care
facilities and implemented in small groups, twice weekly for 30 minutes for a period
of four weeks. A nested, mixed-model design compared pre- and post-intervention
cognition, depression, quality of life and physical and emotional well-being scores
between the three treatment groups. The ﬁndings showed a signiﬁcant improvement
over time (p < .01) in cognition (Total MMSE and Recall subscale scores), depression,
and physical well-being (RAND-36 Physical Health subscale), but no signiﬁcant
group or interaction eﬀects. There were no signiﬁcant main or interaction eﬀects
with regard to quality of life. These ﬁndings indicate that there was no signiﬁcant
advantage of exposure to music therapy in comparison to animal-assisted therapy
or involvement in a discussion group. Non-signiﬁcant trends did, however, show
evidence of a greater improvement in cognition and quality of life following music
therapy. Theoretical implications and suggestions for methodological reﬁnements in
future research are discussed.




                                        126
ICMPC 10                                                  Friday 29 August 2008

An Interpretative Phenomenological Analysis (IPA) Study of
Musical Participation by Individuals with Mental Health Problems
Julie C. De Simone, Raymond MacDonald; Glasgow Caledonian
University, UK
5AM2-S02-05
This is a joint study between Glasgow Caledonian University, Greater Glasgow
Health Board and a music charity, Polyphony that provides access to a wide range of
music activities for individuals with mental health disabilities in a large psychiatric
hospital in the west of Glasgow, Scotland. This paper investigates, from a qualitative
perspective, the subjective experiences of individuals with mental health problems
who participate in structured musical activities. Six community patients took part
in this study. All participants were interviewed and interviews were transcribed
and analysed using Interpretative Phenomenological Analysis (IPA) as a theoretical
framework. This paper will discuss some of the key issues involved in IPA, a modern
approach which is an increasingly popular qualitative approach to research within
health care contexts. Recurrent and emerging themes will be presented highlighting
key subjective issues for the participants with reference to the music intervention
and also with reference to the importance of music in their lives. This research
highlights a number of subjective issues for individuals with mental health problems
who participate in music interventions, signalling a range of possible beneﬁts from
a subjective perspective such as reduced isolation and improved mood. It also
highlights the utility of IPA as a research approach in this area.

Evolutionary and Neurobiological Foundations of Speech and Song
Development
Wilfried Gruhn; Freiburg University of Music, Germany
5AM2-S02-07
Evolutionary psychologists have suggested that music and language evolved from a
common ancestor and share the same neural resources. Therefore, an evolutionary
aspect is evident as well as the neurobiological supposition that resource sharing
networks (RSN) enable humans to speak and sing. Music and language are cultural
phenomena which incorporate a broad variety of arbitrary pattern characteristics.
These are aurally learnt, vocally performed, and cognitively discriminated. Vocal
learning g, i.e. the ability to imitate arbitrary sounds, is supported by a neural
mechanism that directly connects auditory and motor areas. It basically consists of
an anterior and a posterior pathway which perform a phonological loop. By this, the
aural perception immediately stimulates a sensorimotor activation in the vocal tract
which then controls the vocal production with the ﬁnal intent to match the aurally
perceived pitch or sound. Neurobiological research on vocal learning in animals,
namely in songbirds, and young children, will be reviewed. By this an evolutionary
perspective will be introduced to the neurobiological foundations of speech and
song acquisition. The underlying neural mechanisms and structural properties are
integrated into a common model of audio-vocal learning which develops at a very
early age when song and speech are not yet separated.

The Role of Musical Environment at Home in the Infant’s
Development (Part 2): Exploring Eﬀects of Early Musical
Experiences on the Infant’s Physical and Motor Development
During the First 2 Years
Izumi Kida, Mayumi Adachi; Hokkaido University, Japan
5AM2-S02-09
The purpose of this study was to examine eﬀects of early musical experiences
on infants’ physical and motor development through a longitudinal study. 31
infants and their mothers began participating in this study when the infants were
3- to 4-months old; data collection for each infant continued for two years. The
participants were assigned randomly one of the two conditions: Musical Interaction
and Control. The mothers in the Musical Interaction group were asked to play with
their infants while singing Japanese traditional play songs. The mothers in the
Control group were requested nothing special. Data of infants’ physical and motor
development and information of their home musical environment were collected
through semi-structured interviews with the mothers. Permutation tests on the
motor development data up to 12 months revealed no eﬀects of requested musical
interaction but signiﬁcant eﬀects of home musical environment. Infants raised in
richer musical environment acquired gross motor skills earlier than those raised in


                                         127
Friday 29 August 2008                                                       ICMPC 10

ordinary environment. Richer musical environment gives infants more opportunities
in paying attention to music, exploring sound objects, and moving their own body
to the beat with their parents even without external requests. These opportunities
may facilitate infants’ gross motor development during the ﬁrst year. The poster
presentation includes results of later analyses up to 24 months, and our discussions
for the overview of longitudinal eﬀects of early musical experiences on infants’
motor development.

The Role of Musical Environment at Home in the Infant’s
Development (Part 4): Japanese Mothers’ Involvement in Music
and Its Eﬀects on Parenting
Kumi Matsuda 1 , Mayumi Adachi 2 ; 1 Sapporo Specialty School of
Welfare and Child Care, Japan; 2 Hokkaido University, Japan
5AM2-S02-11
In the present study, we considered the Japanese mothers’ everyday involvement in
music as a part of their personal values, exploring how the degree of their musical
involvement would be related to their attitude and aﬀection toward their child as
well as to their beliefs in childrearing. We asked 31 Japanese mothers with 3- to
4-month-olds the types of musical activities they were involved in and the frequency
of each musical activity in their daily lives. As a result, two groups of mothers were
identiﬁed: “musically active” (MA) and “musically passive” (MP). Semi-structured
interviews revealed that all the mothers in this study held relatively positive attitudes
toward their children and their childrearing. MA mothers held lesser negative
feelings toward their own child than MP mothers, and took their stress on their child
less frequently than did MP mothers. MA mothers wished to expose their child to
music and the arts more strongly than MP mothers. These results may suggest that
the mother’s own active involvement in music can release her stress and tension,
being able to control her emotion positively toward her child. Alternatively, those
who can control their emotions and manage their stress and tension may bring music
into their lives more actively than their counterpart. Even though the directionality
of the results cannot be determined, this study illuminates that music accompanies
Japanese mothers’ childrearing, at least for music lovers.

Deﬁcits in Detecting Pitch Violations in Music and Language in
Patients with Right Temporal Lobe Lesion
Kohei Adachi 1 , Takayuki Nakata 2 ; 1 Nagasaki Junshin Catholic
University, Japan; 2 Future University-Hakodate, Japan
5AM2-S02-13
We investigated if music and language pitch processing diﬀers for three patients
with lesion on the right temporal lobe. Ninety four undergraduates served as the
control group. Using eight short Japanese sentences with ﬁve syllables, sung and
spoken versions were recorded as the base stimuli. For each trial, short melody or
utterance was repeated twice with half trials repeating the same melody/utterance.
For the other half, the last syllable of the second melody’s or utterance’s F0 was
shifted upward for .25, .5, 1, or 2 semitones. After listening to each pair of stimuli,
participants were asked to judge whether the last syllable was the same or diﬀerent.
For all of the three patients, percentile ranks relative to the control group were lower
for language excerpts than music excerpts at .5, 1, and 2 semitones. One of the three
patients had lesion on Heschl’s gyrus on the right temporal lobe and his performance
at two semitones for language excerpts, but not for music excerpts, was lower than
the other two patients with Heschl’s gyrus intact. Our results support previously
reported ﬁndings that right temporal lobe is involved with the detection of pitch
deviation both in music and language. Interestingly, we also found that, in detecting
pitch contour violation, trauma on the right temporal lobe, including Heschl’s gyrus,
may lead to a greater increase in threshold for language than music.

The Inﬂuence of Weak Central Coherence in Auditory Processing
Hayato Watanabe, Harumitsu Murohashi; Hokkaido University, Japan
5AM2-S02-15
The autism spectrum disorder (ASD) is a pervasive neurodevelopmental disorder
characterized by impairments in social interaction, communication, and by restricted
repetitive patterns of behavior. It is thought that the weak central coherence (WCC)
inﬂuences these features which are appeared as local bias in ASD. We used Autism-
spectrum Quotient (AQ) measure to investigate whether the performances vary


                                          128
ICMPC 10                                                  Friday 29 August 2008

according to their AQ scores in hierarchical melody discrimination task. Participants:
typically developed 19 males and 5 females participated in this study. They were
classiﬁed as high AQ group and low AQ group by AQ score. Stimulus: Nine tones
were grouped sequentially to form a melody. These tones were arranged to form
rising-falling or falling-rising contour of the melody. Condition: All tasks consisted
of same-diﬀerent judgements of pairs of melodies. There were 4 kinds of conditions;
global and local que condition, global cue condition, local cue condition, and no
cue condition. Performance of low AQ group was improved in global cue condition
regardless of local cue. It was suggested that local cue was important to recognize
global cue for high AQ group. Therefore, it is suggested that the individuals with
high AQ can not understand global aspect in the melody suﬃciently.

Change of Movement and Behavior by Music in Patients with Rett
Syndrome
Tohshin Go, Yukuo Konishi; Tokyo Women’s Medical University, Japan
5AM2-S02-17
Rett syndrome is a neurodevelopmental disorder characterized by stereotyped hand
movement such as wringing. Although patients with Rett syndrome typically have no
verbal skills, they are reported to respond well to music. Therefore, a questionnaire
was sent to participants in an annual summer camp for them in Japan to study
their movement and behavior, and their change by music. The answer was obtained
from 34 patients. All were female aged from 4 to 30 years (11.5 +/- 8.1, mean +/-
standard deviation). Age of onset was 1.5 +/- 0.7 years. Ambulatory patients with
and without help were 32% and 38%, respectively. Stereotyped hand movements and
body swinging were observed in 76% and 38%, respectively. Major response to music
was described as follows. Smile was observed in 85% of patients. Body swinging
started with music in 35% and stopped in 12%. On the other hand, stereotyped
hand movements started in 12% and stopped in 35%. Seizure was induced in 9%. In
conclusion, many patients with Rett syndrome seemed to be pleased with music and
music had diﬀerent eﬀect on stereotyped hand movement and body swinging.

Chord Estimation Using Chromatic Proﬁles of Sounds Played by an
Electric Guitar
Yasushi Konoki, Norio Emura, Masanobu Miura; Ryukoku University,
Japan
5AM2-S02-19
This paper describes a system that estimate a chord label in real-time for sounds
played with several electric guitars. We had studied a chord estimation system
for MIDI signals in previous study. Chord labels are estimated by considering the
combination of pitch classes and validity of performed chord progression. We aim
at extending our previous system to be able to use in more practical situations, by
dealing with acoustic signals instead of MIDI signals. Performed pitch classes are
estimated by converting input sounds into the chroma vectors and selecting strong
pitch classes depending on intensity relations in these vectors. Finally, a chord label
for performance is estimated for obtained pitch classes.

Jazzbot: An Anthropomorphic Music Cognition Research Tool
Charles Hart, Michael Connolly Brady; Indiana University, USA
5AM2-S02-21
Recent years have seen a shift in cognitive science where the autonomous robot has
begun to supplant the symbol-processing computer as dominant metaphor. The time
has come to consider music processing and interaction in terms of the behavioral
robot. We introduce JAZZBOT, a saxophone-playing automaton. It uses an air com-
pressor, an electro-pneumatic breath system, and a mechanical mouthpiece to vibrate
the saxophone’s reed and generate a sound source. It then uses solenoid-controlled
mechanical ﬁngers to manipulate the keypads and thus the resonant dynamics of
the saxophone’s cavity. We ﬁrst report insights from these fabrication eﬀorts. For
instance, the embouchure mechanism is extremely sensitive to combinatorial changes
in air pressure and saxophone ﬁngering. The embouchure control that a professional
saxophonist acquires through years of practice becomes somewhat automated and
reﬂexive or “sub-cognitive,” yet it is very diﬃcult to implement this kind of control
in terms of a well-speciﬁed executive signal. This prompts the question: what form
should an executive signal take? Furthermore, based on numerous conversations
and critiques, we came to realize the importance of a social presence in musical


                                         129
Friday 29 August 2008                                                     ICMPC 10

performance. As a result, we provide the robot with animatronic facial features to
allow for the impression of shared attention and to facilitate eventual musical turn
taking.

A System Generating Jazz-Style Chord Sequences for Solo Piano
Junko Watanabe 1 , Kaori Watanabe 1 , Norio Emura 2 , Masanobu
Miura 2 , Masuzo Yanagida 1 ; 1 Doshisha University, Japan; 2 Ryukoku
University, Japan
5AM2-S02-23
In composition and arrangement of tonal music, it is important to give appropriate
harmony to a given set of melody and its chord name sequence, though that is a tough
task for non-professionals. Then there are a lot of research works and commercially
available systems, such as systems for automatic arrangement of music pieces given
as note sequences for solo pianos into a piano score in a speciﬁc style. These systems,
however, are usually designed to generate music by concatenation of existing arrange-
ment patterns. Such systems can not meet user requirements. This paper proposes
a system that generates jazz-style chord sequences taking the structure of the input
set of melody and chord name sequence into account reﬂecting user requirements.
The system is implemented in an integrated modular structure based on the classical
theory of harmony and several jazz arrangement techniques described in Jazz theory.
The performance of the proposed system is evaluated by comparing the results given
by the proposed system with those given by a popular arrangement system available
on the market. Experimental results show that the proposed system yields more
preferable outputs then a commercial system available on the market.

MusicKiosk: When Listeners Become Composers — An Exploration
into Aﬀective, Interactive Music
Laurence Pearce 1 , Lassi A. Liikkanen 2 ; 1 XIM Ltd, UK; 2 Helsinki
Institute for Information Technology, Finland
5AM2-S02-25
Music has for long been a one-way medium where professionals compose and others
enjoy. However, with new interactive applications, listeners are receiving a novel op-
portunity to shape music in real time. Emotions are at the core of music appreciation
and thus it appears natural to examine the discipline of aﬀective computing to unite
these two themes.
We present a case study of an interactive, assisted composition system called
MusicKiosk. The system creates a composition based on the emotional states
detected from users’ voices. The experience is augmented by visualizing the
music with interactive, animated characters. Custom made musical elements are
added or removed dynamically according to the detected mood. The input for
emotion detection is derived from the fusion of emotional speech recognition and
keyword spotting. In upcoming user evaluation, we will use this system to explore
natural interaction and the capacity of the system ot create emotional feedback loops.
Our presentation will demonstrate how this system is going to be implemented and
investigated in detail. We attempt to show that developing an interactive, aﬀective
music composing system is becoming feasible with the state-of-the-art technology.
We aim to display MusicKiosk in a museum environment by the end of 2008.


5PM1-S01 : Performance VI
Space 1, 14:10 – 15:40 Friday 29 August 2008, Oral session

Communication of Emotions with Diﬀerent Intensities Through
Performances of Professional Musicians and Non Musicians
Teruo Yamasaki; Osaka-shoin Women’s University, Japan
5PM1-S01-1
This study focuses on the relation between the intensity of intended emotions and
music performance and consists of two experiments. First experiment was a per-
formance experiment, in which two professional drummers and ten non musicians
expressed three emotions (‘joy’, ‘anger’, and ‘sadness’) at three degrees of intensity
by playing a MIDI drum improvisationally. Second experiment was a listening ex-
periment, in which musically untrained listeners heard the performances in the ﬁrst


                                         130
ICMPC 10                                                 Friday 29 August 2008

experiment and judged their emotional intentions. In experiment I, performances
by both types of players were compared and analyzed in terms of various acoustic
features. As the results, the close relation between the types of emotions and two
acoustic features, i.e. sound level (MIDI velocity) and interval of beats, were found.
The intensities of emotions were mainly related to sound level and duration between
onset time and oﬀset time. These ﬁndings were common to professional musicians
and non musicians. On the other hand, professional musicians were diﬀerent from
non musicians in that sound level and interval of beats varied considerably. As the
results of experiment II, the average rate of correct response for the performances of
professional musicians was 41.5% and that for the performances of non musicians
was 21.9%. Not only rate for professional musicians but also rate for non musicians
exceeded the chance level signiﬁcantly.

Gestures and Music Analysis in Piano Performance
Roberto Caterina, Mario Baroni, Luisa Bonﬁglioli, Maria Teresa
Storino, Michele Privitera, Iolanda Incasa, Fabio Regazzi; University of
Bologna, Italy
5PM1-S01-2
In our work we tried to see how body expressions in musical performance are
important elements of the interpreter musical thinking. We asked 7 professional
piano players to perform twice two pieces belonging to diﬀerent historical periods
and styles. The performances were videotaped by 3 cameras in diﬀerent positions
(face, body, back). A special device, called Moog PianoBar, was put on the piano
keyboard in order to get data about some interpretative variables. A special grid was
prepared to compare musical aspects with body and facial expressions.
The ﬁrst results of our analyses indicate that facial and body movements are strictly
connected with dynamic proﬁles and phrase segmentations. Not all the interpreters
use the same signals, but they do use body expressions to stress the same main
points of articulation of musical structure. Body expressions seem to be more
appropriate to express music segmentation, while facial expressions seem to be
linked with expression of dynamic and agogic qualities, as we could observe in a
previous study (Caterina et al. 2003).

Discovering the Body: Insights into Music Performance and
Education from Highly Practiced Musicians
Shin Maruyama; Rikkyo University, Japan
5PM1-S01-3
The aim of this study is to describe a role that the sense of bodily movement plays in
the production of music with reference to the theoretical framework of the ecological
approach. To do so, I individually interviewed prominent musicians (instrumental
conductor, violinist and cellist) who have rich experience of teaching and asked
them questions about the role of body movement in their performance and about
instructional techniques they usually use when teaching. I report four cases implying
that knowing physics of the body is a fundamental skill to reach a right expression
of music. These examples strongly support the speculation that the sense of bodily
movement is a necessary element to musical activity. This implies that, without
learning about such physical solutions, we might not access the deeper meaning
of music. Although further study is required empirically to qualify the practical
relationship between the body and music, we need to discuss an alternative pedagogy
that enables students to discover and enhance the sensitivity to their own body.


5PM1-A01 : Cognitive Processes and Music Psychology
Atrium 1, 14:10 – 15:40 Friday 29 August 2008, Oral session

Diﬀerences in the Cognitive Processing of Music and Soundscapes
Revealed by Performance on Spliced Stimuli
Jean-Julien Aucouturier; University of Tokyo, Japan
5PM1-A01-1
Considerable eﬀort is spent optimizing computational techniques to simulate human
perception of music and environmental sounds. Most of these techniques take no
account of time, but rather focus on global statistical distributions. In other words,
machines listen in eﬀect to spliced audio signals, i.e. signals which frames have


                                        131
Friday 29 August 2008                                                     ICMPC 10

been shuﬄed randomly in time. In this study, we investigate how well humans
perform on such kind of signals. We compare human and machine performance in
both a similarity and a categorization task, for spliced versions of both music and
soundscape signals. Splicing is found to signiﬁcantly degrade human categorization
performance, both for music and soundscapes. Concurrently, splicing also degrades
similarity, but signiﬁcantly more so for music than soundscapes. This establishes
that humans are capable of comparing soundscapes in a timeless, amorphous way
which resists well to splicing. On the other hand, humans perform very poorly on
the kind of musical data we typically expect algorithms to succeed on — in fact
they’re even worse than machines. Finally, good human performance for soundscape
similarity doesn’t appear to require identiﬁcation of e.g. constituent sound sources,
which suggests that, contrary to music, soundscapes can be compared in an acoustic-
only manner, without much semantic analysis.

“Imagery” and “Force”: Conceptual Metaphors in the Early Music
Psychology of the Late Nineteenth and Early Twentieth Centuries
Youn Kim; University of Hong Kong, China
5PM1-A01-2
This paper deals with the early music psychology of the late nineteenth and early
twentieth centuries (e.g., Herbart 1824–25; Helmholtz 1863; Stumpf 1890; Riemann
1884, 1916; Kurth 1931). It analyses conceptual metaphors found in these writings,
particularly the spatial metaphor of “imagery” (Vorstellungen) and the dynamic
metaphor of “force” (Kraft). The examination of the linguistic and metaphoric formu-
lations in the early music-psychological discourse reveals diﬀerences and changes in
conceptions of music and the mind. The change from imagery to force and the use
of two diﬀerent notions of force (i.e., physical force and life-force) can be construed
signifying a shift from the mechanistic to the holistic, more intuitive conception of
music and the mind — in Berman’s (1984) words, a shift from “disenchanted” to
“re-enchanted” music psychology).
Our eyes are not exclusively set on the past: Both imagery and force are frequently
found in the present-day discourses (e.g., Howell, West & Cross, 1991; Godøy &
Jørsensen, 2001; Larson 1993; Larson & VanHandel 2005). How do we conceptualize
music and how does this conception shape the ﬁeld of music psychology? Reﬂections
on the early music psychology from the historical and critical perspective may well
serve to provide an opportunity of rethinking current psychology of music.

Cognitive Processes During Piano and Guitar Performance: An Eye
Movement Study
Satoshi Kobori, Katsunori Takahashi; Ryukoku University, Japan
5PM1-A01-3
The purpose of this study is to clarify the cognitive processes during music perfor-
mance, namely how the players view the music and make the ﬁngers move. This
paper focuses on preview time (hand eye span) in piano and guitar playing. Two
experiments were conducted, one was for piano and the other was for guitar. In
both experiments, musical score was presented on a computer screen, and subject
was required to play the instrument ﬂuently. The music pieces were well-known
piece, unknown piece and diﬃcult piece, and all were single melody music. The basic
characteristics of eye movements during piano and guitar playing were analyzed.
Also, the preview times were estimated using relation between eye tracking data and
ﬁnger movement data. The preview for the music was observed in all experimental
conditions for both instruments, but the preview times were diﬀerent among sub-
jects, music pieces, and trial numbers. The preview times were longer in the higher
skill subjects than in the beginner skill subjects. Also, the preview times were longer
for diﬃcult pieces than for easier pieces. The results suggested that skill level of
performers, diﬃculty of music pieces and knowledge for music pieces were crucial
factors which inﬂuenced the preview time.




                                         132
ICMPC 10                                                  Friday 29 August 2008


5PM1-S02 : Music Listening and Preferences /
Development / Methodology
Space 2, 14:10 – 16:00 Friday 29 August 2008, Poster session

Perceived Fitness of Music and Film Genre: Color, Light, Style and
Period Combinations
Blas Payri; Universidad Politécnica de Valencia, Spain
5PM1-S02-02
This experiment studies whether musical features are systematically associated
with visual features in an audiovisual context. The visual material consisted in 72
still pictures from recent (2004–2007) Japanese animation series, as animation has
clear genre codes (color palette; character, place and time period design). We used
stills rather than video to avoid interferences of speech, sound, gestures and shot
changes with the music. Music material included 72 excerpts from electroacoustic,
classical instrumental, classical Japanese and pop music works. 288 image-music
combinations were divided in 5 sets and were presented to 5 groups of subjects (130
audiovisual communication). Subjects saw an image while listening to the music
during 15s, then they rated from 1 to 5 the ﬁtness music-image if it were a ﬁlm
music. They rated also music tension and liveliness from 1 to 5.
Results show that 19th European classical music was better accepted for every image
style. Electroacoustic music was associated with futuristic images and with dark and
faded colors; 17th and 18th music with historic images, while pop music to bright
and light colours regardless of time and location. Diﬀerent codes are used depending
on the music genre and results basically agree with ﬁlm music handbooks.

Development of Infant Cry Acoustics: A Basis of Musical and
Linguistic Skills
Yulri Nonaka, Kentaro Katahira, Reiko Shiba, Kazuo Okanoya; RIKEN
Brain Science Institute, Japan
5PM1-S02-06
Infant cry gains complexity in acoustic and syntactical properties as it develops. This
complexity may be useful for the care giver to identify the need of the infant. Also,
intentional control of vocal apparatus to change pitch and rhythm maybe considered
as an origin of musical skills. In the present study, we obtained developmental
recordings of infant cries from two babies. We analysed unit sounds of cry through
the development and examined the nature of developmental changes and situational
speciﬁcity. Our analyses revealed that the acoustics of infant cry is monotonous
during the ﬁrst month. When overall tendency is considered, these parameters show
systematic U-shaped changes as the infant mature: duration becomes longer and
entropy decreases ﬁrst and then increase. When situational speciﬁcity is considered,
each of these parameters showed speciﬁc pattern of development through which
cries gain individuality and situational speciﬁcity. Voco-motor skill necessary to
express situational speciﬁcity maybe a basis of later musical skills.

The Role of Musical Environment at Home in the Infant’s
Development (Part 1): Japanese Mother’s Understanding and
Practice of Taikyo and Its Eﬀects on the Mother’s Labor and the
Infant’s Development
Akio Akasaka 1 , Mayumi Adachi 2 , Hitoshi Chino 2 ; 1 Kodoryoku
Kenkyukai, Japan; 2 Hokkaido University, Japan
5PM1-S02-08
In Japan, the word taikyo (prenatal training) is often heard among pregnant women
and their families. Some people believe and share their experiences that taikyo can
lead to the safe delivery and the healthy birth of the child, as well as the child’s
language and intellectual development. Do internet self-reports represent average
experiences of pregnant women in Japan? Can anecdotes of taikyo be veriﬁed
scientiﬁcally? To answer these questions, we conducted two studies, focusing on
eﬀects of the mother’s private music listening, playing music to the fetus, and singing
to the fetus. In STUDY 1, 30 mothers with children up to age 4 were interviewed at
home. All the mothers knew about the word taikyo during their pregnancy. The ma-
jority considered taikyo as the mother-child interaction in the womb. Private music


                                         133
Friday 29 August 2008                                                       ICMPC 10

listening was practiced by 10 mothers (3 as taikyo), playing music to the fetus by 11
mothers (6 as taikyo), and singing to the fetus by 12 mothers (7 as taikyo). Eﬀects
of taikyo were examined on the mother’s labor and on the child’s physical and other
forms of development. Children whose mothers listened to favorite music during
pregnancy came to understand easy words faster than their counterpart. In STUDY
2, we replicated the same study with 31 mothers and infants whose development of
vocal expressions and motor skills were recorded longitudinally during the ﬁrst two
years. In the poster, we report results from both studies, discussing implications of
the present study.

The Role of Musical Environment at Home in the Infant’s
Development (Part 3): 3-Month-Olds’ Responses to Music and
Their Subsequent Motor Development
Mayumi Adachi 1 , Akio Akasaka 2 , Izumi Kida 1 , Shunsuke Kon-no 1 ;
1
  Hokkaido University, Japan; 2 Kodoryoku Kenkyukai, Japan
5PM1-S02-10
Earlier studies suggest that music elicits repetitive body movements in 12-month-
old infants, and that it functions diﬀerently in 3-month-olds, depending on their
preferred body positions when they are placed alone. The purpose of this study
was three-folds: (1) to conﬁrm that music reduces 3-month-olds’ movements in a
supine position and increases their movements otherwise, (2) to examine whether
3-month-olds’ individual diﬀerences in response to music or in their preferred
position would relate to the musical environment at home, and (3) to explore whether
such individual diﬀerences could predict the course of their motor development
later in infancy. Total of 63 college students watched 5s-long video clips of 10
3-month-olds lying in a supine position and 5 lying or sitting in diﬀerent positions.
They judged how much an infant was moving in each video clip on a 6-point scale
with 1 as “not moving at all” and 6 as “moving a lot.” For 10 infants in a supine
position, their movements with music were judged less than those without music,
supporting the earlier study. Data analyses for 5 infants in other positions (e.g., lying
in a prone position, sitting on the mother’s lap) are still in progress at the time of
this writing. In the poster presentation, we will provide a full picture of what music
can aﬀord to young infants.

Interpretative Phenomenological Analysis and Music Psychology:
An Overview
Raymond MacDonald, Paul Flowers, Jane Oakland, G.N. Caldwell,
Julie C. De Simone; Glasgow Caledonian University, UK
5PM1-S02-12
Qualitative methods are now ﬁrmly established as an important approach to research
within the psychology of music. Over the past 15 years there has been a signiﬁcant
increase not only in the volume of published research adopting qualitative methods
but also in the range of diﬀerent qualitative approaches that can be legitimately
utilised. This paper overviews Interpretative Phenomenological Analysis (IPA) as
a methodology for collecting and analysing qualitative data in Music Psychology
Research. The key elements of IPA are outlined and compared to other forms of
qualitative analysis such as thematic analysis and discourse analysis. A review of
existing studies within music psychology is presented along with key challenges to
researchers interested in developing IPA. The paper will demonstrate how IPA, as a
rigorous, innovative and now well established methodology can be employed within
music psychology research to further our knowledge over a range of diverse but
related areas. Its utility as a research approach across a range of topics areas such as
musical identities, music therapy, music education and motivation and music will be
presented. This paper highlights the potential to develop IPA as a valuable research
tool that could make a considerable contribution to the growing body of qualitative
work within the ﬁeld of Music Psychology.

The Complex Dynamics of Repeated Musical Exposure
Patrick C.M. Wong 1 , Anil Kumar Roy 1 , Elizabeth Hellmuth Margulis 2 ;
1
  Northwestern University, USA; 2 University of Arkansas, USA
5PM1-S02-14
Repetition is a fundamental part of engagement with the auditory world; it inﬂuences
what gets perceived as important and selected for attention. It also inﬂuences the



                                          134
ICMPC 10                                                Friday 29 August 2008

mysterious phenomenon of aesthetic enjoyment. This project aims to identify the
perceptual and cognitive non-linearities that underlie changes in preference across
multiple musical exposures.
We report on a set of experiments that used behavioral measures to assess perceptual
changes across multiple exposures (ﬁve over one week) of a twenty-minute orchestral
piece. The behavioral tests included tasks related to preference, memory, attention,
aﬀect, syntax, and error detection. Participants performed these tasks after each of
the ﬁve exposures in response to stimuli drawn from the exposure piece and from a
control piece.
Across the ﬁve exposures, we found an inverted U-shape preference response,
which possibly is decomposable to the complex interaction of multiple nonlinear
components, represented by performance on tasks relating to memory, attention,
aﬀect, syntax, and error detection.
We argue that musical preference has remained a puzzle because the behavior
depends on an underlying complex system with numerous nonlinear cognitive
components revealed by our results. [Work supported by a grant from the Center for
Interdisciplinary Research in the Arts at Northwestern University, and by NIH]

A Ground-Truth Experiment on Melody Genre Recognition in
Absence of Timbre
José M. Iñesta, Pedro J. Ponce de León, José L. Heredia-Agoiz;
Universidad de Alicante, Spain
5PM1-S02-16
Music genre or style is an important metadata for music collections and database
organization. Some authors claim for the need of having ground truth studies on this
particular topic, in order to compare results with them and lead to sound conclusions
when analyzing software performances. When dealing with digital scores in any
format, timbrical information is not always available or trustworthy so we have
avoided this information in our computer models, using only melodic information.
The main goal of this work is to assess the human ability for recognizing music
genres in absence of timbre in order to assess comparatively the performance of
computer models for this task.
For this, we have experimented with fragments of melodies in absence of accompa-
niment and timbre, as our computer models do. For this particular paper we have
worked with two well-established genres in the music literature, like classical and
jazz music.
A number of analyses in terms of age, group, education, and music studies of the
people subjected to the tests have been performed. The results show that, on
average, the error rate was about 18%. This value shows the base line to be improved
for computer systems in this task without using timbrical information.

The Role of Music in Videogames: The Eﬀects of Self-Selected and
Experimenter-Selected Music on Driving Game Performance and
Experience
Gianna Cassidy, Raymond MacDonald; Glasgow Caledonian
University, UK
5PM1-S02-18
Videogames present a uniquely interactive yet relatively untapped platform for
musical experience, with music providing a means of communication, interaction
and expression (Lipscomb & Zehnder, 2004; 2006; Whalen, 2004). This raises two
relatively unexplored areas of interest for the psychology of music (1) the uses and
functions of music during gameplay, and (2) the elucidation of music’s contribution
to gameplay performance and experience. The present paper addresses both these
needs through a summary of (1) a survey study, and (2) a series of three experimental
studies, which adopted a listener-centred approach to establish investigation of
music in videogames. Experiment 1 compared the eﬀects of exposure to self-selected
and experimenter-selected music on driving game performance and experience. The
ﬁndings of Experiment 1 were extended in the subsequent studies, with Experiment
2 focusing on the role of tempo, and Experiment 3 focusing on the role of vocal
content. The collective ﬁndings highlight the eﬃcacy of self-selected music as a tool
to optimise both performance and experience in the gameplay context. Additionally,
the survey ﬁndings highlight the pervasive and highly personalised nature of music
listening during gameplay, and the genre dependent nature of its use.


                                        135
Friday 29 August 2008                                                       ICMPC 10

When do People Feel a Sense of Incongruity in Listening to Music?
Takashi Taniguchi; Osaka Gakuin University, Japan
5PM1-S02-20
The purpose of this paper is to collect and classify events and situations related to
music and a sense of incongruity in order to investigate on what and when people
feel a sense of incongruity in musical behavior, especially listening to music. In Study
1, 118 participants were asked to describe when they feel a sense of incongruity in
listening to music, singing songs, and playing instruments on a free narrative form
of questionnaire. Total descriptions were 181 in listening, 170 in singing, and 151 in
playing instruments. Free descriptions showed 6 categories of a sense of incongruity
in music. In Study 2, on the basis of the descriptions collected in Study 1, another
questionnaire was constructed of 80 items about a sense of incongruity in listening to
music. Other 141 graduates participated to ﬁll in the questionnaire. Factor analysis
yielded 7 factors: playing deviation, image inconsistency, situational incompatibility,
unnatural performance, noise and interruption, displeasure, and oﬀ-balance. It
shows various factors contribute to a sense of incongruity in listening to music. The
factor pattern suggests the sense is experienced as a result of interaction between
listening situation and listeners’ condition in addition to musical and acoustical
quality.

Musical Trends Among Japanese Young People: The Boom of
‘J-Pop’
Junko Matsumoto 1 , Shoko Kobayashi 2 ; 1 Nagano College of Nursing,
Japan; 2 Suzaka Municipal Oﬃce, Japan
 5PM1-S02-22
This study investigated preferences with respect to music in the daily lives of
young people and the attraction of Kumi Koda who has become popular among
the youth of Japan, aiming to reveal a relationship among these variables. Results
indicated that most participants preferred listening to music, listened to music
frequently, and felt it necessary to have music in their daily lives. There were positive
and signiﬁcant correlations among the items related to music, but the correlation
between interests in trends and the items related with music was small. Favorite
musicians were almost always ‘J-pop’ musicians, and participants were attracted to
‘J-pop’ music, including both words and melodies. Factor analysis of the attraction to
Kumi Koda yielded three factors: Healing, Appearance, and Fashion. There were few
signiﬁcant correlations between the attraction of her and the items related to music;
however, there were positive, signiﬁcant correlations between the attraction of her
and interests in trends. These results suggest that most young people in Japan listen
to music in their daily lives and prefer the music of ‘J-pop’ musicians. However, the
results also suggest that, with regard to musicians who are currently part of a boom,
young people may prefer them due to their trendiness rather than their music.

Short Term Memory for Music in Patients with MCI and Early Stage
of Dementia
Manuela Kerer, Josef Marksteiner, Elisabeth Weiss; Medical University
Innsbruck, Austria
5PM1-S02-24
There is evidence from a variety of sources that the system for short term memory
for pitch is precisely and systematically organized, in several ways along the same
lines as the system that processes pitch information at the incoming level. In this
study we tested short term memory for music in patients with MCI (Mild Cognitive
Impairment) and early stage of dementia. The most noticeable deﬁcit in these
patients is short-term memory loss and the consequent problems to acquire new
information. Although memory span varies widely with populations tested and
with material, human short-term memory in general has a forward memory span of
approximately 7 items ± 2.
A new test was created with 7 tone-lines and 6 intervals. Subjects were presented to
these test items, which were followed by 3 similar and 1 identical items. They were
instructed to choose the right one. 10 patients with MCI and 10 with diagnosis of
early stage of dementia were compared with 23 controls.
Results are in accordance with the proposed hypothesis that subjects with diagnosed
MCI and early stage of dementia show signiﬁcantly lower results for almost all
presented tasks than controls. Respectively, MCI patients showed better results than
early-stage-patients.


                                          136
ICMPC 10                                                  Friday 29 August 2008

Thinking in Two vs. Three Beats: Metric Structure is Represented
in Multiple Cortical Areas as Revealed by
Magnetoencephalography in Skilled Musicians
Takako Fujioka, Benjamin Zendel, Bernhard Ross; Rotman Research
Institute, Canada
5PM1-S02-26
People tapping to music often tap to metric structure, deﬁned by strong and week
accents, rather than to each pulse. This internalized metric structure is inﬂuenced
by individual experience involving body and head movement. The current study
investigated where and when diﬀerent brain areas are involved in metric processing
when tapping experience deﬁnes meter even if stimulus are unaccented using mag-
netocephalography (MEG). Skilled musicians tapped for twelve clicks to every second
(2-beat) or third (3-beat) click, alternated with a subsequent resting period of the
same length. Time-averaged MEG data during the resting period were transformed
to source activities in brain volume using beamformer technique to extract multiple
sources without a-priori assumptions about number and conﬁguration of sources.
Results show that in addition to bilateral auditory activities, bilateral sensori-motor,
prefrontal, left inferior gyrus, left pre-supplemental motor area, and then sensori-
motor areas became active even before the next click. Similar periodic pattern of
activation was observed in left angular gyrus, and supra marginal gyrus, cerebellum
but peaking around between the clicks. We suggest that metric interpretation has a
top-down eﬀect on neural activities in multiple cortical areas, likely reﬂecting motor
imagery and timing encoding.


5PM2-S01 : Neuroscience and Pathology
Space 1, 16:10 – 17:40 Friday 29 August 2008, Oral session

A Role for Pitch Memory in Congenital Amusia
Lauren Stewart 1 , Claire McDonald 1 , Sukhbinder Kumar 2 , Diana
Deutsch 3 , Timothy D. Griﬃths 2 ; 1 Goldsmiths University of London,
UK; 2 Newcastle University, UK; 3 University of California at San Diego,
USA
5PM2-S01-1
Congenital amusia (CA) is a perceptual agnosia, deﬁned as abnormal musical
perception in the presence of normal hearing and otherwise preserved cognition.
The disorder has been characterized by elevated thresholds for the discrimination
of changes in pitch direction. The present study tested the hypothesis that CA
individuals also have diﬃculty with the retention of pitch over time. A group of CA
participants (n=16) was compared with a matched group of non-amusic controls.
Two tones were presented over headphones with an intervening silent pause of
variable duration (2s, 5s, 8s). The tones were either the same or diﬀered in pitch
by two semitones (suprathreshold for pitch discrimination in both groups). There
was a signiﬁcant main eﬀect of group, a signiﬁcant main eﬀect of condition and an
interaction between group and condition. The CA group performed signiﬁcantly
worse than the control group in all conditions, despite normal performance on digit
span — test of verbal working memory. This indicates that pitch retention, and not
only pitch perception, is deﬁcient in amusia and suggests that the disorder might
be considered a deﬁcit of dynamic pitch tracking. Ongoing studies will determine
whether this memory deﬁcit relates to an impairment of storage or rehearsal of pitch
information.

Investigating the Language and Music Lexicons Using Repetitive
TMS
Rebecca Sussex 1 , Sarah Wilson 1 , David Reutens 2 ; 1 University of
Melbourne, Australia; 2 Monash Medical Centre, Australia
5PM2-S01-2
Repetitive transcranial magnetic stimulation (rTMS) is a technique used to brieﬂy
disrupt brain activity in small areas of the neocortex. It can be used to determine
whether a particular area is necessary for a cognitive function. The current study is
the ﬁrst to use rTMS to disrupt singing and speaking by stimulating regions in the
temporal lobes thought to underpin the music and language lexicons. Participants
underwent a structural MRI scan which was used to co-register scalp locations with


                                         137
Friday 29 August 2008                                                     ICMPC 10

underlying neocortex sites. During two subsequent testing sessions participants
produced three highly familiar excerpts (song, text & melody) while receiving sham
and active rTMS on the left and right temporal lobes. Performances were video and
audio recorded for the purposes of analysis. Recruitment for the study is ongoing
and will be completed by July 2008. Pilot data indicates that rTMS of the right
temporal lobe interrupted singing but not speaking. In contrast, rTMS of the left
temporal lobe selectively interrupted speaking, although the disruption was more
subtle than that seen for singing after right rTMS. Further data will determine the
robustness of this eﬀect. These preliminary ﬁndings support the results of previous
studies that indicate that music and language functions may in part be mediated by
separate neural networks.

A Neurobiologically Plausible Schema for Auditory Information
Processing Including the Auditory Cortex
Neil McLachlan, Sarah Wilson; University of Melbourne, Australia
5PM2-S01-3
The broad range of neural responses observed in studies of the auditory cortex will be
better understood in relation to a model of neural mechanisms. This paper presents
a ﬁrst approximation to such a model which can be tested by speciﬁc experimental
studies. The principle feature of the model is a mechanism of streaming information
based on stored templates of activation while the echoic trace is formed. It is
proposed that the echoic trace is formed along an array of neural circuits in the
thalamus that are sequentially activated by quality speciﬁc disinhibition from clas-
siﬁcation hierarchies in the primary auditory cortex. The streamed pitch, loudness
and direction information is accumulated in neural arrays in the primary auditory
cortex before being associated with source identity in the associative cortices in
the superior temporal and other brain regions. A successful model will enable an
artiﬁcial listening system to be developed to provide objective and reproducible
predictions of human responses to acoustic stimuli. This will have applications in the
analysis and composition of music outside the Western tonal cannon, the design and
analysis of musical instruments, and automatic music transcription. Such a model
will also have implications in a wide range of ﬁelds including hearing disorders,
environmental acoustic monitoring and robotic sensing.




                                        138
