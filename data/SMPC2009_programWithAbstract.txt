                                           Monday, August 3rd

Welcome
10:00 – 10:10             Dr. Tonya Bergeson, Indiana University School of Medicine


President‟s Address
10:10 – 11:00             Dr. Aniruddh Patel, The Neurosciences Institute


Session 1A                Performance I                         (Chair: Roger Chaffin)

1A) 11:10 – 11:30

Evaluating expressive performance: How performance individuality and structural complexity affect
the preferences of experienced listeners

Stacey Davis
University of Texas, San Antonio

   Purpose
   Many empirical studies have investigated the relationship between expressive performance and musical structure, but
   fewer have examined how expression affects judgments of performance creativity and opinions about preference.
   Some research suggests that the relationship between originality and preference could be depicted with an inverted-U
   shape (just as with complexity and arousal). In this case, experienced listeners would prefer more idiosyncratic
   performances, with the unpredictable expressive deviations providing novelty that captures attention and generates
   liking. Other research highlights the interaction between performance originality and structural complexity, with simple
   music requiring more ―structurally ambiguous‖ performances to engage interest and elicit preference. Given these
   suggestions, what occurs when experienced listeners encounter music that is both familiar and complex? Does
   familiarity create a preference for performance novelty or does an awareness of complexity engender preference for
   more conservative interpretations?
   Methods
   Two excerpts from Bach‘s unaccompanied violin pieces were chosen, along with commercial recordings of both
   excerpts by ten expert violinists. For each performance, participants provided ratings for perceived tempo, technical
   goodness, amount of performance expression, and overall preference. Ratings of perceived tempo were compared to
   measurements of actual tempo, while ratings for technical goodness, expression, and preference were compared with
   perceived complexity and amount of musical training and/or familiarity with this repertoire.
   Results
   As predicted, results indicate a significant, positive correlation between perceived tempo and actual tempo and
   between perceived technical goodness and overall preference. For the less complex excerpt, participants preferred
   performances judged to have a greater amount of expressive individuality. As perceived complexity increased,
   participants preferred the performers that adopted a more conservative expressive approach. These results were
   more pronounced as overall experience and familiarity increased.
   Conclusion
   Overall, the evaluations of expression and preference were affected more by complexity than by familiarity. Although
   strength of correlation varied with experience, participants generally preferred less exaggerated performances of the
   more complex excerpt. This suggests that sufficient structural complexity engages listener attention, even when the
   music is familiar and the performances are expressively conservative (or even strict).
   Research and/or Educational/Clinical Implications
   Although musical expression is subjective and individual, a greater awareness of the relationship between complexity,
   familiarity, and preference can help performers make more informed expressive choices. The results of this study
   therefore have implications for how we teach musicians to understand musical structure and produce expressive
   performances that are positively evaluated and generally enjoyed.




                                                            5
1A) 11:30 – 11:50

Brain science to performance: the view from a pianist‟s bench

Lois Svard
Bucknell University

    Purpose
    How can a performing musician make practical use of the information gained from the myriad studies in the
    neuroscience of music, or for that matter, from recent non-music-based neuroscience research? This question serves
    as the impetus for the presentation here of research findings that have great potential for practical application in the
    study of music. During the past two decades, the brain basis of almost every aspect of music has been studied, often
    with corresponding brain scans, from the basic musical building blocks of melody, harmony, and rhythm, to more
    complex issues such as emotion in music, sight-reading, motivation, performance anxiety, improvisation and memory.
    But few attempts have been made to translate these research findings into practical applications for those of us who
    actually perform and teach music. One area of particular interest to any performer and teacher is the learning of a
    new piece of music. In considering the Western European tradition of music, the learning of a score is basic to the
    entire process of making music, whether one plays at an elementary or at a concert level. How can one be most
    effective, not just in learning quickly and accurately but more importantly, in playing musically as well?
    Methods
    This presentation reviews some diverse studies that can offer new insights into the process of learning music. I begin
    by reviewing selected literature concerning mental motor practice, mirror neurons, and the effect of different teaching
    methods on specific brain activation patterns.
    Results
    In some cases, these studies confirm conventional wisdom about the teaching of music. In others, they suggest
    exciting new ways to approach practice that may ultimately make us stronger musicians.
    Conclusion
    There is no question that, over the past two decades, research in the neuroscience of music has uncovered
    tremendous knowledge about the complex art of making music. Performers are vitally interested in learning how to
    translate that knowledge into practical applications for the study and performance of music.
    Research and/or Educational Implications
    Experimental teaching applications in my own studio based on the research cited in this presentation suggest that
    musicians can benefit a great deal from teaching practices derived from neuroscience research. This is an ideal time
    for researchers and practicing musicians to work together to define possible research directions that may contribute to
    promising educational practices in the musician‘s studio. This presentation is an attempt to promote that collaboration.


1A) 11:50 – 12:10

Individual differences in muscle tension and air support during trumpet performance
                  1                  2                     3
Jonathan Kruger , James McLean , and Mark Kruger
                                 1              2                            3
Rochester Institute of Technology , SUNY-Geneseo , Gustavus Adolphus College

    Purpose
    Research done on muscle tension in brass performance has focussed on the embouchure (e.g., White and
    Basmajiian, 1973), on remedying embouchure problems using biofeedback (Hauser and McNitt-Gray, 1991), or
    selective control of individual muscles within the face and embouchure (Lapatki, Stegeman, & Jonas, 2002). For string
    players, a larger body of literature has examined the relationship between muscle tension, perceived exertion and
    fatigue, or reduction of muscle strain (c.f. Chan et. al., 2000; Berque and Gray, 2002; Fjellman-Wiklund et. al., 2003;
    Shan et. al. 2004) Although Henderson (1979) demonstrated tension in the throat of trumpet players changes with
    pitch, much less work has been done on general body tension in brass performers. Our study examines changes in
    muscle tension in the shoulder and lower back in the context of changes in mouthpiece pressure applied to the
    embouchure, intra-oral air pressure used to energize the embouchure, and breathing patterns during a musical task
    and as a function of expertise. We hope to identify strategies that performers use to create air pressure which are
    counter-productive because they lead to extraneous muscle tension and/or mouthpeice pressure on the embouchure.
    Method
    Thirteen trumpet players played a concert Bb scale, arpeggios differing in articulation and dynamics, and excerpts
    from the Haydn Trumpet Concerto. Extraneous muscle tension was measured electromygraphically at the left
    trapezius (shoulder) and the lower back. Measures were also made of expansion and contraction of the upper chest
    and abdomen, air pressure inside the mouth measured by placing a fine tube (attached to a gas pressure sensor)
    inside the performer‘s mouth, and mouthpiece pressure on the embouchure recorded by placing a force sensor in a

                                                               6
   sleeve between the mouthpiece and the instrument. A trumpet mounted camera recorded embouchure movement
   during performance.
   Results
   Successful performers demonstrated selective control of extraneous muscle activity and created the highest levels of
   air pressure at the embouchure. This control was most apparent in high ranges of the arpeggios and during difficult
   passages of the Haydn concerto.
   Conclusions
   By observing muscle tension, internal air compression, breathing patterns, mouthpiece pressure, and sound
   simultaneously it is possible to see differences in physical approaches to wind instrument playing as a function of both
   individual expertise and musical task.
   Educational/Clinical Implications
   These observations support pedagogical approaches to brass teaching that make students aware of the effect of
   excess muscle activation at either the embouchure or in other muscle systems on performance.
   Acknowledgement of Research Funding
   Rochester Institute of Technology and Gustavus Adolphus College


1A) 12:10 – 12:30

Temporal patterns and formal structures in the performance of an unmeasured prelude for
harpsichord

Meghan Goodchild, Bruno Gingras, Pierre-Yves Asselin, and Stephen McAdams
McGill University

   Purpose
   Several studies have recently renewed interest in the relationship between music-theoretical analysis and
   performance by advocating a collaborative view between analyst and performer (Lester 1995; Rink 1995). Gingras,
   McAdams and Schubert (2009) investigated the connection between the performer‘s concept of a piece as an analyst
   and performer to circumvent the apparent disconnect between the language and goals of music theory and
   performance. They observed that organists used a strategy of ―phrase-final lengthening,‖ the degree of which
   corresponds to the hierarchical importance of major subdivisions in their written analyses. However, individual
   organists‘ analyses did not correspond unequivocally to their tempo profiles. The present study continues this
   research by examining the analyses and performances of an unmeasured prelude for harpsichord. This genre, which
   allows considerable interpretative freedom regarding its temporal structure, provides an ideal context for further
   analysis of the link between temporal patterns and formal structure.
   Methods
   Twelve professional harpsichordists were invited to perform the Prélude non mesuré No. 7 by Louis Couperin on a
   MIDI harpsichord, which allows precise measurement of performance parameters. Immediately following their
   performances, the harpsichordists were invited to submit their own analyses of the piece, indicating its main formal
   subdivisions. Performances were matched to the score using an algorithm developed by Gingras (2008).
   Results
   A comparison of the written analyses shows that, despite a fair amount of individual variation, the harpsichordists
   generally agreed on the main structural boundaries. Their performances, however, displayed considerable variation in
   acceleration and/or deceleration patterns. Through detailed analyses of performance parameters such as tempo
   fluctuation, articulation, and velocity, we show how harpsichordists conveyed their structural interpretation of the piece,
   assess whether these interpretations correspond to their written analyses, and compare structural readings of the
   piece offered by different performances.
   Conclusion
   Although the unmeasured prelude is a genre characteristic of the Baroque era, no empirical research has been
   conducted on it or on harpsichord performance in general. By showing that performances of unmeasured preludes
   display a wide range of possible structural realizations which is more varied than what is found in measured music,
   this study constitutes a first step in characterizing this neglected genre which stands between strictly-notated music
   and improvisation.
   Research Implications
   This study will contribute new insights into the complex relationship between performance and analysis by focusing on
   the actualized music rather than score-based analytical readings, thus proposing a reevaluation of the performer‘s role
   in music analysis.
   Acknowledgement of Research Funding
   CIRMMT, NSERC and a Canada Research Chair to Stephen McAdams.




                                                             7
Session 1B                 Genre                                 (Chair: Alexander Rozin)

1B) 11:10 – 11:30

Music genre schema construct accessibility in evaluation of a charity

Mark Shevy
Northern Michigan University

    Purpose
    The present study explores the cognitive influence of country music and hip-hop music genres in a persuasive
    message. In doing so, it works to further define genre as a cognitive schema; a definition that has been suggested
    (e.g., Bordwell, 1989; Shevy, 2008; Zillmann & Bhatia, 1989) but been barely researched empirically. Prior schema
    research in other contexts has shown that a primed schema activates associated concepts, making those concepts
    more accessible for use in evaluation (e.g., Iyengar & Kinder, 1987). The present study continues this line of research
    by examining the effect that primed music-genre schemas might have on the evaluation of a charity presented in an
    appeal presented in association with a piece of music.
    Methods
    An online experiment presented participants (n=182) at a large Midwestern university with an ostensibly live audio
    recording of a music performer making a charity appeal between songs at a concert. Participants were randomly
    assigned to a country music, hip-hop music, or no-music condition. In the music conditions, a few seconds of either
    country or hip-hop music preceded the appeal, and a few seconds of the respective music followed it. The appeal itself
    (approximately 100 seconds long) was the same recording across all three conditions. After hearing the recording,
    participants answered questions regarding their attitude toward the charity, how much money they would probably
    give to it, and their perception of the performer.
    Results
    Results showed that brief exposure to country music caused perceived performer friendliness and political ideology to
    become positively correlated with charity evaluation, whereas exposure to hip-hop music caused perceived performer
    expertise to become correlated with attitude toward the charity. Also in the hip-hop condition, perceived liberal
    ideology of the performer correlated with positive evaluation for liberal participants. Trustworthiness was significantly
    correlated with evaluation in the country condition and in the no-music control; the correlation was marginally
    significant in the hip-hop condition.
    Conclusion
    Exposure to a music genre does not necessarily make audiences more or less favorable toward the charity, but it
    causes them to consider different characteristics when making their decision.
    Research and/or Educational/Clinical Implications
    The present research supports the definition of music genre as a type of cognitive schema. Future research should
    further investigate the nature of these schemas in various contexts.


1B) 11:30 – 11:50

The effect of musical and visual components on genre classification and plot continuation in the
opening credits of Hollywood feature films

John Hajda
University of California, Santa Barbara

    Purpose
    Music can substantially change the interpretation of a visual scene. Vitouch (2001) found that, for the opening
    segments of a film, musical underscoring was a salient feature for plot continuation. This study is the first to
    systematically investigate opening segments of films of different genres. Specifically, this research investigates the
    effect of musical underscoring on expectations about genre and plot.
    Methods
    Stimuli: 32 stimuli were generated from opening credit segments of Hollywood feature films. Altogether there were 8
    original clips—two from each of the following genres: adventure, comedy, horror and romance—and 24 ―fake‖ clips, in
    which the visual elements from a film of one genre (e.g. comedy) were combined with the musical underscoring from a
    film of a different genre (e.g. horror). Each stimulus contained visual scenes, credits and titles with names matted out,
    and musical underscoring.
                                                             8
   Experimental Task
   Each subject was presented with four stimuli; no subject saw the same visual scene or heard the same musical
   underscoring twice. After each stimulus presentation, a subject (1) gave the genre to which the film likely belonged
   and (2) wrote a story (plot continuation) about what might happen immediately following the presented clip.
   Results
   Results show that, overall, music is more salient than visual features in determining genre classification and whether a
   plot outcome is positive or negative. This is likely due to the absence of characters or plot. One notable exception for
   genre classification was generated by the opening images from the romance film Emma (1994), during which a planet
   rotates quickly against a starry background. This scene elicited genres and plots related to science fiction themes
   regardless of underscoring.
   Conclusion
   This study, conducted in light of Cohen‘s (2005) Congruence-Associationist Model, stands in contrast to existing
   theories on multimedia cognition. Many Hollywood films begin with opening credits, and this research indicates that
   music, not visual scenes and text, is more salient in communicating genre and general plot expectations. Future
   research will attempt to determine conditions under which music or visual saliency predominates.
   Research Implications
   The larger goal of this research is to increase understanding of the processes by which meaning and interpretation
   occur in multimedia contexts. Therefore, this research impacts the fields of film theory, music theory and psychology
   (multimedia cognition).
   Acknowledgment of Research Funding
   Hellman Family Faculty Fund Fellowship, UC Santa Barbara


1B) 11:50 – 12:10

Participatory discrepancies and the perception of beats in jazz

Matthew Butterfield
Franklin and Marshall College

   Purpose
   What generates ―swing‖ in a jazz rhythm section? Charles Keil proposed that swing stems specifically from
   asynchronous timing between bass and drums in their shared articulation of the beat, a phenomenon he dubbed
   ―participatory discrepancies,‖ or PDs. The ―push and pull‖ between these instruments purportedly generates a
   ―productive tension‖ thought to drive the groove with energy, prompting listeners to tap their feet and bob their heads
   along with the beat. No one, however, has proven that PDs are actually available to human perception, much less that
   they do in fact produce swing. This paper presents the results of two experiments on the perception of PDs and their
   purported effects.
   Methods
   Experiment 1 employed synthetic recordings of a conventional swing groove in which the onset asynchronies between
   bass and drums were varied between 10, 20, and 30 ms at three different tempos. Participants used three different
   listening strategies in an effort to perceive the asynchrony and its purported effects. Experiment 2 employed
   recordings of professional jazz musicians and tested for the effects of learning in the perception of PDs.
   Results
   Average scores on both experiments did not exceed chance levels, indicating that most listeners are unable to
   perceive PDs or their purported effects as predicted in the PD framework. Participants had marginally more success in
   perceiving drum leads than bass at higher tempos. Widening the asynchrony modestly facilitated perception of bass
   leads, though it had no significant effect on the perception of drum leads. Repeated listening did not help, but a few
   individuals (14-18%) could perceive the discrepancy fairly effectively with one particular listening strategy. Neither
   formal musical training nor a stylistic preference for jazz distinguished these individuals, however; they appear simply
   to have a greater perceptual acuity for temporal discrimination than others.
   Conclusion
   These experiments failed to confirm that most ordinary listeners are able to discern the discrepancy between bass and
   drums with any consistency across a range of tempos and timing values. The expressive effects of PDs, though not
   negligible, are quite modest, minimally salient, and thus not likely the driving force behind the production of swing.
   Research and/or Educational/Clinical Implications
   Though not inconsequential, the expressive effects of PDs must be understood as more limited and local than
   previously thought. The driving force behind the swing groove probably stems from aspects of syntactical pattern, and
   not subsyntactical timing processes. PDs may temper or condition the motional energy of this groove, but they do not
   likely generate it.




                                                            9
1B) 12:10 – 12:30

Genre identification of very brief musical excerpts

Sandra Mace, Cynthia Wagoner, and Donald Hodges
University of North Carolina at Greensboro

   Purpose
   The purpose of this study was to examine how well individuals are able to identify different musical genres from very
   brief excerpts and whether or not musical training, gender, and musical preference play a role in genre identification.
   Methods
   Listeners were asked to identify the genre of classical, jazz, country, metal, and rap/hip-hop excerpts that were 125
   ms, 250 ms, 500, or 1000 ms in length. Participants were 219 students recruited from two universities in the
   southeast region of the United States.
   Results
   The results indicated that participants were very successful at identifying genres of very brief excerpts. Overall, they
   were correct 75% of the time. At 1000 ms, accuracy reached 89% for all genres combined and 93% and 91% for
   Classical and Jazz, respectively. Even at 125 ms, listeners performed well above chance: Classical (71%), Jazz
   (38%), Country (56%), Metal (51%), and Rap (38%). Clearly, these participants were able to make fairly accurate
   judgments on the basis of extremely limited information.
   In general, the length of excerpt made a significant difference in performance, with longer time lengths leading to
   greater accuracy. No significant differences were found between those with musical training and those without.
   Generally, there were no significant differences between males and females and expressed preference did not play a
   significant role in listener‘s ability to identify genres of different lengths.
   Conclusion
   These findings support a primary conclusion that people are very adept at identifying particular musical styles when
   presented with excerpts that are one second in length or less.
   Research and/or Educational/Clinical Implications
   Previous research has indicated that listeners can make emotional judgments very rapidly and the current results
   extend this ability to the identification of specific genres. Based on Ashley‘s (2008) findings that listeners can make
   judgments of valence (happy/sad) and mode as rapidly as 100 ms (and in some cases in 50 ms), it is tempting to
   speculate that genre identification takes slightly longer. It is also possible that valence judgments and genre
   identifications are made by different cortical pathways. However, further studies are needed to test these hypotheses.
   Numerous additional studies could be designed to explore this fascinating and quite extraordinary ability to make rapid
   musical judgments based on extremely brief snippets of sound.


Session 2A                Performance II                        (Chair: Bruno Repp)

2A) 2:00 – 2:20

After-effects of alterations to the timing and pitch of auditory feedback during sequence production at
the keyboard

Peter Q. Pfordresher and John David Kulpa
University at Buffalo, SUNY

   Purpose
   Understood is the tendency for pitch alterations and asynchronous feedback to affect, respectively, error rate and
   timing in the performance of musical sequences. The following three experiments were designed to investigate the
   effects of these alterations on performance both during their application and after their withdrawal. Of interest was to
   determine if feedback alterations of pitch and/or synchrony would exert their effects immediately or gradually and
   whether the effects would be persistent or transient.
   Methods
   In all experiments, each trial consisted of three segments and began with a 500ms metronome. First, participants
   synchronized with the metronome and heard normal feedback. Second, participants heard altered auditory feedback
   while attempting to maintain the metronome rate. Third, altered feedback either reverted to normal or was removed
   while participants continued to attempt maintaining the same rate.
   Participants either tapped (experiment 1) or played an eight-note melody from memory (experiments 2 and 3).
   Feedback alterations as well as the number of keystrokes in segment 2 varied across experiments. In experiments 1
   and 2, participants‘ feedback was asynchronous relative to their keystrokes. The number of keystrokes was 1, 2, 4, 8,


                                                           10
   or 16. In experiment 3, participants‘ feedback was the note which corresponded to the immediately preceding
   keystroke. The number of keystrokes was 1, 8, 16, or 32.
   Results
   Experiment 1 showed that participants‘ production slows significantly when feedback is asynchronous. Furthermore,
   this effect did not depend on the number of asynchronous events. Time-series analyses indicated that slowing was
   effected immediately and that recovery upon termination of the alteration was rapid. That these results generalize to
   more typically musical behaviors was shown by the largely equivalent findings of experiment 2.
   Experiment 3 found that, during segment 2, error rates increased as the number of pitch alterations increased,
   suggesting that their effect takes place gradually over time. Importantly, pitch alterations continued to exert their effect
   on error rate even in segment 3 when these alterations were no longer being applied.
   Conclusion
   These results indicate that alterations to the timing of a person‘s productions have effects which are immediate, but do
   not persist. Conversely, alterations to pitch have effects which persist, but take time to build up.
   Research and/or Educational/Clinical Implications
   The studies herein described suggest the existence of separate timing and sequencing mechanisms for tasks in which
   perception and action are dynamically interrelated.
   Acknowledgement of Research Funding
   NSF Grant 06042592


2A) 2:20 – 2:40

Analyzing expressive timing data in music performance: A multi-tiered time-scale sensitive approach

Panayotis Mavromatis
New York University

   Purpose
   We develop a method for analyzing expressive timing data, aiming to uncover rules which explain a performer‘s
   systematic timing manipulations in terms of the music‘s structural features.
   Methods
   We adopt a multi-tiered approach. We first identify a continuous tempo curve by applying non-linear regression
   analysis to inter-onset durations extracted from an audio recording. Our non-parametric regression employs radial
   basis functions, thus avoiding rigid a priori assumptions about which specific factors contribute to the tempo
   fluctuations (e.g. grouping structure) or what the functional form of that contribution is (e.g. parabolic segments). Once
   the effect of tempo is factored out, subsequent tiers of analysis examine how the performed subdivision of each metric
   layer (e.g. quarter note) deviates from an even rendering of the next lowest layer (e.g. two equal eighth notes) as a
   function of time. The contribution from each metric layer is factored out before the next one is analyzed.
   Results
   We analyzed harpsichord performances of Bach preludes from commercially available recordings. The most salient
   factors shaping the tempo curve (Fig. 1) are • an initial small accelerando and a pronounced final ritardando;
   • less pronounced, but consistent ritardandi leading to important cadences, with magnitude reflecting the cadence‘s
   depth in the tonal hierarchy; • small but measurable contrasts in tempo to differentiate sections marked off by
   distinctive texture or tonal function. The metric layer analysis begins by examining the timing of individual measures in
   response to specific features of the music (Fig. 2). Variations of this type include lengthening a measure that
   • begins a hypermetric pair; • effects tonal arrival or resolution of a dissonant chord; • contains unexpected material. In
   subsequent metric layers, the nature of the deviations from exact subdivisions varies with metric depth. Systematic
   variations include a lengthening of the metrically strongest member of the subdivision (agogic accent). It is perhaps
   most remarkable that, even though deviation from exact subdivision is free to vary on a point-by-point basis, the
   observed deviations sometimes vary smoothly over extended time spans, typically corresponding to formal units such
   as phrases (Figs. 3-4). This phenomenon typically occurs on specific metric layers that perhaps bear some special
   relation to the tactus.
   Conclusion
   Our analysis confirms that expert performers systematically manipulate timing to communicate musical structure.
   Manipulations are typically linked to specific structural features of the music, and are manifested differently at different
   time scales.
   Research Implications
   The data-analytic technique proposed here can be used to quantitatively model the mapping of musical structure to
   expressive timing patterns. Consistently executed timing deviations can shed light on mental representation of the
   music in the performer‘s long-term memory, offering indirect evidence that complements results established through
   controlled laboratory experiments.



                                                             11
2A) 2:40 – 3:00

Emotional and neural response dynamics depend on performance expression and listener experience
                    1                  2             1                   3                         1
Heather L. Chapin , Kelly J Jantzen , JAS Kelso , Fred Steinberg , and Edward W. Large
                           1                               2                               3
Florida Atlantic University , Western Washington University , University MRI of Boca Raton

    Purpose
    The goal of this study was to link dynamic emotional and neural responses of listeners with time-varying music
    performance parameters, and to investigate the role of musical experience in modulating these responses.
    Methods
    Emotional responses and neural activity were observed as they evolved over several minutes in response to changing
    stimulus parameters. Our experimental stimulus was a skilled music performance that included natural fluctuations in
    timing and sound intensity that musicians use to evoke affective responses. A mechanical performance of the same
    piece served as a control. Participants reported emotional responses in real-time on a 2-dimensional rating scale
    (arousal and valence), before and after fMRI scanning. During fMRI scanning, participants listened without reporting
    emotional responses.
    Results
    Stimulus changes predicted real-time ratings of emotional arousal and real-time changes in neural activity. Limbic
    areas responded to the expressive dynamics of music performance and activity in areas related to emotion processing
    and reward were dependent upon the musical experience of listeners. Dynamic changes in activation levels of the
    mirror neuron system, insula, anterior cingulate and basal ganglia were found to correlate with expressive timing
    fluctuations in performed music.
    Conclusions
    These findings are consistent with the hypothesis that music influences emotional responding through an empathic
    motor resonance mediated by the mirror neuron system. We also showed that music’s affective impact on neural
    activity depends on the musical experience of listeners.
    Research Implications
    Apart from its natural relevance to cognition, music is shown to provide a window into the intimate relationships
    between production, perception, experience, and emotion.
    Acknowledgements
    This work was supported by NSF grant BCS-0094229 and a Fulbright Visiting Research Chair awarded to EWL, and
    NIMH grant MH80038 and the Pierre de Fermat Chair to JASK.


Session 2B                Memory and Cognition I                  (Chair: Judy Plantinga)

2B) 2:20 – 2:40

Associating sounds: Tone envelope, timbre, and associative memory
                1                          2                  2
Michael Schutz , Jeanine Stefanucci , and Sarah Baum
                      1                              2
University of Virginia , College of William and Mary

    Purpose Associating sounds and images is important in everyday tasks such as pairing faces with voices and
    identifying unseen events. Although tone envelope is known to play a key role in the perception of timbre, its effect on
    cognitive tasks such as associative memory is less thoroughly researched. As recent research shows that
    ethologically relevant sounds facilitate associative memory in young chicks (Field et al., 2007), here we ask whether
    sounds with ecologically familiar exponentially decaying envelopes (regularly produced by the collision of solid
    objects) are easier to associate with target objects than sounds with artificial flat envelopes. These results have
    relevance for psychological research on audition and music perception, as well as our understanding of timbre and
    associative memory.
    Methods
    In the first experiment, participants were asked to associate various household objects (e.g., cell phone, key, credit
    card) with 4-note melodies made of tones using one of two amplitude envelopes: ―flat‖ (off-on-off) or ―percussive‖
    (exponential decay). In the second experiment we crossed training and testing conditions such that some participants
    were trained on percussive sequences but tested on flat sequences, and vice versa. Both experiments were designed
    using a modified version of the old-new paradigm (Mandler, 1980; Tulving, 1985). We presented 10 sequences during
    training and 20 during testing, allow for separate evaluation of sequence recognition (old/new judgments) and recall of
    associated objects.



                                                            12
   Results
   Neither of the experiments showed any difference in recognition sensitivity to sequences between tone conditions.
   However, participants hearing percussive sequences in the first experiment correctly recalled 62% more of the
   sequence-object pairs. In the second experiment, this advantage was found only for those both training and testing on
   percussive sequences, who showed a 45% advantage in the number of items recalled over the other three conditions.
   Conclusions
   Although sequences with percussive envelopes were no more memorable (as assessed by the recognition task), they
   were significantly more easily associated with targets, as long as they were heard both at training and at test.
   Implications
   Although tone onsets are generally regarded as more important than offsets, these findings demonstrate offsets in fact
   play a crucial role in forming associations between sounds and objects. Among other implications, they suggest that
   human-computer-interfaces requiring users to associate sounds with messages may be improved with the use of
   percussive envelopes.



2B) 2:40 – 3:00

The effect of musicality and scale type on memory for tone sequences

Charles Barousse and Michael Kalish
University of Louisiana at Lafayette

   Purpose
   Music researchers have hypothesized that unequally spaced intervals provide more tonal orientation and
   psychologically distinct information for listeners than equally spaced intervals, making it easier to remember tone
   sequences based on unequal step scales. Experimental results have provided support for this hypothesis. There is
   also research that demonstrates better memory performance for tonal sequences. These tonal sequences, in addition
   to being based on unequal step diatonic scales, typically exhibit a high degree of musicality (the quality of sounding
   musical). However, there has been no research that has investigated the separate contributions of scale and
   musicality to memory for tone sequences. Three experiments examined these contributions.
   Methods
   In each trial, participants heard a tone sequence followed by two transpositions. Participants chose which
   transposition contained a mistuned note. In Experiment 1, scale step spacing and musicality were investigated using
   random tone sequences (low degree of musicality). The sequences for Experiment 2 were pre-selected randomized
   tone sequences rated as being ‗equally musical‘ in a ratings study. In Experiment 3, tone sequences were written to
   cross scale type and musicality.
   Results
   In Experiments 1 and 2, participants performed better with tone sequences that were based on the major scale when
   compared to a seven pitch equal step scale (Experiment 1) or a whole tone scale (Experiment 2). In results for
   Experiment 3, a 4x1 ANOVA showed that four different tone sequence categories were significant. Performance was
   best to worst in the following order: Major scale musical sequences, whole tone scale musical sequences, major scale
   unmusical sequences and whole tone scale unmusical sequences. A separate rating study of these sequences
   showed mean musicality ratings followed the same trend. A 2 way ANOVA showed significance for musicality only,
   with no significant interaction between musicality and scale. In linear regression analyses, in which three factors were
   tested as predictors of participants' scores, scale was insignificant while musicality and rating were significant.
   Conclusion
   There are many characteristics of tone sequences that may contribute to our remembrance of them. Specific scales
   have an effect on memory in both musical and unmusical tone sequences. Though scales still contribute to tone
   sequence memory when crossed with musicality, they are less of a contributor to tone sequence memory than
   musicality.
   Research Implications
   In future research involving tone sequence memory, researchers should consider musicality as either an experimental
   variable or a possible untested explanatory contribution to results for experiments that do not vary or control for
   musicality.




                                                           13
2B) 3:00 – 3:20

The relationship between music perception and self-reported memory in breast cancer survivors

Debra Burns, Tonya Bergeson, Bryan Schneider, Fred Unverzagt, and Victoria Champion
Indiana University–Purdue University Indianapolis

   Purpose
   A variety of chemotherapy agents have ototoxic effects that can decrease both high and low frequency hearing.
   Several studies have shown that chemotherapy may also induce cognitive impairments such as decreased information
   processing speed, motor function, verbal memory, visuospatial skill, and visual memory. It is unclear whether reports
   of qualitatively different and negative musical listening experiences in cancer patients who have received
   chemotherapy are due specifically to the ototoxic effects (i.e., hearing loss) or to more general neurotoxic effects (e.g.,
   decreased cognitive function). Finally, it is also unclear whether such changes in music perception are domain-specific
   (i.e., limited to music) or domain-general (i.e. cognition). The purpose of this study is to describe the relationships
   between the results of auditory-based perception tests and self-reported memory in breast cancer survivors who have
   received adjuvant cancer treatment and age-matched healthy controls.
   Methods
   Breast Cancer Survivors (BCS) and age-matched Healthy Controls (HC) completed audiometric testing and the
   Montreal Battery of Evaluation of Amusia to determine hearing thresholds, pitch perception, rhythm perception, and
   melodic memory. Participants also completed the Squire Memory Self-report Scale.
   Results
   The groups (BCS & HC) were similar in years of education, hearing PTA, race/ethnicity, and marital status. There was
   a moderate, negative correlation between hearing and the scores on the auditory perception tests. There were no
   significant differences in self-reported memory or perception tests between the two groups, although effect sizes were
   moderate. Correlations between scores on working memory and music perception (Overall score, Scale, Interval,
   Meter, Melodic Memory) were significant for HC, but not for BCS.
   Conclusions
   Although BCS and HC received very similar scores on tests of working memory and music perception, their
   processing of such information differs dramatically, as revealed by significant correlations between the two tasks for
   healthy controls but not for breast cancer survivors.




Session 3A                Music and Language I                    (Chair: Steven Livingstone)

3A) 3:40 – 4:00

Production of vocal prosody and song in children with cochlear implants

Tonya R. Bergeson, Matthew Kuhns, Steven B. Chin, and Annabelle Simpson
Indiana University School of Medicine

   Purpose
   The goal of this study was to explore spoken prosody and music production in children with cochlear implants (CIs).
   Methods
   Four 8- to 15-year-old children with 5-8 years of CI experience completed three tasks: 1) Production of sentences in
   happy, sad, questioning, and neutral manners of speaking; 2) Production of a variety of pitch contours; and 3)
   Production of the familiar song ―Happy Birthday.‖ Recordings from the first task were presented to a listener panel of
   three normal-hearing adults, who completed two additional tasks: identification, a closed-set task in which listeners
   listened to low-pass filtered (400 Hz) versions of the recordings and identified the emotion, and rating, a task in which
   listeners were given the ―correct‖ emotion and rated the quality of the child‘s representation of that emotion on a 7-
   point scale.
   Results
   Sentence Prosody: The listener panel correctly identified the children‘s sentence emotion 36.8% (s.d. = 17.1) of the
   time, which did not significantly exceed chance values. The children‘s prosody productions received lower quality
   ratings (question = 3.48, sad = 4.23, happy = 4.60, neutral = 4.65) than the model‘s productions (range = 6.00-7.00).
   Pitch Contour: In general, we found smooth pitch progressions in the examiner‘s model but not for the children with
   CIs. Across children‘s productions, pitch direction was relatively preserved, but pitch range was smaller than that of
   the model. The timing of the pitch contour glides was similar across the model and children‘s productions.
   Song: The model produced ―Happy Birthday‖ with stable pitch on each syllable, whereas the children with cochlear
   implants produced significantly more pitch fluctuations. As expected, the children‘s pitch production was generally

                                                             14
   worse than their rhythm production. In the best song production, the child included a good singing tone and pitch
   variation, but pitch changes were not in the right direction or the right interval size. Unexpectedly, the children‘s rhythm
   production was also inaccurate. Although global rhythmic patterns were largely maintained, the temporal patterns did
   not fall into a periodic hierarchical framework (i.e., a ―beat‖).
   Conclusion
   In summary, although cochlear implantation has significantly improved the communication abilities of many deaf
   children, these findings suggest that pediatric CI users still have difficulty producing prosody in a variety of contexts.
   Clinical Implications
   Because prosody also influences the intelligibility of spoken language, it will be important to develop therapeutic
   interventions that address the prosody production difficulties in deaf children who use cochlear implants.
   Acknowledgement of Research Funding
   NIH-NIDCD Training Grant T32DC00012


3A) 4:00 – 4:20

Vocal imitation of speech and song: Effects of phonetic information and temporal regularity

James Mantell and Peter Q. Pfordresher
University at Buffalo, SUNY

   Purpose
   Four experiments addressed the accuracy with which people imitate pitch-time trajectories from spoken sentences
   and sung melodies. Experiments 1 and 2 were designed to address the role of phonetic information in imitation while
   experiments 3 and 4 were designed to address temporal regularity of syllables in the target sequence.
   Methods
   Participants imitated sentences and melodies that were presented in their original form and as phonetically neutral
   sequences (each syllable synthesized to a ‗hum‘). Melodies were created based on the pitch-time contour of
   sentences but were diatonic and isochronous.
   The original stimulus set was imitated as heard in Experiment 1. In Experiment 2, participants ignored phonetic
   information and imitated using the syllable ―ah.‖ Experiments 3 and 4 manipulated temporal structure so that sequence
   timing always matched speech (Experiment 3) or was isochronous like melodies (Experiment 4).
   We analyzed accuracy of pitch imitation by comparing the pitch trace during imitations with the pitch trace of the
   original stimulus, after adjusting for differences in production rate. These adjustments were used to measured
   accuracy of timing.
   Results
   Three primary results emerged. First, imitation of speech may rely on phonetic information more so than song. Though
   both domains benefit from phonetic information, imitation of speech was disrupted when phonetic information was
   irrelevant to imitation (Experiment 2). Second, a general singing advantage emerged with respect to pitch matching
   (imitation of absolute pitch information). This advantage may be due in part to timing characteristics of music
   (Experiments 3 and 4). Third, overall ability of individuals to imitate song correlated with imitation of speech.
   Conclusion
   Results are consistent with the idea that music and language use common resources but different representations
   (Patel, 2008). Correlated imitation abilities across individuals may reflect a domain-general capacity for linking
   perception with action. At the same time, domain-specific sensitivity to the integration of articulation (phonetics) and
   phonation (pitch) suggests the presence of distinct representations. Finally, contrary to common belief, we find no
   evidence of an advantage for speaking over singing. In fact we find the opposite, though we interpret our musical
   advantage in light of stimulus structure, rather than domain specificity.
   Research Implications
   Recent work on speech imitation suggests that certain speech sounds are imitated automatically. Further work utilizing
   the current imitation paradigm, with an emphasis on the pitch-time contour, could provide insight into the dynamics of
   speech perception and production while providing clues about the role of indexical information in the signal.
   Acknowledgement of Research Funding
   NSF Grant 06042592




                                                             15
3A) 4:20 – 4:40

Does pitch processing in English have a musical basis?
                1               1                  1                       2
Laura C. Dilley , Louis Vinke , Elina Banzina , and Aniruddh Patel
                                1                             2
Bowling Green State University , The Neurosciences Institute

    Purpose
    A variety of meaningful distinctions are conveyed by pitch variations in speech, which linguists have characterized in
    terms of abstract phonological categories based on primitives like H (high) and L (low) and combinations of these.
    What is the relationship between pitch processing in music and in language, and do fundamentally ―musical‖
    processes or abstractions underlie linguistic distinctions based on pitch?
    Methods
    We tested the hypothesis that differences of contour (e.g., up-down pitch pattern) underlie contrasts in linguistic
    phonological categories (e.g., H* vs. H+L*) pegged to widely-described distinctive fundamental frequency (F0) timing
    properties across languages. In particular, the timing of F0 maxima and minima relative to speech phonemes, which is
    widely claimed to distinguish meanings and phonological categories across languages, was manipulated using speech
    resynthesis techniques for short English phrases in several experiments. These experiments assessed: (1) how
    participants perceptually classified F0 patterns in speech on the basis of the timing differences, (2) whether
    participants‘ classifications were based on differences in the relative pitch levels of syllables, and (3) the role of these
    timing differences in linguistic communication. A variety of tasks were used, including AX discrimination (Experiment
    1), AXB categorization (Experiment 2), speech imitation (Experiment 3), judgment of the relative prominence of
    syllables (Experiment 4), and judgment of the relative pitch of syllables using participants who were musicians
    (Experiment 5a) or nonmusicians (Experiment 5b). Moreover, pitch perception for the speech stimuli on the basis of
    F0 was modeled using the Prosogram algorithm (Mertens, 2004).
    Results
    Results show converging evidence that listeners form distinct perceptual categories on the basis of timing differences
    in F0 maxima and minima in speech. Moreover, crossover points from one category to another in speech-based
    categorization and discrimination tasks in Experiments 1-4 were in good agreement with those derived from judgments
    of relative pitch in Experiments 5a and 5b. Large individual differences in ability to process pitch differences in speech
    were also observed.
    Conclusions & Research Implications
    These results support the hypothesis that differences of up-down contour underlie contrasts in at least some linguistic
    phonological categories, suggesting that similar or overlapping cognitive mechanisms are engaged in processing
    music and language.


Session 3B                 Memory and Cognition II                 (Chair: Stacey Davis)

3B) 3:40 – 4:00

Modeling a melody recognition task for musicians, nonmusicians, and amusics using a cohort network

Naresh N. Vempala and Anthony S. Maida
University of Louisiana at Lafayette

    Purpose
    Dalla Bella, et al. (2003) studied effects of musical familiarity on melody recognition by comparing performance
    between musicians and nonmusicians in a melody gated-presentation (MGP) task. They identified three events in this
    task: (1) familiarity emergence point (FEP), (2) isolation point (IP), and (3) recognition point (RP). The FEP was the
    point at which the listener correctly judged the presented melody as `familiar.‘ The IP was the point where the listener
    identified the melody. The RP was the point where the listener identified the melody with maximum confidence. The
    qualitative results of the MGP task are the following. The FEP occurred earlier in musicians than nonmusicians, but
    the IP occurred earlier in nonmusicians. Finally, the RP occurred slightly earlier in musicians. Our aim was to simulate
    the MGP results using a connectionist simulation of the cognitive processes underlying the emergence of the FEP, IP,
    and RP in musicians versus nonmusicians. We call this a melody cohort network (MCN) which is based on analogy to
    cohort models of word recognition. We also used the simulation to predict the effect of acquired amusia on the MGP
    task.
    Methods
    We built an MCN for this task. Its core consisted of a sequence recognition neural network. This network consisted of
    sequence recognition (SR) neurons interconnected in a winner-take-all competitive network. SR neurons were
    activated by notes in a melodic sequence connected to temporal delay filters. SR neurons, with associated input

                                                              16
   weights, represented stored melodies. Separate neural networks modeled musicians and nonmusicians. The musician
   network represented a larger corpus of stored melodies than the nonmusician network. The core network modeled the
   IP. Meta-level networks modeled the FEP and RP. These networks used the core network as input. By lowering pitch
   perception thresholds, two states of acquired amusia were simulated in the musician network. The states were: (1) a
   minimum of two semitones for detecting pitch changes, and (2) a minimum of three semitones for detecting pitch
   changes Memories, as represented by the melody-specific SR neurons, remained unaffected.
   Results
   Our MCN captured the qualitative results of the MGP task in musicians versus nonmusicians. It also makes
   predictions about performance of acquired amusics for the MGP task.
   Conclusions and Research Implications
   Our model shows how stored memory size may affect the recognition process as indicated by the differential IP
   phenomena, and how meta-level processes monitoring the status of the recognition process may explain contrasting
   FEP and RP phenomena for musicians and nonmusicians.


3B) 4:00 – 4:20

The influence of time and memory constraints on the cognition of hierarchical tonal structures

Morwaread Farbood
New York University

   Purpose
   The presence of tonal hierarchical structures in music has long been observed by theorists and experimental
   psychologists. While there is general agreement as well as supporting empirical data indicating that these structures
   exist, the extent to which listeners perceive them is still under investigation. The goal of this research is to examine the
   issue in more detail and offer a perspective that incorporates models of short and long-term memory. Within this
   context, it proposes some modifications to Lerdahl‘s tonal tension model (2001) in order to better explain certain
   experimental data.
   Methods
   Data was gathered in an experiment measuring continuous listener responses to tension in musical excerpts from the
   classical repertoire. Thirty-five subjects from the MIT community took part in the study and were asked to move a
   slider on a computer interface in response to how they felt tension was changing in a given musical excerpt.
   The primary musical excerpt chosen for the purposes of this investigation was analyzed and quantified according to its
   salient musical features (harmonic tension, pitch height of the bass and soprano lines, and onset frequency). The
   harmonic tension values were obtained using Lerdahl‘s tonal tension model excluding the melodic attraction
   component. In addition, mathematical derivatives were calculated for each musical feature—that is, a description of
   how each feature was changing in time, where the difference in time ranged from 0.25 to 20 seconds. All of the feature
   descriptions and their derivatives were used as input to regression analysis in order to identify the best predictors of
   the mean tension curve.
   Results
   The results showed that for every feature except harmony, the derivatives that were most effective were those that
   described changes taking place under 3 seconds. Harmony, on the other hand, best fit the tension data when the time
   differential was 10-12 seconds in length.
   Conclusion
   This 10-12s window suggests that a decay factor imposed on inherited hierarchical value in Lerdahl‘s prolongational
   reduction should be applied after a minimum of 10 seconds of elapsed time. Furthermore, the decay factor should
   taper the value to zero at around 17 seconds in time, where the mean square error flattens out at a maximum.
   Research and/or Educational/Clinical Implications
   Given these preliminary results, it appears that the cognition of tonal hierarchies goes beyond the regular limits of
   short-term memory and is processed in a different manner than other structures such as pitch contour and rhythm.
   More data will be gathered to verify these findings.




                                                             17
3B) 4:20 – 4:40

A procedural take on the Deutsch/Feroe formalism: Cognitive motivation and computational realization

Craig Graci
State University of New York at Oswego


    Purpose
     In previous talks (Graci, 2008a, 2008b) I have discussed the computational modeling of grouping structure within a
    symbolic framework that features tools of analysis based on grouping preference rules (Lerdahl & Jackendoff, 1983).
    In this talk I will present a computational realization of a procedural variant of the Deutsch/Feroe formalism for
    representing reductional structure in tonal music (Deutsch & Feroe, 1981). A simple symbolic language called Clay
    which possesses a large measure of structural generality (Wiggins & Smaill, 2000) serves as the medium in which
    computational constructs for both grouping structure and reductional structure have been shaped. Elements of Clay
    which are particularly relevant to the implementation of alphabets, sequences, and reduction operators will be
    discussed.
    Method
    The work is grounded in simple but powerful concepts found within the field of computer science. For example, stacks
    are used to conveniently scope Deutsch/Feroe alphabets, and Post productions (Post, 1943) serve as the conceptual
    basis for my interpretation of the Deutsch/Feroe ―prime‖ operator. In short, a spectrum of ideas associated with the
    theory and practice of symbolic computation constitutes the core methodology employed in this particular investigation
    of the internal representation of tonal melody.
    Results
    Generally speaking, the results of this effort take the form of a computational framework called MxM (Music
    Exploration Machine) and an executable music knowledge representation language (Clay) within which reductional
    structure, grouping structure, and the interaction between these two forms of structure may be studied. Investigation of
    melodic structure is facilitated by a collection of graphical tools and a collection of analytical tools. More specifically, an
    alternative inscription language (Pea, 1996) to that proposed by Deutsch and Feroe for representing the internal
    structure of tonal melody is defined. One notable aspect of this inscription language is that, unlike the Deutsch/Feroe
    formalism, it accommodates durational variablility. Another characteristic is that it fairly naturally supports the
    incremental modeling of reductional structure from both the top-down and the bottom-up perspectives.
    Conclusions
    Sloboda (2005) aptly observes that making sense of music has often been equated with the process of discovering
    and representing its structure. Writing expressions for melodic fragments in the elegant notation defined by Deutsch
    and Feroe is a worthwhile, albeit challenging, exercise in making sense of music. The computational system described
    in this talk changes the nature of the reductional inscription process, and can be viewed as a form of cognitive
    scaffolding (Wood, Bruner, & Ross, 1976) for tasks which require insight into melodic structure.
    Research and/or Educational/Clinical Implications
    MxM/Clay has, in fact, been deployed both as an educational microworld (Papert, 1980) and as a cognitive artifact
    (Norman, 1991) in classroom situations (Graci, 2000, 2008b). Based on these experiences the system appears to hold
    considerable potential for a range of educational applications involving the distributed cognition of melodic knowledge.


3B) 4:40 – 5:00

Serial position effects in a singer‟s long term recall identify landmarks and lacunae in memory
               1                   2                       1
Roger Chaffin , Jane Ginsborg , and James Dixon
                         1                                  2
University of Connecticut , Royal Northern College of Music

    Purpose
    When a piece of music is first learned, memory for what comes next is activated by serial cuing as the current
    passage cues motor and auditory memory for what comes next. During memorization, serial memory is overlaid with
    more explicit performance cues that provide content addressable access, where the musician can recall a passage by
    simply thinking of it, e.g., ―G section‖. We expected content addressable access and serial cuing to produce negative
    serial position effects in free recall: Recall would be best at cues and decline progressively in succeeding bars.
    Method
    An experienced singer memorized Stravinsky‘s Ricercar I. She reported features of the music that she attended to in
    practice and performance cues where she thought about during performance, e.g., where she needed to attend to an
    upcoming entry or to the other musicians (Prepare-PC’s). Later, she wrote out the melody line and words from
    memory six times, after 0, 18, 32, 42, 47, and 59 months. We used mixed hierarchical regression analysis to identify
    relationships between the singer‘s reports and her practice and recall.

                                                               18
Results
The singer used beginnings of sections and phrases as starting places during practice. There were corresponding
effects on recall, which declined from 100% accuracy at 0 months to just over 50% after 4 years. Recall was better at
beginnings of sections and phrases and declined linearly in successive bars after the cue (negative serial position
effects). In contrast, recall was worse at Prepare-PC‘s, and improved as distance from the cue increased in bars both
before and after the cue (a negative curvilinear effect). Other performance cues produced positive curvilinear effects.
Conclusions
Starts of sections and phrases became landmarks, providing the singer with content addressable access to her
memory for the piece. Prepare-PC‘s became lacunae where the music was forgotten, probably because the singer
paid less attention to it during practice.
Research Implications
Content addressable access to serially cued memory was reflected in negative serial position effects, i.e., linear
declines in recall in bars after retrieval cues. Serial position effects that extended symmetrically both before and after
retrieval cues are more plausibly attributed to effects of attention.




                                                          19
                                             Tuesday, August 4th



Keynote I                  Infancy: A Musical History Tour
9:00 – 10:00               Dr. Sandra Trehub, University of Toronto


Session 4A                 Music and Language II                   (Chair: Joy Ollen)

4A) 10:20 – 10:40

Resolving conflicting linguistic and musical cues in the perception of metric accentuation in song

Jieun Oh
Stanford University

    Purpose
    This study explores the interplay of prosodic and musical accentuation by examining situations in which implicit
    accents in a song's melody conflict with stress patterns in the lyrics. While previous studies offer examples in which
    the musical accentuation dominates over text, the opposite phenomenon has only been artificially generated. This
    study examines an instance in which the Korean translation of the song "Happy Birthday" renders its implicit musical
    accentuation subordinate to the conflicting linguistic features, resulting in a perceptual alteration of the intended metric
    structure of the song.
    Methods
    Twelve native English speakers with no knowledge of Korean (Group I) and eighteen Korean-English bilinguals
    (Group II) participated in the study. Subjects were asked to sing "Happy Birthday" in English (Group I and II) and in
    Korean (Group II only) while finger tapping along to the song's rhythm. An acoustic drum trigger, affixed to the bottom
    surface of a table, was used to record the finger taps, and the amplitude data were extracted for analysis. A final
    survey collected information about the subjects' musical and language background.
    Results
    Our analysis focused on the change in the tapping intensity over the musical anacrusis. Across-group comparison of
    the tapping patterns over the syllables "happy" and "birth", sung in English, showed a tendency for speakers less
    fluent in English to de-emphasize "birth", which corresponds to the music's downbeat. Moreover, a within-subject
    comparison over the syllable "to" and "you" showed that relative beat-strength perception reversed depending on the
    language used to sing the song: "you" was emphasized over "to" when sung in English, but de-emphasized when
    sung in Korean.
    Conclusion
    The across-group data suggest that offline effects of one‘s native language can be manifested when singing in a
    second language, and that language fluency may affect the extent to which linguistic stress patterns play a role in the
    overall beat-strength perception of songs. Furthermore, the within-subject data imply strong online effects of language,
    demonstrating that beat-strength perception of songs can be altered based on the lyrics' stress patterns.
    Research Implications
    Our findings imply that linguistic cues can play a significant role in the inference of beat accentuation (and by
    extension, the metric perception) of songs.
    Acknowledgment of Research Funding
    Major Grant, awarded by the Stanford University Undergraduate Research Program




                                                              20
4A) 10:40 – 11:00

The costs and benefits of background music for processing written and spoken verbal materials
                       1                           2                    1
William F. Thompson , E. Glenn Schellenberg , and Jana Letnic
                    1                        2
Macquarie University , University of Toronto

    Purpose
    The success of MP3 players illustrates the widespread use of music as accompaniment for other activities. Although
    background music is often irrelevant to task performance, it demands cognitive resources. Kahneman‘s (1973)
    capacity model maintains that a limited pool of resources is distributed over cognitive processes. The arousal-mood
    hypothesis posits that background music affects task performance through its mediating influence on arousal and
    mood (Thompson, Schellenberg & Husain, 2001). Two experiments addressed the hypothesis that the use of
    background music involves a trade-off between cognitive capacity limitations and arousal-mood benefits. We
    examined the effects of tempo and intensity of background music on comprehension of written (Experiment 1) and
    spoken (Experiment 2) verbal materials.
    Methods
    Mozart‘s Sonata for Two Pianos in D major was presented through headphones to 25 listeners following manipulations
    of tempo (150 bpm / 110 bmp) and intensity (60 dB / 72 dB). Tempo and intensity changes were matched for
    psychological magnitude. Reading passages (GMAT papers, Martinson & Ellis, 1996) were presented as written
    (Experiment 1) or spoken (Experiment 2) stimuli. Comprehension was assessed by multiple-choice questions.
    Results
    Baseline comprehension (no music) was 46%. Manipulations of intensity and tempo influenced comprehension of both
    written and spoken materials. For written materials there was an interaction between intensity and tempo. For slow
    tempo music, comprehension scores did not differ for loud and soft conditions (~40% accuracy). For fast tempo music,
    comprehension ranged from 49% (soft) to 32% (loud).
    Conclusion
    Music is a source of distraction for comprehension of verbal materials but the degree of distraction is dependent on
    attributes of the music. Loud music is generally more distracting, especially if the musical tempo is relatively fast.
    Research and/or Educational/Clinical Implications
    Background music while reading or listening to verbal material is a fact of life and widespread in educational contexts
    (studying, libraries). Understanding how to minimize musical distraction effects may have educational benefits.
    Acknowledgement of Research Funding
    Supported by the Australian Research Council (WFT), the MQRES Fund (WFT), and NSERC (EGS).


4A) 11:00 – 11:20

Affective and cognitive changes following prolonged exposure to Music and Speech
             1                              2
Gabriela Ilie and William F. Thompson
                     1                       2
University of Toronto , Macquarie University

    Purpose
     We examined the affective and cognitive consequences of listening to music (Experiment 1) or speech (Experiment 2)
    for prolonged periods of time. We also examined the effects of manipulating pitch height, rate and intensity in the two
    domains.
    Methods
    In Experiment 1 a classical piece of music was manipulated in pitch (high/low), rate (fast/slow) and intensity
    (soft/loud). Manipulations yielded eight conditions (2 pitches x 2 tempi x 2 intensities). For each, participants listened
    to music for seven minutes and reported their affective experiences based on ratings of valence (pleasant-unpleasant)
    and two types of arousal: energy (awake-tired) and tension (tense-relaxed). Creativity and routine task performance
    was also examined after each condition.
    Experiment 2 reproduced the condition Experiment 1 using a male and a female speech about sea turtles (samples
    were longer versions of stimuli used by Ilie & Thompson, 2006).
    Results
    Manipulations induced changes in experiences of valence, energy arousal, and tension arousal in music (Experiment
    1) and speech (Experiment 2). Manipulations of intensity and rate had identical effects in music and speech. Intensity
    manipulations affected ratings of tension arousal but not valence or energy arousal. Rate manipulations affected
    judgements of energy and tension arousal but not valence. Manipulations of pitch height had similar effects on valence
    ratings for music and speech, but different effects for ratings of experienced energy arousal. Manipulations of pitch
    height influenced energy arousal for speech, but not music.

                                                             21
   Conclusion
   Affective consequences of manipulating pitch height, rate and intensity were observed for both music and speech
   stimuli. Music and speech were not associated with identical effects, however, indicating that their connections with
   affective experience are partially domain specific. We discuss the relationship between perceived and felt affect by
   comparing the results obtained with Ilie & Thompson (2006). We also discuss a psychological framework for
   conceptualizing the affective and cognitive implications of the human auditory affective code.
   Acknowledgement of Research Funding
   This research was supported by the NSERC of Canada through a Canada Graduate Scholarship awarded to the first
   author and a discovery grant awarded to the second author.


4A) 11:20 – 11:40

Parallel acoustic cues in sad music and sad speech

David Huron, Olaf Post, Gary Yim, and Kelly Jakubowski
The Ohio State University

   Purpose
   Linguistic research has established that "sad speech" is characterized by six prosodic cues: (1) quiet voice, (2) slow
   speaking rate, (3) low pitch, (4) small pitch movement, (5) slurred articulation, and (6) dark timbre (Fairbanks &
   Pronovost, 1939; Eldred & Price, 1958; Davitz, 1964; Siegman & Boyle, 1993; Banse & Scherer, 1996; Sobin & Alpert,
   1999; Breitenstein, van Lancker & Daum, 2001). Listeners make use of all six cues in assessing the sad mood state of
   speakers. In this presentation, we review a series of published and unpublished analytic studies showing that Western
   music in the minor mode commonly exhibits these same six features.
   Methods
   The studies described in this presentation focus on aspects of musical organization and test hypotheses using
   correlational rather than experimental approaches. The studies involve large-scale analyses of several musical
   corpora, including Beethoven piano sonatas, Germanic folksongs, twentieth-century percussion music, the Barlow &
   Morgenstern dictionary of musical themes, and random samples of music from the Baroque, Classical, and Romantic
   periods. Both score-based and recording-based analyses are described.
   Results
   Music in the minor mode tends to exhibit lower dynamic markings, has slower notated and performed tempos, is lower
   in overall pitch, involves smaller pitch intervals, and shows greater use of slurring and pedaling. In addition, when
   musical repertoires are compared for similar instruments with darker (marimba) and brighter (xylophone) timbres,
   there is a marked tendency for the brighter instrument repertoire to favor major-key works.
   Conclusion
   In general, these corpora studies show a broad agreement between sadness cues in speech prosody and features in
   Western music (Juslin & Laukka, 2003).
   Research Implications
   The biological role of sadness is discussed and a theory proposed as to why "sad" sounds exhibit the six
   aforementioned cues. Grief (high arousal) is distinguished from sadness (low arousal). Low epinephrine levels are
   linked to weak muscle tone and slow muscle movement which produces low subglottal air pressure and slow
   movement of lips and tongue. It is proposed that, rather than being innate, the acoustic cues for sadness are learned
   associations or mirrored affects that are artifacts of low arousal. Evidence consistent with this interpretation is found in
   the difficulty listeners have in distinguishing "sad" voice from "sleepy" voice.


Session 4B                Rhythm and Meter I                      (Chair: Petr Janata)

4B) 10:20 – 10:40

Sustained sound in a rhythmic context does not cause a filled duration illusion

Bruno H. Repp and Rachel Marcus
Haskins Laboratories

   Purpose
   Filling intervals between short ―beat‖ tones with subdivision tones makes the beat interonset intervals (IOIs) seem
   longer and the sequence tempo slower—a kind of ―filled duration illusion‖ (FDI) in music (Repp, 2008; Repp &
   Bruttomesso, submitted). Another kind of FDI has been obtained in psychophysical studies: An interval marked by
   onset and offset of continuous sound seems longer than a silent interval of the same duration. Two explanations for

                                                             22
   this type of FDI have been proposed: acceleration of an internal pacemaker by continuous sound (Wearden et al.,
   2007), and slower perception of sound offsets than of sound onsets (Burghardt, 1972; Fastl & Zwicker, 2007). Two
   experiments tested whether this latter type of FDI might play a role in music.
   Methods
   Experiment 1 tested the pacemaker hypothesis in a rhythmic context by asking musicians to compare or reproduce
   short isochronous sequences of low-pitched piano tones played legato or staccato at different tempi. If there is a FDI,
   legato sequences should be perceived as slower than staccato sequences. Experiment 2 tested the offset perception
   hypothesis by asking musicians to judge the timing of tone offsets relative to the IOI midpoints of short isochronous
   sequences of low-pitched piano tones (gradual offsets) or artificial tones (abrupt offsets), played at three different
   tempi. If there is a FDI, tone offsets should be judged as late when they occur at the IOI midpoint, and would have to
   occur early to be judged as occurring at the midpoint.
   Results
   Results of Experiment 1 gave no indication that legato sequences were perceived as slower than staccato sequences.
   Results of Experiment 2 indicated that tone offset timing was judged quite accurately with both types of tone. There
   was no evidence of delayed perception of offsets, although piano tone offsets had to occur about 10 ms earlier for
   their timing to be judged like that of abrupt offsets.
   Conclusions
   These results show that a FDI with sustained sound is not obtained in a quasi-musical rhythmic context, in contrast to
   the reliable FDI obtained when additional sound events are inserted into an interval. The results raise questions about
   the explanations proposed for the first type of FDI in psychophysical experiments.


4B) 10:40 – 11:00

Differences in metrical structure confound tempo judgments

Justin London
Carleton College

   Purpose
   Musical tempo is usually regarded as simply the rate of the tactus or beat, yet most rhythms involve multiple,
   concurrent periodicities. Two experiments were conducted to investigate relations between the absolute rate of the
   tactus versus a more global sense of speed via a tempo discrimination task involving typical rhythmic patterns.
   Method
   Stimulus rhythms in 4/4 meter were presented at two tempos (component periodicities of 2400, 1200, 600, and 300ms
   vs. 2000, 1000, 500, and 250ms). Seven patterns using all combinations of the longest periodicity with one, two, or all
   three other components were employed. Trials consisted of a standard followed by a comparison, a 3AFC design
   (comparison = slower, same, or faster). In Exp1 participants simply judged relative speed. In Exp2 participants
   focused on the tactus rate. In both participants were told to refrain from tapping or making other synchronization
   movements.
   Results
   In both experiments when standard and comparison used the same pattern responses were accurate (97% correct for
   same absolute tempo; 85% when the comparison was slower or faster). This did not hold when standard and
   comparison involved different patterns: The presence of beat subdivisions (SDs) seems key. In both experiments,
   when Std and Comp both had SDs accuracy remained high. If SDs are lacking in either the standard or the
   comparison, performance was poor, as the presence/absence of SDs largely determined the response. In Exp. 2,
   focusing on the beat level improved performance where it was poor in Exp. 1, but degraded performance where it was
   good.
   Conclusion
   Periodicities in the 200-400ms range are highly salient in creating a sense of speed, even though these periodicities
   lie well outside the region of maximal pulse salience (Parncutt 1994). Beat subdivisions, while beneficial in rhythmic
   production, may not always enhance tempo perception (Repp 2003).
   Research and/or Educational/Clinical Implications
   This work provides a baseline for future studies of tempo perception involving more ecologically valid stimuli and/or a
   motor behavior component.




                                                           23
4B) 11:00 – 11:20

Temporal stability in rhythmic continuations by drummers and dancers

Christine Beckett
Concordia University

   Purpose
   To explore rhythmic capability across two time-based arts, music and dance; to explore whether an implicative
   paradigm could be applied to rhythm and dance.
   Method
   Participants—drummers (13, 2F) and dancers (11, 8F)—improvised continuations of 2 Slow, 2 Medium, and 2 Fast
   drum opening gestures. (Opening dance stimuli of similar nature were also improvised upon, but are not the main
   focus here.) Opening drum stimuli, each containing two time intervals delineated by 3 strikes of the right hand on a
   conga drumhead, were taped for uniformity. Each stimulus started with a one bar count-in to establish the beat and
   tempo. Participants saw/heard each opener twice (drummers) or 3 times (dancers). They counted in at the given
   tempo for one full bar and performed seated at the same drum. They were free to use both hands, and were
   videotaped. Two null hypotheses were adopted: that there would be no difference of temporal stability between
   dancers and drummers; and that there would be no difference in temporal stability between slow, medium and fast
   stimuli. The initial 20s of each improvisation was evaluated by 3 independent judges for temporal stability, as defined
   by the regularity of the beat and conformity to original tempo.
   Results
   Overall, drummers were significantly more temporally stable than dancers. Fast openers elicited improvisations of
   significantly more stable temporality than medium and slow openers. There was no significant difference of stability
   comparing improvisations elicited by medium compared to slow openers.
   Conclusion
   That fast openers led to more stable continuations may be a result of beat proximity (more beats in a given time span)
   and/or related to preferred tapping/tempo rates. Musicians‘ clear superiority at maintaining temporal stability was
   somewhat surprising given that dance is rhythmic, dancers often work to a drum, and the conga used was not
   technically challenging.
   Research/Educational Implications
   Whether this was a musical training effect, or whether these musicians were more accustomed to improvising than
   were the dancers, could be tested through use of highly over-learned materials in drumming and dance. Such
   materials could be used, further, to explore MMN and CPS, etc., in a standard/deviant ERP paradigm. Given that
   musicians and dancers frequently coordinate their arts, specific musico-rhythmic training for dancers might be
   warranted.


4C) 11:20 – 11:40

Fractal structure of tempo fluctuations in skilled piano performance
                    1                    1                   2
Summer K. Rankin , Edward W. Large , and Craig Sapp
                           1                     2
Florida Atlantic University ,Stanford University

   Purpose
   In previous work the performances of one skilled pianist revealed 1/f type serial correlations and fractal scaling
   (Rankin, et al., 2009). In order to investigate whether and how this finding generalizes we conducted fractal analyses
   on 2 databases of piano performances.
   Methods
   Database 1 (http://mazurka.org.uk) included commercial audio recordings of 5 pieces of music (Chopin Mazurkas)
   performed by professional pianists. The beat times were manually coded and inter-beat intervals were extracted for
   the analyses. Database 2 (http://www.piano-e-competition.com) consisted of performances from the top 5 pianists at
   the Minnesota International Piano-e-competition 2008 & 2004. Performances were recorded on a Yamaha CFIIIS
   Disklavier Pro concert grand piano. Each performance was matched to its score using a custom dynamic
   programming algorithm (Large & Rankin, 2007), to extract inter-beat intervals (IBIs) for analysis.
   Results
   Power Spectral Density (b) and Rescaled Range (H) analyses of the IBI time-series at multiple metrical levels, showed
   that the majority of performances in both databases revealed fractal scaling and long-range (1/f type) serial
   correlations.
   Conclusion
   Our results show that piece and style of music were better predictors of H and b values than pianist. Fractal scaling

                                                           24
                                                                         1
    implies that fluctuations at lower levels of metrical structure (e.g., ⁄16-note) provide information about fluctuations at
    higher levels of metrical structure (e.g., ¼-note). Long-range correlation implies that fluctuations early in the time
    series provide information about fluctuations later in the time-series.
    Research Implications
    Researchers have suggested a deep relationship between musical and other biological rhythms. Fractal stimulus
    fluctuations may facilitate perception-action coordination, such that endogenous processes are better able to perceive
    structure, dynamically engage, and adapt to changes. Such processes may enable more successful interaction with
    the environment and between individuals.
    Acknowledgement
    This research was supported by AFOSA grant FA9550-07-C0095.




Session 5A                  Music and Language III                  (Chair: Tonya Bergeson)

5A) 1:30 – 1:50

Congenital amusia is not a music-specific disorder: Evidence from speech perception
         1                       2                        1
Fang Liu , Aniruddh D. Patel , and Lauren Stewart
                         1                              2
University College London , The Neurosciences Institute

    Purpose
    The domain-specificity of congenital amusia is a topic of active research. This paper investigates whether individuals
    with amusia have intonation perception deficits in speech when the pitch contrasts are subtle, and considers the
    relationship between performance on intonation tasks and sensitivity to changes in pitch.
    Methods
    Ten British amusics participated in a set of intonation tasks and pitch threshold tasks. The intonation tasks involved
    three same-different discrimination tasks: statements versus questions in 1) natural speech, 2) gliding-tone analogs,
    and 3) nonsense-speech analogs. A statement/question identification task was also included. The pitch threshold
    tasks involved the use of adaptive-tracking, forced choice procedures to determine the threshold for a) detection of a
    pitch change, and b) discrimination of pitch direction.
    Results
    In the intonation tasks, discrimination accuracy was not equivalent across the three conditions, even though the pitch
    patterns were identical. 7/10 of amusics performed worse on discrimination of speech stimuli (mean percentage
    correct: 75%) than tone analogs (91%). Furthermore, 7/10 of amusics performed better on discrimination of speech
    stimuli (88%) than nonsense-speech analogs (79%). Accuracy rates for identification of statements and questions
    were relatively low (71%) across amusics. Regression analyses revealed a significant negative association between
    both pitch threshold tasks and performance on the gliding-tone analog condition (F(1,7) = 16.7, p = 0.0047; F(1,7) =
    38.8, p = 0.0004). A similar relationship was also found between pitch direction threshold and performance on the
    nonsense-speech analog condition (F(1,7) = 11.3, p = 0.0121). In contrast, performance on the speech
    discrimination/identification tasks was not predicted by either of the pitch threshold tasks.
    Conclusions
    The above results extend those of Patel et al., 2008 (Music Perception, 25: 357-368), where 30% of amusics had
    difficulties in distinguishing between statements and questions that differ only in the direction of their final pitch glides.
    The present paper, which used more subtle pitch contrasts and gliding-tone analogs, reveals that the majority (7/10) of
    individuals with amusia show worse performance for discrimination of speech-intonation versus tonal-analogs.
    Research implications
    The findings point to the non domain-specificity of amusia, and constrain theories of the underlying neural
    mechanisms associated with this deficit.
    Acknowledgement of Research Funding
    Supported by Neurosciences Research Foundation and the Economic and Social Research Council




                                                               25
5A) 1:50 – 2:10

Illusory conjunctions in memory for phonemes and melodic intervals: Vowels sing but consonants
swing
             1                    3                   2                      3                    3
Pascale Lidji , Régine Kolinsky , Isabelle Peretz , Hélène Lafontaine and José Morais
                 1                        2                                3
McGill University , University of Montreal , Université Libre de Bruxelles

    Purpose
    The lyrics and the tune of songs are known to leave associated memory traces (e.g., Peretz, Radeau & Arguin, 2004;
    Crowder, Serafine, & Repp, 1990). The nature of these connections between text and melody are, however, still
    unknown. The present study investigates whether the phonetic properties of the lyrics influence the strength of the
    tune-lyrics connections. Indeed, a recent study in song perception (Kolinsky et al., in press) revealed that consonants
    are less integrated to melodic information than are vowels. Based on these results, we expect to find similar
    differences between consonants and vowels in song memory. The occurrence of illusory conjunctions of lyrics and
    tune was taken as an index of the strength of association between these song components (Thompson, Hall, &
    Pressing, 2001).
    Methods
    After the learning of bisyllabic nonwords sung on two-tone intervals, the participants had to recognize these ―old
    stimuli‖ among four types of foils. These foils were (1) completely new stimuli (new interval and new nonword), (2) a
    new interval combined with an old nonword, (3) a new nonword combined with an old interval, or (4) mismatch stimuli.
    The mismatch stimuli were new combinations of familiar components, namely, the interval and the nonword of two
    stimuli presented separately in the learning phase. Twelve participants were presented with stimuli in which the
    nonwords differed on one consonant, and twelve other participants were presented with stimuli in which the nonwords
    differed on one vowel. False alarms to mismatch stimuli, i.e. erroneous recognitions of these mismatch stimuli as old
    stimuli, reflected to occurrence of illusory conjunctions.
    Results
    The participants produced more false alarms for mismatch stimuli than for any other kind of foils, suggesting that they
    made illusory conjunctions of nonwords and intervals. The illusory conjunction rate was significantly higher for stimuli
    in which nonwords varied on consonants than for stimuli in which nonwords varied on vowels.
    Conclusion
    The results confirm that the phonetic properties of the lyrics modulate the lyrics-tunes interactions: consonants are
    less strongly associated with the melody than are vowels. In other words, vowels sing but consonants swing, hence
    leading to more numerous illusory conjunctions. These results will be discussed in the light of the different evolutionary
    origins and linguistic functions of consonants and vowels.


5A) 2:10 – 2:30

Using a rhythm-based pedagogical technique to improve reading fluency
                    1                 2                   3
Scott D. Lipscomb , Dee Lundell , and Larry Scripp
                       1                            2                           3
University of Minnesota , Minneapolis Public Schools , New England Conservatory

    Purpose
    Past research has revealed that music can serve as a useful means of facilitating learning in other academic areas
    (Burnaford, 2007; Catterall, 2005; Deasy, 2002 & 2003; Peterson, 2005), including benefit specifically to reading ability
    (Andrew & Sink, 2002; Butzlaff, 2002). For the past five years, the present authors have been collaborating with
    classroom teachers and music specialists to integrate music across K-12 curriculum. In the Fall of 2007, a reading
    program was established in a highly diverse northside Minneapolis school. The Purpose of this investigation is to
    determine to what extent the use of musical rhythm can facilitate the acquisition of reading fluency with high frequency
    sight words by third graders.
    Methods
    During the 2007-08 academic year, a Rhythm & Reading Group (RRG) was established in each of three third grade
    classrooms. The pedagogical method involves establishment of a steady rhythm to which students read lists of 25
    words. Four lists of words were used during the testing period, with two- to three- week practice periods per list,
    resulting in a total duration of 12 weeks for empirical investigation. The RRG sessions were led by a teaching artist
    twice a week for 20 minutes in each classroom. A variety of tempos and word orders were used to continuously
    engage the students. A carefully designed assessment schedule was established to measure reading ability at two- to
    three-week intervals throughout the testing period.



                                                              26
   Results
   Results revealed a significant level of improvement for both student below grade-level reading ability and for those at
   or above grade-level reading, as determined at the beginning of the academic year. Students improved dramatically
   in their reading fluency on the list of 25 words they had practiced during the weeks preceding each test, as measured
   by the number of words read correctly (increased significantly) and the time required to read the list of words
   (decreased significantly). Results revealed significant transfer of reading fluency, as both the accuracy and reading
   time improved significantly for the lists of words not seen since the initial pre-test as well.
   Conclusions & Educational Implication
   Integration of the Rhythm & Reading Group appears to significantly improve student reading fluency within a very
   short period of time. This example of music integration may benefit students and teachers across the glove. Future
   research needs will be explicated.
   Acknowledgement of Research Funding
   The authors express their appreciation to the University of Minnesota Public Engagement Grant, Music-in-Education
   National Consortium, Learning Through Music Consulting Group, and the U.S. Department of Education for their
   generous funding of this project and other related projects.


Session 5B                Rhythm and Meter II                   (Chair: Jon Prince)

5B) 1:30 – 1:50

Turn that noise up: How Rock Band© helps youth develop rhythmic intuitions

Michael P. Downton, Kylie A. Peppler, and Ken Hay
Indiana University

   Purpose
   With the growing popularity of rhythmic videogames (e.g., Rock Band), youth are spending large amounts of time
   playing music in the context of games although we know little about the efficacy of such environments for musical
   learning. As such, videogames present a novel opportunity to examine rhythmic games as an alternative pathway into
   music education. In this study, we investigate the following research questions: 1) Through game play, do youth
   improve their abilities to perform rhythms accurately and in tempo? 2) In what ways does Rock Band teach youth to
   understand how rhythms and patterns are a part of music?
   Methods
   The study took place in a local after-school program. A total of 30 participants took part in the study over a three-
   month period. At the start of the study, 44% of participants were unfamiliar with the game. A quantitative performance
   analysis that logged the number of hits, misses, near hits, extra hits, and non-synched hits during game play was used
   to measure Rhythmic Perception (i.e., rhythm that can be attended to immediately), Rhythmic Accuracy (i.e., the
   number of correct hits made during game play), and Rhythmic Estimation (i.e., the reconstruction of rhythmic
   information from stored memory).
   Results
   Researchers chose six case studies that represented the larger trends amongst the participants. The results show
   that youth do improve their abilities to perform rhythms accurately and in tempo. One case, Karen, is shown for the
   purposes of this abstract. In Figure 1, observations 1 and 5 are compared and demonstrate that Karen‘s rhythmic
   intuition was becoming more developed (i.e., her perception, estimation, and accuracy all improved). In Figure 2, the
   graph shows that the margin for error decreased for Karen and accounted for more accurate hits overall,
   demonstrating some understanding of the rhythm in the song.
   Conclusion
   The nature of the Rock Band experience provides the benefits of simultaneous music-making as commonly realized in
   the classroom setting, while also framing the learning within an activity that leads youth to make connections at their
   own speed. Findings demonstrate that rhythmic intuitions develop during game play as evidence by the improvement
   of rhythmic perception, accuracy, and estimation.
   Educational Implication
   This research builds a foundation for the use of new technologies, and particularly videogames, in the classroom. In
   addition, this study holds implications for capitalizing on informal, out-of-school music experiences for musical
   learning.




                                                           27
5B) 1:50 – 2:10

A perception-action model for similarities in perceived musical tempo and the kinematics of physical
action

Aysu Erdemir, Erdem Erdemir, and John J. Rieser
Vanderbilt University

    Purpose
    Many pieces of music slow down at the end, and the ―final ritardandi‖, signals that the end of the piece is near.
    Controlled locomotion ends with a ―ritardandi‖ as well, as runners slow down to stop. The main thesis of this paper is
    that the ritardandi that listeners prefer in music have commonalities with the ritardandi in physical actions. We propose
    a 2nd order homogenous differential equation modeling approach, which is used to model a variety of real world
    systems such as diffusion processes, control of electrical/mechanical devices and human arm/hand/leg movements, to
    capture the similarities of musical ritardandi and stopping from running. Our model consists of (1) a goal/attraction
    point, xref (2) an internal driving factor, _ coefficient, which moves the system towards the final goal (3) a viscous
    damping factor, _ coefficient, which provides dissipation of energy and a controlled trajectory and (4) an energy-
    storage element, _coefficient. The equation of motion for such system is __+ __+ _x=0, where x is displacement as a
    function of time.
    Methods
    The model parameters were changed iteratively to simulate several 2nd order systems until we find the system output
    that was closest to the observed data profile. These data include 16 individual velocity patterns of runners‘
    deceleration as well as 20 individual tempo patterns of ritardandi, generously shared by Friberg & Sundberg (1999)
    and Sundberg & Verillo (1980).
    Results
    Our model not only matched the velocity and tempo patterns of 12 stopping and 12 ritardandi samples as well as the
    kinematic model proposed by Friberg & Sundberg (1999), but also produced a higher mean correlation (p<0.02) for
    the additional 4 velocity and 8 tempo profiles which have been discarded from the calculations due to worries about
    low aesthetic rating and tempo fluctuations.
    Conclusion
    Based on the idea of a unifying mechanical formula, our findings suggest that expressive timing in music can be
    explained by parameters of physical/mechanical motion that deal with force, mass and velocity; and provide further
    support with regard to the common analogy between ―music‖ and ―movement‖.
    Research and/or Clinical or Educational Implication
    We aim to assess the generalizability of the model as well as its potential to account for individual differences by
    analyzing the kinematics of various actions such as playing ritardandi, stopping from running, grasping and hand
    shaking by both within and between subjects designs; and accordingly we plan to develop a personal visuomotor
    training method primarily for use in sports and musical education.


5B) 2:10 – 2:30

Temporal context and choice reaction time

Robert J. Ellis and Mari Riess Jones
The Ohio State University

    Purpose
    The majority of influential models of choice reaction time (RT) do not consider the temporal context in which a to-be-
    decided event appears. In this study, we manipulated the temporal context (―Rhythmic‖ vs. ―Scrambled‖) of a
    sequence of clicks that preceded a final target tone to determine how temporal context contributes to choice RT.
    Methods
    Listeners heard a sequence of 6 or 7 clicks (10 ms tones, F#5) that preceded a final tone (25 ms, C5 or C6), and
    made a speeded judgment whether that tone was ―low‖ or ―high.‖ The timing of the click sequence was either
    ―Rhythmic‖ (in which inter-onset intervals [IOIs] between tones corresponded to a strong 4/4 metrical grid) or
    ―Scrambled‖ (permutations of the rhythmic sequences that were designed to hinder the percept of meter). All
    sequences were exactly 4 s from the onset of the first tone to the onset of the target tone. The final IOI (the
    ―foreperiod‖) before the target was, equally often, 250, 500, or 1000 ms; we included this variable to uncover whether
    the classic foreperiod effect (i.e., RT decreases as foreperiod increases when multiple foreperiods vary randomly
    within a session) holds when a foreperiod is part of a larger sequence. Listeners heard either a block of Rhythmic trials
    or a block of Scrambled trials first (72 each).


                                                             28
   Results
   Four effects were noteworthy. (1) Listeners were faster overall during the rhythmic block. (2) Those listeners with a
   faster overall RT showed the first effect more strongly than those listeners with a slower mean RT. (3) Those listeners
   who heard the Scrambled block first showed the first effect more strongly than those listeners who heard the Rhythmic
   block first. (4) RTs in the Rhythmic block were faster when the foreperiod was 500 ms (vs. 250 or 1000 ms),
   suggesting that a subdivision of the 1000-ms ―beat‖ prior to the target tone facilitated performance; no significant
   foreperiod effects were present in the Scrambled condition.
   Conclusion
   The rhythmic structure of the temporal context preceding a target tone judgment significantly affects RTs, despite the
   fact that the temporal context contains no information about the what of the decision, only the when.
   Implications
   These findings are consistent with entrainment approaches to temporal sequencing (e.g., Large & Jones, 1999;
   McAuley & Jones, 2003). They also have implications for models of choice RT (e.g., Ratcliff, 1978; Luce, 1986) which
   are currently tacit with regards to how temporal structure influences decision making.
   Acknowledgement
   This research was supported by The Caroline B. Monahan Fund for Experimental Research Support within the
   Department of Psychology at Ohio State University.


5B) 2:30 – 2:50

Comparing synchronization to auditory and visual rhythms in hearing and deaf individuals
            1                  1                       2                         2
John Iversen , Aniruddh Patel , Brenda Nicodemus , and Karen Emmorey
                           1                             2
The Neurosciences Institute , San Diego State University

   Purpose
   The ease with which humans synchronize their movements with sound suggests a tight coupling between auditory and
   motor systems. Is this a special relationship, or can other sensory modalities drive motor synchronization? In previous
   studies, synchronization with flashing lights was far worse than to equivalent auditory rhythms. Flashing lights may
   however not be optimal for a visual system often concerned with detecting motion. We ask if synchronization accuracy
   to moving visual stimuli approaches that for auditory stimuli. The role of experience in shaping sensory-motor
   synchronization is also examined by comparing the performance of hearing participants and deaf signers with
   extensive visuo-motor experience through sign language.
   Methods
   Deaf and hearing participants (n=22 each) tapped to isochronous (600 ms period) visual and auditory (hearing
   participants only) stimuli. Visual stimuli included a flashing light, and an animated bouncing ball. Participants were
   instructed to synchronize with the moment of contact of the ball with a surface. The auditory stimulus was a
   metronomic series of beeps. Tap times were collected using a drum pad, and inter-tap intervals and asynchronies
   were computed.
   Results
   The substantial difficulty of synchronizing to a flashing light was confirmed in both groups. In contrast, for hearing
   participants synchronization with the bouncing ball was just as good as with the auditory metronome (mean standard
   deviation of asynchrony (sd. async) 35 ms vs. 36 ms; p = 0.59). Deaf participants synchronized with the bouncing ball
   as well as did hearing participants (sd. async 38 ms; p = 0.98).
   Conclusion
   The results suggest that synchronization with a moving visual stimulus can be as accurate as synchronization with
   auditory stimuli. Deaf signers performed the same with visual synchrony as hearing non-signers, suggesting visuo-
   motor synchronization is not modified by experience, at least for these simple stimuli. Future experiments will examine
   more ecologically relevant stimuli.
   Research Implications
   Both auditory and visual systems appear capable of driving accurate synchronization. A question for future cross-
   modal comparisons is whether synchronization to the beat of metrical patterns (multiple time-scales of temporal
   structure) is as good for visual as for auditory stimuli.
   Acknowledgement of Funding
   This work was funded by Neurosciences Research Foundation as part of its program on music and the brain at The
   Neurosciences Institute, where AP is the Esther J. Burnham Fellow, and by National Institute for Child Health and
   Human Development, R01 HD13249, awarded to KE and San Diego State University.




                                                           29
Poster Session I
3:00 – 5:00

1. Implicit and explicit memory for melodies in aging and cognitive impairment
   Ashley D.Vanstone, Lola L.Cuddy, Angeles Garcia, Rosalind G.Sham, and Leila Tangness
   Queen’s University

   Purpose
   We sought to clarify the effects of aging and cognitive impairment on explicit and implicit memory for melodies.
   Previous results are conflicting. In studies of aging and explicit memory, older participants performed as well as
   (Blanchet, Belleville, & Peretz, 2006), or, alternatively, more poorly than (Bartlett, Halpern, & Dowling, 1995) younger
   controls. Further, Alzheimer participants performed similarly to older controls on recognition (explicit) tests and were
   impaired on preference (implicit) tests (Halpern & O‘Connor, 2000), but the reverse pattern has also been reported
   (Quoniam et al. 2003).
    Methods
   Both young (N = 30, ages 19-25) and elderly (N= 40, ages 68-88) adults were recruited. Older adults included both
   cognitively healthy individuals and those who had been diagnosed as cognitively impaired (with or without dementia).
   Eight unfamiliar melodies were presented, in random order, on each of three study trials and participants were given
   intentional learning instructions. Study trials were followed by a recognition trial containing 16 melodies. Eight of the
   16 were the study melodies; eight were novel. Study melodies and novel melodies were counterbalanced across
   participants. Participants were asked both to indicate the pleasantness of each melody on the test trial—a test of
   implicit memory, or mere exposure effect—and also to respond yes/no to each melody whether it had appeared in the
   study trials—a test of explicit memory.
   Results
   On both explicit and implicit tasks, melody retention scores were, in order, young adults highest, followed by healthy
   older adults, followed by impaired adults. Young and older adults scored well above chance, but impaired older adults
   were at chance.
   Conclusions
   Our experimental method was sensitive to both the effects of aging and the effects of impairment. Retention of
   unfamiliar melodies is subject to aging, but the effects of healthy aging are not necessarily severe. The effects of
   cognitive impairment are additive to the effects of aging and, in contrast to healthy aging, may be severe.
   Research/clinical implications
   A major challenge for future theory and research, with implications for music therapy, is to reconcile the evident loss of
   retention of unfamiliar tunes in aging and impairment with the well documented sparing of long-term musical
   memories.
   Acknowledgments
   Research supported by the Ontario Mental Health Foundation (ADV), the Natural Sciences and Engineering Council of
                                      ®
   Canada (LLC) and the GRAMMY Foundation (LLC). We thank Andrea Halpern for kindly supplying the test stimuli.


2. The obsessive song phenomenon: Induction, memory and emotions
   Andréane McNally-Gagnon, Sylvie Hébert, and Isabelle Peretz
   University of Montreal

   Purpose
   Earworms, or songs that get ―stuck" in one‘s head, are a common experience and yet, we know very little about this
   strange mental phenomenon. The goal of this study was to 1) obtain descriptive as well as production data of
   obsessive songs, and 2) examine whether they could be induced experimentally.
   Methods
   One group of musicians and one group of non-musicians who self-reported having obsessive songs frequently were
   recruited and given a personal recording device. They had to reproduce their obsessive songs when they appeared,
   by singing them as accurately as possible, over two non-consecutive 3-day periods. They also had to fill out
   questionnaires describing the songs and associated emotions. Before each 3-day period, half of each group was
   exposed to an ―induction‖ condition where they were presented with five catchy songs and had to reproduce them.
   The other halves were exposed to a control condition.
   Results
   Induction was deemed successful when one or more of the ―induced‖ songs were recorded by the participant during
   the subsequent 3-day period. It was successful in 47.22% of the participants in the induction group. Songs that were
   induced were very or moderately familiar in 84.6% of the cases, which makes familiarity a necessary but not sufficient
   factor for induction to be successful. Also, musicians had longer episodes and their reproductions were more accurate
   than the ones of non-musicians, that is, they were closer in pitch and time to the original versions. Musicians also
   recorded more classical and invented songs, which indicates that the phenomenon is closely related to exposure and
   musical habits. Lastly, our data show that emotions preceding the earworm episodes were mostly positive, and only
                                                            30
    rarely neutral (3,16%). In addition, the emotional states described after (or during) the earworms were more often
    neutral (32,27%) and less strongly positive or negative.
    Conclusion
    Our findings suggest that like voluntary musical imagery, earworms are closely related to the absolute memory
    representations of music in the mind. Moreover, they suggest that one of the earworm's function could be related to
    emotional regulation. Finally, although our induction condition was not optimal, the fact that it was partly successful is
    encouraging and warrants further investigation.
    Research and/or Educational/Clinical Implications
    This study enables a view of the phenomenology and physical attributes of involuntary mental imagery and furthers
    our knowledge about musical memory and the role of music in emotional regulation.
    Acknowledgement of Research Funding
    This research was funded by the Natural Sciences and Engineering Research Council of Canada.


3. Speaking, singing and observing: A TMS Study
                                      1            2                     2
   B. Stahl, F. Lessard, Pascale Lidji , H. Theoret , and Isabelle Peretz
                    1                         2
   McGill University , University of Montreal

    Purpose
    During the past decade, neurophysiological studies in primates have identified a population of neurons that respond
    not only to executed actions but also to the observation of these actions. This neurophysiological phenomenon has
    become well-known under the label of ―mirror neurons‖ (Fadiga et al., 1995). In humans, a similar mirror neuron
    system seems to be involved in speech perception (e.g. Watkins et al., 2003). However, it remains unclear whether
    such a system also exists for singing. The aim of this study was to assess the contribution of mirror neurons in speech
    and music.
    Methods
    Professional singers performed speech and singing tasks while single pulse transcranial magnetic stimulation was
    applied over the right and left motor cortices. Cortical excitability changes were measured by motor-evoked potentials
    in the contralateral hand. We compared the motor-cortical excitability during production tasks (i.e., speaking and
    singing) and perceptual tasks (i.e. judging accuracy of verbal and sung productions). The stimuli in the perceptual
    tasks were auditory, visual (lip-reading for speech, facial expression for singing; Thompson & Russo, 2007) or audio-
    visual.
    In the speech tasks, subjects were instructed to explain French proverbs (production) or were observing someone else
    explaining French proverbs (perception). Subjects had to rate whether the explanation of proverbs was correct in the
    auditory and audio-visual condition, or whether distinct words could be read on the lips in the visual condition. In the
    singing tasks, subjects were instructed to sing intervals (production) or to observe someone else singing intervals
    (perception). Subjects had to rate whether the sung intervals were correct based on auditory information or on facial
    expression. Control condition included meaningless figures which had to be classified as closed or open. Speech and
    singing conditions were tested in two different sessions involving production and perception trials. Meaningless figures
    were presented separately before or after each session.
    Results
    Speech production and perception equally increase the excitability of the left motor cortex whereas song production
    and perception increase the excitability of the right motor cortex.
    Conclusion
    The results are in line with prior studies in showing increased motor-cortical excitability during speech perception
    (Watkins et al., 2003) as well as during speech and music production (Sparing et al., 2007). Our data further suggest
    the involvement of mirror neurons that are differently lateralized in speech and music.
    Acknowledgement of Research Funding
    CFI and NSERC to IP, FNRS and WBI-World to PL.


4. Relative influence of musical and linguistic experience on the subcortical encoding of pitch
   Gavin M. Bidelman, Jackson T. Gandour, and Ananthanarayan Krishnan
   Purdue University

    Purpose:
    Neural encoding of pitch in the auditory brainstem is known to be shaped by long-term experience with language or
    music, implying that early sensory processing is subject to experience-dependent neural plasticity. In language, pitch
    patterns consist of sequences of continuous, curvilinear contours whereas in music, pitch patterns consist of discrete,
    stair-stepped sequences of notes. The aim of this study was to examine how domain specific experience (language
    vs. music) influences the pre-attentive encoding of pitch within the human brainstem.



                                                             31
   Methods:
   Brainstem frequency-following responses (FFRs) were recorded from native Chinese, English amateur musicians, and
   English non-musicians in response to iterated rippled noise (IRN) homologues of a musical interval (major third; M3)
   and a lexical tone (Mandarin tone 2; T2). Pitch strength (50 ms sections) and pitch-tracking accuracy (whole contour)
   were computed from the FFRs using autocorrelation algorithms. In addition, narrow-band spectrograms were used to
   evaluate their spectral composition.
   Results:
   Chinese and musicians showed higher pitch-tracking accuracy than the non-musicians regardless of the domain.
   Relative to non-musicians, musicians showed more robust pitch strength across all sections whereas Chinese did so
   only in those sections containing the most rapid changes in pitch. Interestingly, musicians exhibited greater pitch
   strength than Chinese in one section of M3, corresponding to the onset of the 2nd musical note, and two sections
   within T2, corresponding to a note along the diatonic musical scale.
   Conclusion:
   Despite the striking differences in the nature of their pitch experience, both Chinese and musicians, relative to non-
   musicians, show positive transfer across domains in terms of pitch encoding. Musicians have a more robust
   representation of pitch than Chinese, but only in subparts of the stimulus which can be related to perceptually salient
   features found in music. As such, we infer that brainstem neurons are differentially tuned to extract specific acoustic
   features relevant to a listener‘s domain of expertise. Cross-domain enhancement of pitch representation appears to be
   greater from music to language than the reverse at the level of the brainstem.
   Research Implications:
   Our results imply that the auditory brainstem is not simply a passive way station, but rather, is shaped by long-term
   experience. As in the cortex, this early, pre-attentive sensory processing is subject to experience-dependent neural
   plasticity. More importantly, processing relevant to music and speech perception may begin as early as the level of the
   brainstem.
   Acknowledgement of Research Funding:
   Research supported by NIH R01 DC008549 (A.K.) and NIDCD predoctoral traineeship (G.B.)


5. Musicians display enhanced auditory event-related potentials to both music and voice
   Natalya Kaganovich and Christine Weber-Fox
   Purdue University

   Purpose
   Musical training has a pervasive influence on auditory processing and appears to enhance early sensory encoding of
   not only music but also pure tones and speech. Because speech is necessarily carried by voice, reports of greater
   phonological abilities in musically trained individuals raise the question of whether musicians have an enhanced
   processing of the human voice. Chartrand and Belin (2006) showed that musicians were more accurate in
   discriminating both musical and vocal timbres; however, the cognitive mechanisms underlying such enhancement
   remained uninvestigated. Therefore, the first goal of our project was to compare auditory encoding of musical and
   vocal sounds in musicians and non-musicians. Voices are often described as ―auditory faces.‖ In the visual domain,
   faces have a powerful ability to capture attention. We asked if voices may have a similar power in the auditory domain
   and investigated whether rare changes in musical and vocal sounds were equally distracting for musicians and non-
   musicians.
   Methods
   Ten musicians and 10 non-musicians participated in the study. We combined a version of an auditory distraction
   paradigm (Schröger and Wolff, 1998) with event-related potential (ERP) recordings. During each block, participants
   identified sounds as either short (300 ms) or long (500 ms) by pressing response buttons. In half of the blocks, musical
   sounds (cello or French horn playing a G4 note) were present on 80% of trials while voices (male and female saying
   neutral ―a‖) were present on 20% of trials (deviants). In the other half, the reverse was true. A change in the type of
   sound (from music to voice or the reverse) was irrelevant for the duration judgment task. We collected RT, accuracy,
   and ERPs in response to all sounds.
   Results
   Preliminary analysis of ERP data shows that both music and voice deviants elicited a larger fronto-central negativity
   between 100 and 400 ms post-stimulus onset in musicians compared to non-musicians, with a greater group
   difference for music deviants. Inspection of group averages suggested a greater re-orienting negativity component in
   musicians to both types of deviants.
   Conclusion
   Early exposure to musical timbres may enhance perception of other spectrally complex sounds, such as the human
   voice.
   Research Implications
   This study extends research into the relationship between music and speech perception by suggesting that one
   reason for the beneficial effect of musical training on linguistic skills may be the enhanced sensory encoding of
   complex spectral information present in the voice.

                                                           32
   Acknowledgement of Research Funding
   This project was supported in part by NIH-NIDCD R01 DC00559-18 awarded to CWF.


6. Inferring rules from sound: The role of domain-specific knowledge in speech and music perception
   Aaronell Matta and Erin E. Hannon
   University of Nevada, Las Vegas

   Purpose
   Speech and music are two forms of complex auditory structure that both play fundamental roles in everyday human
   experience and require certain basic perceptual and cognitive abilities. Nevertheless, human listeners may process
   the same information differently depending on whether a sound is heard in a linguistic vs. musical context. The goal of
   these studies is to examine the role of domain-specific knowledge in auditory pattern perception. Specifically, the
   study examines the inference of ―rules‖ in novel sequences containing patterns of both spectral structure (i.e., changes
   in speech or instrument timbre) and fundamental frequency (i.e., changes in pitch).
   Methods
   In Experiment 1, participants were familiarized with sequences containing contrasting rules, where speech syllables
   followed one sequential rule (e.g., ABA) and pitch (of the voice) followed another (e.g., ABB). In a test phase,
   participants rated the similarity of novel stimuli that were consistent or inconsistent with one or all of the rules
   established during familiarization. In Experiment 2, participants will be again be familiarized with sequences containing
   contrasting rules (ABA vs. ABB), however instrument timbre will replace speech syllables. Pitch stimuli will follow one
   consistent instrument timbre. The test phase will also follow the same procedure as Experiment 1.
   Results
   Results of Experiment 1 indicate that listeners respond primarily to violations of syllable structure and largely ignore
   changes in pitch, presumably because of domain-specific knowledge within a speech context. In a control condition
   where the same stimuli are presented but syllables are held constant, listeners show no difficulty picking up on pitch
   patterns, suggesting that they are capable of detecting pitch patterns in music but ignore them when contrasting
   speech information is available. Conversely, in Experiment 2 we expect domain-specific musical knowledge to lead
   listeners to attend to patterns of pitch change and ignore changes in instrumental timbre.
   Conclusions
   These results support the notion that adults are capable of learning patterns in a number of domains, but that they
   attend to different cues depending on the context, essentially ―filtering‖ information according to domain-specific
   knowledge.
   Research Implications
   These findings will contribute to research and knowledge in the area of auditory cognition, particularly music and
   speech perception, offering insight into populations including infants, children, and adults.


7. Song prosody: Electrophysiological correlates of temporal alignment and metrical regularity in
   textsetting
                     1                  2                     1
   Reyna L. Gordon , Cyrille L. Magne , and Edward W. Large
   Florida Atlantic University1, Middle Tennessee State University2

   Purpose
   The aim of this study is to test the hypothesis that good textsetting helps listeners better understand sung language by
   focusing attention on stressed syllables. Using behavioral and electrophysiological measures, we investigate how the
   proportion of well-aligned syllables (stressed syllables on strong beats) in sung sentences affects intelligibility. We also
   compare the effects of metrical regularity (regular vs. irregular patterns of stressed and unstressed syllables) in song.
   Methods
   We created a corpus of sentences, comprised of regular (iambic and trochaic) and irregular linguistic stress patterns.
   Each sentence was sung on an isochronous melody. For regular sentences, good and bad textsettings were
   generated by pairing the sentences with a metronome beat that was either well-aligned (in-phase) or misaligned (anti-
   phase) with the linguistic stress pattern. Irregular textsettings were created by pairing irregular sentences with a
   metronome beat on alternating syllables, such that some of the syllables were well-aligned and some were
   misaligned. We used a cross-modal priming protocol to assess how sung prime sentences facilitate or hinder semantic
   processing of visual target words. Participants performed a lexical decision task in which they listened to the sung
   prime sentence and then decided whether a target word was a real word or pseudoword.
   Results
   Our preliminary findings indicate that Event-Related brain Potentials related to semantic integration are adversely
   affected by sentences with irregular and misaligned textsettings. Furthermore, induced (non-timelocked) gamma-band
   activity was enhanced for well-aligned versus misaligned syllables.



                                                             33
   Conclusion
   These findings fit with previous studies showing that attention is preferentially allocated to stressed syllables in speech
   and strong beats in music. The present observations add to our understanding of the neurophysiological basis of
   metrical expectancy, providing a viable explanation for the human tendency to unify linguistic and musical stress
   patterns in song.
   Research and/or Educational/Clinical Implications
   This research addresses the ongoing debate regarding the extent to which language and music share cognitive and
   neural resources. Our results may improve therapeutic interventions that incorporate singing in rehabilitation
   programs.
   Acknowledgement of Research Funding
   The first author was supported by the American Association of University Women Doctoral Dissertation Fellowship
   and the Newell Doctoral Fellowship.


8. Musical experience results in better speech-in-noise perception: behavioral and
   neurophysiological evidence
   Alexandra Parbery-Clark, Erika Skoe, and Nina Kraus
   Northwestern University

   Purpose:
   Musicians have life long experience with attentively listening to and manipulating musical sounds. Not surprisingly,
   musical experts demonstrate enhanced basic auditory perceptual skills as well as functional adaptations for the
   processing of acoustic features such as pitch, timbre and timing at both cortical and subcortical levels. These acoustic
   cues contribute to the parsing of multi-source sounds and it would appear that musicians use their heightened
   perceptual skills to better segregate concurrently presented sounds. Here we investigate whether musical expertise
   results in enhanced speech-in-noise perception. To this aim, we examined the effects of musical experience on both
   behavioural speech-in-noise tests and the brainstem representation of speech sounds with and without competing
   noise.
   Methods:
   To investigate the effect of musical experience on speech-in-noise perception we compared behavioural and
   neurophysiological responses in adult musicians and nonmusicians. We administered clinically relevant speech-in-
   noise tests (QuickSIN and HINT) and recorded auditory evoked brainstem responses to speech sounds in both a quiet
   and competing background noise condition.
   Results:
   Musicians outperformed the control group on the measures of speech-in-noise perception with years of musical
   practice positively correlating to speech-in-noise performance. Musicians were also found to have more robust
   subcortical representations of the acoustic stimulus in the presence of background noise suggesting that musical
   experience limits the degradative effects of background noise on subcortical representation of speech sounds.
   Conclusion:
   Musical experience appears to enhance the ability to hear speech in challenging listening environments and the
   underlying subcortical representation. Clinically, these findings highlight the importance of taking musical training into
   consideration when evaluating a person‘s speech-in-noise ability and suggest that musical training may be a useful
   rehabilative tool for populations with known speech-in-noise deficits such as children with learning disorders and
   people with hearing impairments.
   Acknowledgement of Research Funding: NSF SGER 0842376


9. The intentional nature of perception-action coupling as a basis for musical interaction
   J. Scott Jordan
   Illinois State University

   Purpose
   Recent research indicates (1) actions are planned in terms of the distal effects they are to produce, and (2) planning
   and perception share common neural resources. As a result, perceiving the effects of an action (e.g., hearing the
   tones produced by striking a piano key) activates the pre-motor cortical centers one would use while planning to
   produce such effects oneself. In short, perception takes place within an intentional, planning context that emerges out
   of the action-effect contingencies one learns over the life course. Given this intentional, effect-relative context for
   perception, the present talk will present research that (1) supports the assertion that perception is altered in a forward-
   looking manner as one learns the action-effect contingencies that underlie intentional contexts, (2) examines the
   conditions that allow one to develop intentional contexts, (3) examines how one‘s own perceptions are altered as one
   learns to cooperatively generate a distal effect with another (e.g., play a duet).



                                                             34
   Methods and Results
   The experiments to be presented examine intentional contexts in spatial perception. Specifically, research indicates
   participants perceive the vanishing point of a moving stimulus beyond the actual vanishing point. Jordan and
   Hunsinger (2008) found that those who have experience controlling the stimulus‘ movements, perceive the stimulus to
   vanish further ahead than those who do not. Also, one can learn the action-effect contingencies that give rise to such
   forward displacements (FD) by simply observing another control the stimulus. However, larger FD only occurs if one
   perceives the effects of the model‘s actions, as well as the actions themselves. Finally, Jordan and Knoblich (2004)
   found that FD increases as one attempts to control the movements of the stimulus co-operatively with another
   participant.
   Conclusions and Implications
   These data support the assertion that perception entails forward-looking, intentional content derived from planning.
   They further imply that as agents continuously generate distal effects together (e.g., a choir sings together), the
   resultant group effect (i.e., the sound of the choir) become part of one‘s own action-effect contingencies (i.e., one‘s
   own intentional context). One‘s planning therefore, transforms from being about oneself, to being about one‘s own
   effects in relation to the overall group-effect. Perceptions, likewise, become more group-relative.
   Research Acknowledgments
   Various portions of this research were funded by grants from the Alexander von Humboldt Foundation, the Max Planck
   Society, the Center for Interdisciplinary Research at the University of Bielefeld, Germany, and sabbatical grants from
   Saint Xavier University and Illinois State University.


10. When amusics sing in unison: another perspective of poor-pitch singing
                             1          1              2                1
    A. Tremblay-Champoux , I. Peretz , S. Dalla Bella , and M-A. Lebrun
                          1                                               2
    University of Montreal , University of Finance and Management, Warsaw

   Purpose:
   Poor-pitch singing is often seen as a consequence of a poor perception. However, new research has shown that a
   poor production may result from factors other than the perception. Indeed, Pfordresher and Brown found poor-pitch
   singers in a sample of individuals who do not exhibit any difficulties in pitch discrimination (1). Moreover, people who
   cannot perceive music, namely amusics, are able to produce two notes in the good direction even though they cannot
   tell if the second note was higher or lower than the first (2). Similarly, some of them are able to produce a well-known
   song with the right contour when they sing with lyrics, while they are unable to do so when singing on the syllable /la/
   (3). The authors suggest that this dissociation might be due to memory problems for the melody when it is not
   supported by lyrics. The purpose of this study was to examine if singing along with a model could help amusics to
   improve their performance, by reducing memory load. More specifically, the objective was to evaluate if singing with a
   model could help amusics to increase pitch accuracy without the help of the song lyrics.
   Method:
   One group of amusics (n=10) and their controls (n=10) sang a well-known song with lyrics and on the neutral syllable
   /la/. They first sang alone and then with a pre-recorded model in which a student (one male and one female) sang the
   song. Each production was recorded using Adobe Audition and then analyzed acoustically with Pratt and Matlab
   (using the method of Dalla Bella, Giguère & Peretz (3,4)).
   Results:
   The analysis showed that, for melodies sung on lyrics, unison singing did not improve performance of amusics as
   compared to when they sang alone. In contrast, amusics‘ performance on the syllable /la/ was improved by singing
   with the model as compared to singing alone.
   Conclusions:
   The results of this study suggest that choir singing can be useful to recover the melody of a well-known song. Unison
   singing can be seen as a potential solution for the production problems of amusics, mitigating their deficit in musical
   memory. However, the positive effect of the choir singing could also be due to motor entrainment. More research will
   be done to evaluate this possibility.
   References:
   (1) Pfordresher, P.Q. & Brown, S. (2007) Poor-pitch singing in the absence of tone-deafness, Music Perception, 25,
      95-115.
   (2)Loui P, Gunther F, Mathys C, Schlaug G. (2008) Action-perception mismatch in Tone-Deafness. Current Biology,
      18, 331-332.
   (3) Dalla Bella, S., Giguère, J-F., & Peretz, I. (in press) Singing in congenital amusia. Journal of The Acoustical
      Society of America.
   (4) Dalla Bella, S., Giguère, J-F., & Peretz, I. (2007) Singing proficiency in the general population, Journal of The
      Acoustical Society of America, 121, 1192-1189.




                                                            35
11. Neural stratification of sequencing and timing in auditory feedback? An fMRI study
    Peter Q. Pfordresher, Jennifer L. Cox, Michelle Andrews, James Mantell, and Robert Zivadinov
    University at Buffalo, SUNY

   Purpose
   Behavioral research has demonstrated different effects on music performance of altered feedback that results in
   asynchronies between perception and actions (e.g. delayed auditory feedback) and altered feedback that influenced
   pitch contents (e.g., serial shifts). Based on these results, Pfordresher (2006) speculated that links between perception
   and action are stratified according to levels of timing (synchrony) and sequencing (serial order). We tested whether
   these behavioral dissociations are also found in brain activity.
   Methods
   Eleven pianists performed simple melodies while being scanned in a 3-Tesla MRI. In different conditions they
   experienced normal auditory feedback, altered auditory feedback (asynchronous delays or altered pitches), or control
   conditions that excluded movement or sound. Performances were recorded via MIDI in order to compare behavioral
   with neuroimaging data.
   Results
   Behavioral results replicated past findings: Asynchronous delays slowed produced timing but did not increase errors
   whereas altered pitch increased errors but did not increase timing.
   With respect to neuroimaging data, reliable effects emerged when contrasting conditions with auditory feedback
   (normal or altered) against control conditions that involved listening (which reveals motor activity) or that involves
   playing without feedback (which reveals auditory activity). While playing with normal feedback, primary centers of
   activity were in the motor cortex (hand area) and auditory cortices. By contrast, performances with asynchronous
   delays reduced activity in these areas. The effect of pitch shifts was more complex. On average, pitch alterations led
   to similar activity as found for normal feedback. Closer inspection suggested that pitch alterations led to more
   variability across individuals in brain activity than other conditions.
   Conclusion
   Results suggest that altered feedback disrupts the way in which the brain links perception and action. Though not fully
   consistent with the stratification suggested by Pfordresher (2006) the results nevertheless point to distinct effects of
   asynchronous delays versus alterations to pitches.
   Research and Educational Implications
   Results point to the importance of considering the role of both activation and de-activations in neuroimaging analyses.
   Acknowledgement of Research Funding
   NSF Grant 0704516


12. Violations of expectancy: Eyetracking while sightreading
    Kimberly A. Leiken
    University of Rochester

   Purpose
   To what extent are processing mechanisms similar in language and music? Patel (1998) found that expectation
   violations of language (namely complex vs. ungrammatical sentences) differ by a similar degree in the P600 as those
   in music (i.e. phrases containing an out-of-key chord in a nearby key on the circle of fifths vs. a distant key). Thus in
   music and language, as incongruity increases, the P600 increases. Moreover, there are qualitative differences
   between complex and ungrammatical sentences (as there are between nearby and distant keys). Eye-tracking during
   reading provides fine-grained measures of complexity and expectation violation. Data is reported from an eye-tracking
   study examining the effect of these qualitative differences on processing patterns.
   Methods
   We monitored musicians‘ eye movements during sight-reading and piano-playing. Five original phrases were
   manipulated by expectancy violation type; No Surprise, Melodic Leap (a large pitch gap between notes), or Chromatic
   Pitch (a pitch that lies outside of the diatonic scale). We defined 5 regions: Beginning (before critical measure), Pre-
   Critical (within critical measure before Critical note), Critical, Post-Critical (following Critical note within critical
   measure), and Spillover.
   Results
   Although the interaction between conditions and regions was not significant (p<.71), both experimental conditions
   show much time spent on second pass readings in the Critical region, with Melodic Leap showing slightly shorter
   second pass fixation times than Chromatic Pitch (p<.02). There were significantly more regressions (p<.001) and
   second pass (p<.014) fixations in the Chromatic Pitch condition in the Post-Critical region. (Subjects "skipped ahead"
   to this region, regressed to the Critical region, and returned to the Post-Critical region to complete the score naturally).
   While there are regressions from the Critical regions of both Chromatic Pitch and Melodic Leap conditions,
   significantly more occurred (p<.001) in that of the Melodic Leap condition. Here, subjects reach the Critical region,
   then regress to the Pre-Critical region or even more to the Beginning- where there are indeed more second pass


                                                             36
   readings than in the Chromatic Pitch condition. In summary, when subjects encounter a melodic leap they look
   backward in the score, and with a chromatic pitch, subjects jump ahead.
   Research and/or Educational/Clinical Implications
   These results demonstrate that eye-tracking during sight-reading provides insights into music-reading processes. The
   results suggest that processing of the above expectation violations might be analogous to the local coherence effects
   reported by Tabor, Galantucci, & Richardson (2002); melodic leaps resembling ambiguous reduced types and
   chromatic pitches resembling the unambiguous reduced version. Ongoing experiments address these hypotheses.


13. Encoding of musical notation by violinists and pianists
    Elizabeth Wieland and Andrea R. Halpern
    Bucknell University

   Purpose
   For musicians, reading the musical notation of a phrase may appear to be as simple a process as non-musicians
   reading a sentence of literature, but in fact much is still unknown about how musicians encode musical notation.
   Music readers are assumed to mentally process musical notation the same way regardless of their instrument, but it is
   possible that they in fact differ due to different production routines. In general, we predicted that pianists would be
   more affected than violinists by pitch interference, and vice versa for temporal interference. We also predicted
   violinists will demonstrate initial processing in the right hemisphere, whereas pianists would be more symmetrical.
   Methods
   The interference task tested if different aspects of musical notation (rhythm and pitch) are processed independently or
   interdependently. The participants were asked to recall a ten note musical phrase while an interference melody was
   played. If pitch and rhythm are independent processes, a rhythmic distractor should only disrupt rhythmic processing
   whereas a pitch distractor should only interfere with the processing of pitch. The concurrent task tested the locus of
   the initial processing of musical information by creating hemispheric competition. The participants were asked to tap
   their index finger on one hand while identifying musical intervals presented visually. This task occupied the
   contralateral hemisphere with the tapping task, and depending on the hemisphere used for initial processing of the
   musical notation, hemispheric competition would create a decrement in the tapping rate.
   Results
   In the interference task, pianists were better than violinists in both pitch and rhythm recall, but a main effect of
   Interference was found only in pitch recall. Pitch interference reduced pitch accuracy the most with a trend toward
   pianists being more affected than violinists. Temporal accuracy was not affected by any interference. Pilot testing of
   the concurrent study suggests the expected asymmetry in violinists, with a larger decrement in tapping rates of the left
   hand than pianists.
   Conclusion
   Although performance seemed to be better for pianists than violinists overall, a significant interaction was not found
   (although data collection is ongoing). This implies that musicians do encode musical notation similarly.
   Research and/or Educational/Clinical Implications
   Examining these differences in processing may show how musical notation is encoded by both types of musicians as
   well as possible differences between them. This is a fairly unexplored direction, and would give additional information
   about encoding musical notation to the field of psychology and music as well as to areas of music education.


14. Intonation tendencies in solo a cappella vocal performances
    Johanna Devaney, Jonathan Wild and Ichiro Fujinaga
    McGill University

   Purpose
   The intonation practices of the singing voice are a complex phenomenon that has received only limited attention in the
   literature to date. Previous studies have observed that singers do not conform to either equal temperament or any
   other fixed-intonation system, however none of these studies has explored whether singers‘ intonation practices are
   systematic. This research project examines the ways in which melodic interval tuning in solo a cappella singing relates
   to harmonic context.
   Methods
   The first part of this ongoing experiment considers a set of performances of Schubert‘s ―Ave Maria‖. These
   performances are being recorded explicitly for this study by a group of female vocalists. This group is made up of two
   sub-groups, the first comprised of undergraduate vocal majors and the second comprised of professional singers. The
   second part of the experiment uses a composed melody, designed to have strong harmonic implications. The melody
   repeats several semitone patterns in different harmonic contexts, so as to provide a more controlled evaluation
   environment than the ―Ave Maria‖. The purpose of the first part of the experiment is to explore the commonalities that
   exist in intonation tendencies in the performance of a well-known piece, while the second provides the opportunity to
   examine the role of implied harmonic context more precisely.

                                                           37
   Results
   This research builds on a pilot study that related the intonation tendencies in a single performance to Fred Lerdahl‘s
   theory of melodic attraction and Steve Larson‘s theory of musical forces. Lerdahl‘s theory models melodic attraction
   and harmonic stability, while Larson‘s theory defines the forces of gravity, magnetism, and inertia. In this earlier study
   there was no correlation found between any of the components of these theories. The current study aims to evaluate
   the validity of these results on a larger data set. With this larger data set it will be possible to explore which
   components of these theories are applicable and what, if anything, is missing.
   Conclusion
   The experiment is ongoing.
   Research and/or Educational/Clinical Implications
   Empirical studies of recorded performances are a valuable complement to perceptual studies of performance. Not only
   does this type of ―experiment‖ provide new data with which to test the proposed models, it also provides ecologically
   valid data, as it is acquired from a situation that is more familiar to the participant, i.e., the recording studio.
   Acknowledgement of Research Funding
   Centre for Research in Music, Media and Technology (CIRMMT)
   Fonds de recherche sur la société et la culture (FQRSC)


15. Listeners‟ sensitivity to performers‟ expressive intentions
    Kristen T. Begosh and Roger Chaffin
    University of Connecticut

   Purpose
   Previously, researchers collected self-reports from musicians to determine what musical feelings they intend to convey
   to the audience. Additionally, examinations of bar to bar fluctuations in tempo and volume demonstrated that
   musicians‘ expressivity can change both within a given performance and between performances of the same piece.
   The current study examines whether audience members can perceive these variations in the performers‘ intended
   musical feelings.
   Methods
   A professional cellist and pianist reported their expressive intentions when playing the Prelude from Suite No. 6 for
   solo cello by J.S. Bach and the Piano Sonata No.2 in F Sharp Minor, Allegro non troppo ma energico by Johannes
   Brahms respectively. They recorded their pieces at three levels of expressive intensity – exaggerated, normal, and
   minimal. As participants listened to recordings of each of the six performances, they provided continuous ratings of the
   music‘s expressive intensity and emotional arousal that they experienced. Their evaluations were recorded by tracking
   the position of the mouse cursor as they moved it around the computer screen to indicate the intensity and arousal
   levels they felt at each given moment.
   Results
   Differences in intensity ratings were due to the cello being less intense than the piano. Exaggerated performances
   were less intense than normal ones, which were less intense than minimally expressive performances. Differences in
   arousal ratings were due to the piano being more calming than the cello. The interaction between the piece and
   performers‘ expressive intentions on arousal was due to participants rating the piano piece at the same arousal level
   regardless of the musician‘s intentions while arousal ratings for the cello piece increased from the minimally
   expressive performance to the normal and exaggerated performances.
   Conclusion
   Listeners‘ perceptions were affected by the musicians‘ expressive intentions and the piece of music to which they
   were listening. Analyses completed thus far show this for comparisons between performances. We expect further
   analyses to show that listeners perceived differences in the musicians‘ reported expressive intentions within a given
   performance.
   Research Implications
   Our study shows that musicians can deliberately vary the expressive effect of a highly prepared performance on
   listener‘s experience. To do so, they have several techniques and tools at their disposal including selection of the
   piece itself as well as their intentions to convey musical feelings to their listeners during any give performance of the
   composition.
   Acknowledgement of Research Funding
   This research was supported by Research and Teaching Assistantships to the first author from the Department of
   Psychology, University of Connecticut.




                                                            38
16. Good vocal mimics are also good entrainers: Individual differences suggest a shared mechanism
    for entrainment and vocal mimicry
    Adena Schachner, Timothy F. Brady, and Marc D. Hauser
    Harvard University

   Purpose
   It has been proposed that the ability to entrain to an auditory pulse evolved as a byproduct of selection for vocal
   mimicry (Patel, 2008), and cross-species comparative data has provided support for this theory (e.g. Schachner et al.
   2008). We sought to test the theory that vocal imitation and entrainment depend on a shared mechanism by asking if
   there is a specific correlation between skill at entrainment and skill at vocal imitation in human adults.
   Methods
   Observers completed tasks designed to test their accuracy of entrainment and vocal imitation, as well as a control
   task. In the vocal imitation task, observers heard brief single pitches (13 semitones from a single octave, three times
   each in random order). The differences between the mean pitches of the observers‘ imitations and the true pitches
   were then calculated. In the entrained tapping task, subjects heard isochronous drumbeats and were instructed to tap
   the spacebar in synchrony with the timing of each sound as accurately as they could. Tempi ranged from 30 to 200
   beats per minute, with 30-second blocks of each tempo, for a total length of seven minutes. The control tapping task
   was identical except the beats were spaced pseudo-randomly, creating unpredictable inter-onset-intervals. This task
   was intended to resemble the entrained tapping task, but without the possibility of entrainment. To analyze the tapping
   data, we used a previously published algorithm to establish correspondence between the beats and the observers‘
   taps (Elliott et al. 2008), then calculated the errors between the IOI of the beats and the ITI of the observer.
   Results
    N=16. There was a strong correlation between skill at entrainment and skill at pitch imitation (Spearman's rank-order
   correlation, rho=0.64, p<0.01). No significant correlation was seen between skill at the control task and pitch imitation,
   suggesting that the effect was specific to entrainment and not a result of general motivation or motor skill (rho=0.04,
   p=0.87). In addition, the percentage of times observers skipped a beat or tapped more than once per beat was not
   significantly correlated with performance in the pitch task (all p>0.1), providing further evidence against a general
   motivational account.
   Conclusion
   Our results suggest that a persons' skill at entrainment is strongly correlated with their skill at vocal imitation of pitch,
   and that this correlation is not due to general motivational factors or general motor system ability.
   Research Implications
   This supports the theory that vocal imitation and entrainment rely on a shared mechanism.
   Acknowledgement of Research Funding
   Harvard Stimson Grant.


17. The effect of metre on accuracy and consistency of auditory-motor synchronization
    Benjamin Rich Zendel, Takako Fujioka, Bernhard Ross
    Rotman Research Institute

   Purpose
   Accuracy and consistency of finger tapping to an isochronous auditory stimulus is influenced by metric structure (i.e.,
   the number of tones between taps), the rate of the stimuli (i.e., tempo), and the rate of the tapping target (i.e., the time
   between each tone to be tapped to). We investigated how these three factors interact with each other within the upper
   and lower temporal limits of sensori-motor synchronization.
   Methods
   Eight participants were presented with an auditory stimulus resembling a metronome click and were asked to tap their
   finger to different metric divisions (tap to every single [tap1], second [tap2], third [tap3], or fourth [tap4] click). We
   manipulated the stimulus rate so that in experiment 1 the tapping target rate (inter-beat interval: IBI) varied across
   three levels (780, 1170 & 1560 ms). In experiment 2 the tempo (inter-stimulus interval: ISI) was manipulated across
   three levels (260, 390 & 780 ms). Mean and standard deviation (SD) of timing differences between tap and click onset
   and were used to measure accuracy and consistency.
   Results
   In experiment 1, accuracy was significantly influenced by metre (p<0.01). That is, the larger metric division, the greater
   the accuracy. For consistency, the interaction between metre and IBI was significant (p<0.01). This interaction was
   caused by an increase in consistency for larger metric divisions, especially in the larger IBI conditions. In experiment
   2, accuracy was significantly influenced by both metre and ISI (p < 0.05), but the factors did not interact. Accuracy was
   lowest in the tap3 condition, and accuracy increased as ISI decreased. Metre and ISI also significantly affected
   consistency (p<0.01), but did not interact. Consistency increased as ISI decreased, and consistency was highest in
   the tap1 condition.



                                                              39
    Conclusion
    Metric division improved synchronization when the tap interval was constant and shorter than 1.5sec. In addition, a
    decrease in accuracy observed for tap3 condition suggests a binary metre advantage over trinary metre, in line with
    previous findings. Consistency of tapping increased with greater number of intervening tones, especially at longer tap
    intervals. However, this advantage is reduced as the tap intervals approach the upper (longer) limit of synchronization.
    Research Applications
    Auditory-motor synchronization tasks such as dance may be useful for rehabilitation. Understanding the systematic
    relationship between metre and time perception is helpful for future research on the underlying neural mechanisms of
    auditory motor synchronization.
    Funding
    National Science and Engineering Research Council of Canada (NSERC) & Canadian Institute for Health Research
    (CIHR).


18. The heart of the music: Musical tempo and cardiac response
    Robert J. Ellis, John J. Sollers III, Bradley M. Havelka, and Julian F. Thayer
    The Ohio State University

    Purpose
    Empirical investigations of subjective and cardiac responses to music date back over 125 years. However, the
    literature has been plagued by two methodological/interpretive pitfalls. First, most investigations do not ―recompose‖
    music along a particular dimension (e.g., tempo), but rather use unique pieces of music that vary along a number of
    dimensions (genre, instrumentation, dynamics), adding potential confounds. Second, the utility of measuring mean
    heart rate (HR) changes to music is not particularly strong: for as many studies that report significant effects of music
    on HR, nearly an equal number report a null effect. However, two less-frequently-investigated indices of cardiac
    activity — HR variability and phasic HR — may provide better windows into cardiac response to music. HR variability
    (calculated from frequency-domain analyses of underlying periodicities in HR) is a sensitive measure of
    parasympathetic (restorative) nervous system activity; higher resting levels of HR variability have been consistently
    linked with greater health and emotional well-being, and increased cognitive flexibility. Phasic HR refers to beat-by-
    beat changes in HR, and indexes adjustments in attention and emotional processing.
    Method and Results
    We varied the tempo (60, 90, 120 beats per minute) of computer-generated (MIDI) performances of ragtime piano
    music. Experiment 1 examined HR variability to extended excerpts (2.5-m) of music, and revealed that HR variability
    decreased as tempo increased, indicating greater parasympathetic withdrawal to faster music. This pattern of
    response was more pronounced in subjects with higher resting levels of HR variability. Importantly, no significant
    relationship was found between tempo and mean HR. Experiment 2 examined phasic HR responses to shorter
    excerpts (12–16-s) of music, and revealed that phasic responses increase in magnitude as tempo increases,
    suggesting that tempo differentially affects the heart within a few moments of music onset. Similar to Experiment 1,
    differences in phasic responses as a function of tempo were more pronounced in subjects with higher resting levels of
    HR variability. Experiment 3 used even shorter excerpts (6–8-s) of this music to explore how tempo influences simple
    reaction times (RTs). RTs decreased as tempo increased. This novel finding indicates a role for a previously-
    unexplored stimulus property (tempo) on cognitive performance. We also found that the inverse relationship between
    tempo and RT was more pronounced in subjects with higher resting levels of HR variability.
    Conclusion
    The present results attest to both the value of the ―recomposition‖ method and the utility of the physiological analysis,
    as well as the importance of individual differences in physiological activity and cognitive performance.
    Implications
    Taken together, these data help clarify our understanding of how music impacts the autonomic nervous system.
    Noticeable differences in cardiac response and RTs between individuals with high (versus low) resting levels of HR
    variability indicate that the parasympathetic nervous system mediates physiological and cognitive responses to music.
    Acknowledgement
    This research was supported by The Caroline B. Monahan Fund for Experimental Research Support within the
    Department of Psychology at Ohio State University.


19. The effects of cultural experience and subdivision on tapping to slow tempi
    Sangeeta Ullal, Erin E. Hannon and Joel S. Snyder
    University of Nevada, Las Vegas

    Purpose
    The present study explores the role of cultural exposure as a means of more fully understanding the nature and
    underlying mechanisms of temporal processing constraints. Subdivision can improve synchronization at slow tempi,
    but the ability to utilize subdivisions is constrained by the nature of component interval ratios. Unlike Western music,
                                                             40
   Indian music contains very slow tempi and complex interval ratios. Therefore Indian listeners may provide a unique
   opportunity to examine the effects of culture on sensorimotor synchronization.
   Methods
   American and Indian participants were asked to perform synchronous tapping to a stimulus with a slow tempo (i.e.,
   inter-event intervals of 3 s), which was accompanied by silence or by an unattended rhythmic pattern subdividing
   target intervals into groups of two (simple), groups of three (simple), or alternating units of two and three (complex).
   On a subset of trials, the pattern of subdivision switched halfway through the trial, from a simple to simple, simple to
   complex, or complex to simple.
   Results
   Indians listeners performed comparably across all subdivision patters, but showed an increase in error whenever there
   was a switch of subdivision pattern, regardless of the nature of the switch. By contrast, Western listeners showed
   higher overall error for the complex subdivisions, and an increase in error any time there was a switch away from
   simple subdivision.
   Conclusions
   Cultural exposure seems to influence the extent to which subdivision structures improve performance in general and
   dependent on the type of subdivision structure.
   Research Implications
   This study highlights the nature of the temporal processing constraints, by examining the role of cultural exposure to
   music that violates these constraints.


20. Rocking in synch: Effects of music on interpersonal coordination
    Alexander Demos, Roger Chaffin, Alexandra Lewis, Kristen Begosh, Jennifer Daniels and Kerry Marsh
    University of Connecticut

   Purpose
   Music is often used to help individuals intentionally synchronize with one another. Soldiers, for example, use the beat
   of the music to help keep them marching in step. Music may also provide a means for two people to unintentionally
   synchronize in a task. The current study measures unintentional synchronization through rocking chairs. While
   previous research using rocking chairs has shown the importance of visual information (Richardson et. al, 2007), this
   research highlights the importance of auditory information to unintentional synchronization.
   Methods
   Forty-six participants were paired and sat in identical wooden rocking chairs positioned 0.5m apart and were asked to
   rock at a comfortable pace. The experiment contained three counterbalanced conditions, repeated once. Condition
   one contained no auditory stimuli. Condition two had participants rock their chairs over sandpaper. This allowed
   them to hear their own and their partners‘ rocking. Condition three contained Greek music played in common time with
   a steady tempo of about 64 BPM. Each condition contained three partially counterbalanced back-to-back trials. First
   participants looked forward for 30 seconds at an image and then looked directly at their partners chair or away from
   each other for 70 seconds.
   Results
   Visual Information: When the pairs had direct or peripheral visual information, they averaged more unintentional
   synchronization than when looking away [F(2,38)=17.0, p<.001].
   Auditory Information: Overall, participants synchronized most with the sound of the other rocker [F(2,38)=3.25, p=.05].
   Preliminary results show that when musician pairs (n=3) could not see each other, they synchronized more with each
   other in the second exposure to the music over the other conditions [F(4,40)=2.625, p=.049]. Regardless of visual
   information, mixed pairs of musicians and non-musicians and non-musician pairs did not improve on synchronization
   between the first and second exposure to the music, while musician pairs did [F(2,20)=4.757, p<.001].
   Conclusion
   The initial expectation was that music would create the strongest synchronization between pairs. However, the
   strongest effect was the sound of the other rocker as the other rocker may have acted like a metronome. Only
   musician pairs synchronized in the condition with music and no visual information. Musicians may have the ability
   more quickly sort through the complexities of novel music.
    Research Implications
   The current findings suggest that musicians are more unconsciously attuned to auditory information than non-
   musicians. Ongoing research is investigating whether the type and familiarity of the music and/or the size of the
   difference between the tempo of the music and the natural tempo of the rocker affect unintentional synchronization.




                                                           41
21. The effect of melodic complexity and rhythm on working memory as measured by digit recall
    performance
    Michael J. Silverman
    University of Minnesota

   Purpose
   The purpose of the present study was to isolate the effects of melodic complexity (three versus five versus seven
   pitches) with and without rhythm on working memory as measured by sequential digit recall performance.
   Methods
   The recall of information paired with six different melodies was tested on 60 university students (30 music majors and
   30 non music majors). Melodies were composed using three, five, and seven pitches, both with and without a rhythmic
   component. The texts were randomly determined monosyllabic digits. A CD was created with a female voice singing
   the melodies. Participants listened to all six melodies and then were asked to recall the digits immediately after each
   melody was sung. A Latin Square Design was used in an attempt to control for learning and order effects.
   Results
   Results indicated that there was no significant difference between recall of music majors and nonmusic majors,
   although music majors outperformed nonmusic majors. There was a significant difference in the within subject variable
   of rhythm: the mean recall of information paired with a rhythmic component was higher than recall of information not
   paired with a rhythmic component. Recall of information paired with melodies built upon three notes was poorest while
   recall of information paired with melodies built upon seven notes was highest. Participants recalled information in
   serial positions of primacy and recency most accurately while information in the middle positions was poorest.
   Conclusion
   Congruent with previous research, music majors outperformed nonmusic majors and rhythm served as the dominant
   factor in facilitating recall. Also congruent with the literature, participants‘ recall was greatest during primacy and
   recency positions. However, from the results of this study, it seems that recall is greater when information is paired
   with melodies that are built upon seven pitches as opposed to three pitches.
   Educational & Clinical Implications
   In composing original music to facilitate academic and social objectives, a simple rhythmic component should be used.
   This rhythm serves to ―chunk‖ (organize) the information and thus facilitates its recall. From the results of the current
   study, if educators or therapists are composing melodies to teach social and/or academic material, melodies should be
   built from seven pitches as opposed to three pitches and incorporate a rhythmic aspect. Future research could
   evaluate how timbre and other musical aspects of melodies facilitate recall.




                                                            42
                                          Wednesday, August 5th


Session 6A                 Symposium: Pulse, Meter and Groove I (Chairs: Mike Brady & Edward W. Large)

6A) 9:00 – 9:20

Filtering discordant onsets from complex temporal patterns

Michael Brady
Indiana University

    Purpose
    Many approaches to temporal structure analysis presume that an input pattern will have some underlying pulse that an
    oscillator may entrain to. The oscillator synchronizes with this pulse in the face of local temporal noise and broader-
    scale rate fluctuation. How the oscillator aligns with the elements of a pattern can be charted for comparative analysis.
    With some types of complex temporal sequences however, such as the timing of vowel onsets in the casual stream of
    speech, an underlying pulse is often not readily apparent. In analyzing such patterns, is it folly to seek to entrain an
    oscillator to a possibly nonexistent underlying pulse?
    Method
    The method first asks: "what is the opposite of an isochronous pulse?" If a pattern that exhibits maximal temporal
    haphazardness may be successfully entrained to, the assumption of an underlying or cognitive pulse is reinforced.
    Work begins by establishing a definition for the maximally haphazard temporal sequence. An adaptive oscillator is
    then made to systematically synchronize with a variety of patterns that approach maximal haphazardness by
    essentially filtering out the pattern's discordant onsets. Filtering involves the use of a bank of resonators. Each
    resonator is tuned to resonate with a slightly different natural period and the sum of 'expectancies' from the bank
    provides a measure of how periodic an onset is in relation to preceding onsets. The 'cognitive pulse' adaptive oscillator
    entrains to onsets that have stronger periodicity scores, while onsets receiving weaker periodicity scores are filtered
    from the entrainment function.
    Results
    By initializing the cognitive oscillator from different phase angles and watching how it consistently aligns itself with
    patterns that approach maximal temporal haphazardness, the model is shown to be successful. Furthermore, higher
    periodicity scores from the resonator bank tend to correspond to the events of rhythms that people typically
    synchronize with when asked to tap along to various experimental rhythms as reported in the literature. Thus,
    entrainment by the model reflects human performance.
    Conclusion
    It is not folly to seek to entrain an oscillator to temporal patterns that do not exhibit an obvious underlying pulse.
    Research Implications
    The cognitive oscillator may be applied to the analysis of complex temporal patterns using circular statistics. In
    languages like Japanese for instance, where timing cues serve as the basis for phonological distinctions, such an
    analysis may allow a speech parser to distinguish what was said based on temporal relationships.
    Research Funding This work was partially supported by an NSF EAPSI fellowship.

6A) 9:20 – 9:40

A probabilistic model of downbeat identification

Leigh M. Smith
IRCAM

     Purpose
    A number of computational models of musical rhythm and beat induction have been previously presented. Those
    successful approaches using oscillator entrainment, resonant comb filterbanks, Fourier, autocorrelation or wavelet
    methods identify quasi-periodic frequencies contained in the rhythm being analysed. In several of these systems,
    identification of the metrical period, as well as the beat period (tactus), is possible. This success in determining meter
    and beat period does, however, raise the question as to how a listener infers the phase of the meter. That is, how is
    the position of the downbeat determined? Meter is considered to arise from the interaction between rhythmic
    frequencies (strata), yet it is not yet clear how that interaction leads to an unambiguous interpretation of metrical


                                                              43
    phase that rapidly arises in the minds of listeners. It is clear this is not entirely a top down process, illustrated by the
    ability to experience and resolve changes of meter and syncopation.
     Methods
    A model of the bottom-up component of the identification of downbeat is proposed and is currently under evaluation.
    This probabilistic model estimates the most likely choice of beat, within the metrical period, that functions as the
    downbeat. A number of feature observations provide evidence for this estimation. Such observations are performed on
    a continuous measure of onset detection salience from the spectral flux of the audio signal. These observations are
    amassed using Bayesian maximum likelihood estimation to determine the likely downbeat location.
    Results
    The observations being evaluated include: a number of onset detection salience profiles derived from the currently
    perceived metrical structure; metrical locations of perceptually longer local intervals; locations of intervals exceeding
    the beat period; and characterisations of rhythmic phase derived from continuous wavelet representations of the
    rhythm. The model is currently being evaluated against a dataset of 250 annotated recordings of a variety of Western
    popular music.
     Conclusion
    While intensity and duration in experimental stimuli function as accents, in recorded music, these cues are not as clear
    and often contradictory. The proposed model suggests that a combined set of feature observations is necessary to
    successfully estimate the downbeat.
    Research Implications
    This research studies listeners‘ ability to identify downbeat from contradicting cues. It represents that capability as a
    weighted likelihood estimation from evidence from the temporal structure of the audio energy. This attributes the
    contribution of each form of accent to the perception of rhythmic structure.
     Acknowledgement of Research Funding
    This research is performed in the context of the Quaero project (http://www.quaero.org).


6A) 9:40 – 10:00

A damped oscillator model for relating the temporal structures of bimanual tapping responses and
complex musical stimuli

Petr Janata and Stefan Tomic
University of California, Davis

    Purpose
    We describe a model based on reson filters (damped oscillators) that serves as a common framework for the analysis
    of the temporal structure in both musical and behavioral data. We illustrate its use in the analysis of data from an
    experiment in which 34 subjects tapped along with excerpts of recorded music.
    Methods
    We collected bimanual tapping responses to 48 30-s excerpts of music that varied in genre and the degree of
    perceived "groove," with the objective of determining whether there was any systematic variation in the model output
    as a function of how much a person felt "in the groove" while tapping, how much they enjoyed the task, and how
    difficult they found the task. The prominence of periodicities at metric and non-metric levels was assessed with mean
    periodicity profiles (MPPs; distributions of energy across the bank of reson filters). The number of peaks in the MPPs
    was compared across tapping conditions (isochronous and freeform) and music conditions (high groove, mid groove,
    low groove, silence).
    Results
    On average, high groove stimulus MPPs had fewer peaks than mid or low groove MPPs. With few exceptions, the
    major peaks in the MPPs from isochronous and freeform tapping conditions coincided with the major peaks in the
    MPPs of the stimuli. Release of the isochronous tapping constraint resulted in freeform tapping MPPs having more
    peaks, many of which coincided with other peaks in the stimulus MPPs, indicating that subjects spontaneously
    synchronized with more complex aspects of the metric structure. However, additional peaks were also observed that
    did not correspond to the stimulus MPPs, hinting at synchronization failures. Subjective ratings obtained following
    each trial were correlated with the number of peaks in the MPPs such that higher experienced difficulty led to more
    peaks, and higher experienced groove and enjoyment led to fewer peaks. With the exception of experienced difficulty,
    these relationships were observed only in the isochronous tapping condition and primarily for mid and low groove
    stimuli.
    Conclusion
    The pleasure experienced when "finding the beat" in music is related to properties of the metric structure in the music
    and the metric structure of the participant's actions as assessed by a damped oscillator model.
    Research Implications
    Many further metrics can be derived from the model output to better understand the coupling of music, behavior, and
    affect.

                                                              44
   Acknowledgement of Research Funding
   This research was supported by a Templeton Advanced Research Program grant from the Metanexus Institute.


Session 6B                Cross-Modal Interactions                (Chair: Scott Lipscomb)

6B) 9:00 – 9:20

The effects of sensori-motor learning on melody processing

Elizabeth Wakefield and Karin Harman James
Indiana University

   Purpose
   To investigate how motor systems interact with, and/or contribute to the learning of sung melodies, using functional
   Magnetic Resonance Imaging (fMRI).
   Method
   Nine university students underwent a training session, followed the next day by an fMRI scan. During training,
   participants learned 12 melodies under 3 conditions using a recall procedure adapted from Racette and Peretz (2007).
   In an auditory condition, participants heard melodies sung on nonsense syllables (analogous to solfège syllables) and
   repeated them. In a motor condition, in addition to hearing melodies, participants learned hand signs corresponding to
   notes in the melody (analogous to Curwen hand signs). In a visual condition, in addition to hearing melodies, the
   syllables being sung were presented visually. Training was complete when participants had sung each melody
   accurately without help from the experimenter. During the fMRI session, participants listened to learned or novel
   melodies in a random order. Participants indicated whether they remembered learning each melody via button-
   presses on a hand paddle.
   Results
   Behavioral results: The addition of motor or visual components during training may be detrimental to melody learning.
   Participants performed better than chance on recognition of learned melodies in the auditory condition only, indicating
   that the melodies in the motor and visual conditions were not as well committed to memory.
   Functional MRI results: Reactivation of brain regions corresponding to the modality specific to each learning condition
   (motor, auditory, visual) were seen subsequent to learning in auditory and visual conditions, but not in the motor
   condition. After auditory learning, activation occurred in auditory cortex, some frontal regions, and Broca‘s area
   bilaterally. After visual learning, middle occipital gyrus activation occurred bilaterally. Although motor cortex activation
   was seen in four participants after motor learning, it was sub threshold overall.
   Conclusion
   Learning melodies using different modalities results in some reactivation of brain regions associated with these
   modalities during later presentations of the melodies. Additionally, although learning with multiple modalities may be
   beneficial over long-term learning, short-term melody learning is most successful when individuals learn though
   audition alone. Perhaps individuals concentrate less on the melodies if they are visually represented, or cannot learn
   as easily if their attention is divided between melodies and motor patterns.
   Research & Educational Implications
   The present study adds to the understanding of multisensory integration in the brain. It also has implications for music
   education, as it demonstrates that incorporating motor movements into learning, like Curwen hand signs, may not be
   beneficial to music learning in the short term.
   References
   Racette, A. & Peretz, I. (2007). Learning lyrics: To sing or not to sing. Memory & Cognition, 35, 242-253.


6B) 9:20 – 9:40

Visual fixation patterns and the contribution of visual dynamics in perception of vocal music

Frank A. Russo, Michael Maksimowski and Gillian Sandstrom
Ryerson University

   Purpose
   Numerous studies have highlighted the importance of visual information in perception of music. Thompson & Russo
   (2007) demonstrated that in the context of vocal music, relative size of intervals can be discerned on the basis of
   visual information alone. Video-based motion tracking revealed a strong correlation between interval size and maximal
   displacement of the head, eyebrows, and mouth.


                                                             45
   In the current study, we utilize two complementary methodologies (eye tracking and point-light animation) in order to
   better understand the key determinants of visual influence in perception of vocal music.
   Methods
   Experiment 1: Participants viewed audio-visual recordings of sung intervals under 3 signal-to-noise conditions (high,
   medium, low). Participants made judgments of interval size while gaze fixations were tracked using a Tobii x50 eye
   tracker.
   Experiment 2: Participants viewed visual-only recordings of sung intervals under full-light (original) or point-light
   conditions. In each trial, participants were required to judge which of two intervals presented in sequence was larger
   (forced choice).
   Results
   Experiment 1: Frequency and duration of gaze fixations increased as the signal-to-noise ratio decreased (i.e., more
   fixation as listening conditions worsened). Fixation points were distributed across the face with highest concentration
   on the mouth followed by the eyes, and biased overall to the right side (viewer‘s perspective).
   Experiment 2: In both full-light and point-light conditions, accuracy increased as a function of interval size difference,
   with accuracy surpassing chance for intervals differing by 4 semitones or more. Overall accuracy was highest in the
   full-light condition.
   Conclusion
   Perhaps not surprisingly, we found that in making judgments of interval size, viewers tend to track those visual
   features that previous research has shown to be well correlated with interval size, and the extent of tracking intensifies
   as listening conditions worsen. Critically however, the eye tracking allowed us to determine that the mouth is the key
   feature of visual interest. The overall right-side bias is likely task dependent, arising from an analytic strategy that
   favors the left-hemisphere (i.e., contralateral to bias of fixation).
   The above-chance accuracy of discriminations in the point-light condition suggests that dynamic visual information
   contributes to judgments of interval size.
   Research and/or Educational/Clinical Implications
   The current study corroborates the view that individuals may use visual information to support judgments of structural
   information in vocal music. This insight suggests opportunities for enhancing the experience of music in deaf and hard
   of hearing listeners.
   Acknowledgement of Research Funding
   This study was supported by a grant from the Natural Sciences and Engineering Research Council of Canada
   awarded to FA Russo.


6B) 9:40 – 10:00

The color of music: Cross-modal sensory perceptions in musicians

Matthew McCabe, David Biun and Jamie Reilly
University of Florida

   Purpose
   Recent research has discovered cross-modal, possibly synaesthetic associations in healthy, normal adults. Our
   experiments apply concepts from previous research to musicians, with the purpose of understanding rapid cross-
   modal associations and their role in music perception as well as clarifying understanding of human musical capacities
   in both trained and untrained subjects. Further, the adaptations acquired with musical training can give insight into
   neural plasticity, the nature of learning, and the origins of music and language.
   Methods
   Sixty age and gender-matched right-handed native English speakers were divided into two groups: musicians and
   non-musicians. A third group, individuals with absolute pitch, were also tested for the purposes of contrast. All
   participants were screened with color blindness and hearing acuity measures and were administered Gordon‘s AMMA
   prior to testing. In a series of 4 experiments, participants made matching pairs of sounds and colors while viewing
   auditory and visual stimuli by turning dials that controlled pure tone frequency, piano tone height, color hue, and color
   luminance. When the presented stimulus was auditory, the participant was asked to make a visual match, and vice-
   versa.
   Results
   Previous studies have indicated that healthy adults behave like synaesthetes in associating tones with colors and
   phonetic features to physical forms. Musicians, due to training in the auditory sensory realm, show stronger cross-
   modal associative consistency and decreased search time in finding matching pairs.
   Conclusion
   Healthy adults are consistently sensitive to the relationship of pitch height to both hue and luminance, indicating that
   cross-modal associations are a normal perceptual process. Musicians, with specific auditory training, show a greater
   intersensory affinity, demonstrating that the processes involved in cross-modal associations utilize parts of the brain
   also utilized during the perception and production of music.

                                                            46
   Implications
   Since the cortical areas responsible for cross-modal relationships play an important role in musical processing, our
   results have implications for deeper understanding of neural plasticity and musical training effects. Specific
   understanding of cross-modal relationships also offers insight into the evolution, production, and reception of music
   and language. Potential applications include novel means of testing the integrity of association cortexes, cross-modal
   Stroop paradigms, the facilitation of auditory training, and a deepening of the techniques used in art, music, and
   language-based education and therapies.
   Acknowledgement of Research Funding
   All costs were covered by the startup funds of Dr. Jamie Reilly, Departments of Communicative Disorders and
   Psychology, University of Florida.


Session 7A               Symposium: Pulse, Meter & Groove II (Chairs: Mike Brady & Edward W. Large)

7A) 10:20 – 10:40

Modeling of pulse and meter as neural oscillation

Edward W. Large and Marc J. Velasco
Florida Atlantic University

   Purpose
   In rhythm perception the experience of periodicity, namely pulse and meter, can arise from stimuli that are not
   periodic. One possible function of such a transformation is to enable attentional and behavioral synchrony among
   individuals through perception of a common abstract temporal structure. Thus, understanding the brain processes that
   underlie rhythm is fundamental to explaining musical behavior. Here, we ask whether a model of neural resonance
   can account for important aspects of human rhythm perception, rhythmic attending and perception-action coordination.
   Methods
   We derived a canonical model of neural oscillation and used it to define a gradient frequency neural oscillator network
   (GFNN). A GFNN is a one dimensional network of neural oscillators, tuned to different natural frequencies, and
   arrayed along a frequency gradient. Such networks are conceptually similar to banks of bandpass filters, except that
   the resonating units are nonlinear rather than linear. We stimulated the networks with a variety of rhythms from simple
   isochronous sequences to complex rhythmic patterns. We asked how well the behavior of the network matched
   certain well-known features of human behavior, and also compared the predictions of the nonlinear model with those
   of a linear filterbank.
   Results
   The GFNN reproduces several effects observed in perception, attention and coordination. These include persistence
   of pulse in the absence of stimulus events, entrainment with rhythmic stimuli, tendency of pulses to precede auditory
   events in some circumstances, and a form of metrical accent in which active endogenous processes contribute to
   perceived stress patterns. Some of these properties are observed in linear systems, others are not.
   Conclusion
   Neural resonance provides an explanation for many of the basic results associated with human responses to musical
   rhythm. In addition, we discuss several new predictions that have no correlate in music-theoretic models of pulse and
   meter.
   Research Implications
   This approach has the potential to link neurophysiology directly with behavior, an important goal of cognitive
   neuroscience. It also makes novel predictions about the perception of rhythm in music.
   Acknowledgement of Research Funding
   AFOSR FA9550-07-C0095


7A) 10:40 – 11:00

Using cross-entropy to test models of common-practice rhythm

David Temperley
Eastman School of Music

   Purpose
   This study explores ways of modeling the compositional processes involved in common-practice rhythm (as
   represented by European classical music and folk music). I consider a series of probabilistic models of rhythm and


                                                           47
    evaluate them using the method of cross-entropy: the best model is the one that assigns highest probability to the
    data.
    Methods
    Six models were evaluated. The uniform position model decides at each beat (metrical position) whether or not to
                                                                              th
    generate a note there, with notes equally likely on all beats. The zero -order duration model chooses time-points for a
    series of notes, assigning different probabilities for different inter-onset intervals (time intervals to the previous note).
    The metrical position model decides whether to generate a note on each beat, dependent on the strength of the beat
    at that point. The fine-grained position model is similar to the metrical position model but distinguishes between
    different metrical positions of the same strength (for example, the second and fourth quarter-note of a 4/4 measure).
    The hierarchical model decides at each beat whether or not to generate a note there, conditional on the note status of
    neighboring strong beats (i.e. whether or not they contain notes). The first-order duration model chooses time-points
    for a series of notes, dependent both on the metrical position of the note and that of the previous note. Each of the
    models above can be formulated to assign a probability to a rhythmic pattern. The parameters of the models were set
    from a corpus of melodies in 4/4 time from the Essen folksong corpus; the models were then evaluated by the
    probability they assigned to a different set of melodies (also Essen melodies in 4/4 time).
    Results
    The model achieving lowest cross-entropy was the first-order duration model; the hierarchical model was a close
    second. When simplicity (number of parameters) is also considered, it is argued that the hierarchical model is
    preferable overall.
    Conclusion
    When both cross-entropy and simplicity are taken into account, a hierarchical model is the most plausible model
    (among those considered) of the process of common-practice rhythmic composition.
    Research Implications
    This study demonstrates the feasibility and value of the cross-entropy approach as a method of evaluating models of
    compositional processes.


7A) 11:00 – 11:20

Analytical and computational modeling of musical groove
               1                2                      2
Peter Martens , Petr Janata , and Stefan Tomic
Texas Tech University, University of California, Davis

    Purpose
    This presentation will demonstrate a necessary linkage between the psychological/computational investigation of
    musical groove and the traditional music-theoretical tools of transcription and analysis. Janata, Tomic, and Haberman
    (2007) asked subjects to assess groove in 48 excerpts of commercially-available music while engaged in either
    isochronous or freeform tapping, and classified each excerpt as high-, medium-, or low-groove based on these
    assessments. Subjects‘ tapped periodicities were compared with a computer model‘s assessment of the excerpts‘
    meter. Some aspects of subjects‘ tapping were found to be correlated with groove judgments, but no linkage between
    the computer model‘s determination of meter and subjects‘ groove ratings were made, nor could be made. The
    sophisticated computer model correctly identified the primary meter in most excerpts, but could only suggest cross-
    rhythms in excerpts by identifying periodicities in the signal that were weaker than those in the primary meter. While
    some groove patterns exhibit this type of consistent cross-accentuation in the form of conflicting pulse streams, most
    grooves are enlivened with local events, short-term groupings that contradict the primary meter, and inconsistent
    interactions between timbral layers, all of which can be conceived as syncopation, broadly construed. The goal of this
    presentation is to bring this level of information into contact with subjects‘ tapping-influenced judgments of groove, and
    to suggest possible refinements to existing computer models.
    Methods
    Ten 30-second excerpts from Janata et al. (2007) were transcribed by hand; four were drawn from their high and low
    groove categories, and two from their medium groove category. Syncopation in a representative four-bar phrase in
    each of these excerpts was calculated using a quantitative analytical model based on the syncopation definitions of
    Huron & Ommen (2006).
    Results
    Results indicate that the analytical model‘s output matches listeners‘ judgments of groove in Janata et al. (2007), and
    thus could be predictive of similar judgments in future studies.
    Conclusions & Research Implications
    Computer-based analysis of rhythm and meter in audio signals benefits greatly from either 1) comparison with music-
    theoretical analyses, or from 2) building into future computer models such analytical judgments, as well as the generic
    and timbral knowledge on which they are based. For assessments of musical groove to move beyond observation and
    classification, analytical attention must be paid to the complex rhythms and multi-layered texture that creates groove.


                                                               48
11:20 – 11:40              Discussion



Session 7B                 Timbre                                 (Chair: John Hajda)
7B) 10:20 – 10:40

Roughness ratings for just- and micro-tuned dyads from expert and nonexpert listeners
                  1                           2
Susan E. Rogers and Stephen McAdams
                        1                    2
Berklee College of Music , McGill University

    Purpose
    The assumption that expert and nonexpert listeners assess the perceptual qualities of isolated musical sounds
    similarly has been challenged by recent neurophysiological findings. Outside of a tonal context, musical intervals
    evaluated for their sensory properties are under the influence of top-down, cognitive-based processing that affects
    judgments of the magnitude, variety, and relevance of auditory properties. Thus the evaluated sensory dissonance of
    dyads (two simultaneous tones) is not immune to extra-stimulus factors such as how an interval functions in a given
    musical culture.
    Method
    To explore the extent to which musical experts and nonexperts agreed, listeners rated dyads for auditory roughness –
    a primary component of sensory dissonance. The variability of internal roughness standards and the influence of
    musical training on roughness evaluation were compared with objective ratings from two auditory roughness
    analyzers. Stimulus sets included dyads in familiar, just-tuned frequency-ratio relationships as well as microtuned
    dyads – mistuned from the familiar Western standard by a quartertone.
    Results
    Roughness ratings by nonexperts showed greater instability (more variance) than those of experts and less influence
    of the frequency-ratio relationship between tones. Nonexperts‘ pure-tone dyad ratings provided a closer match than
    experts‘ to previously published sensory dissonance data (Plomp & Levelt, 1965). Experts‘ complex-tone (just-tuned)
    ratings provided a closer match than nonexperts‘ to earlier work describing the role of harmonic relationships in
    Western consonance perception (Hutchinson & Knopoff, 1978). The highest agreement between expert and nonexpert
    listeners was seen in the microtuned dyad assessment. Roughness analyzers showed strong correlations with
    listeners‘ just-tuned dyad ratings but correlated weakly with microtuned dyad ratings.
    Conclusion
    Assessment of the perceptual attributes of chords in isolation is mediated by top-down, knowledge-based processing.
    Sources of error or intrasubject variability in psychophysical scaling judgments can be reduced through experimental
    design accounting for the listener‘s experience with the attribute under evaluation.
    Research Implications
    Accounting for sources of listener variability in the perception of musical qualities assists in the development of audio
    analyzers and experimental protocols, and aids in the interpretation of sensory dissonance findings.


7B) 10:40 – 11:00

Vibrotactile discrimination of musical timbre

Frank A. Russo, Michael Maksimowski, Maria Karam, and Deborah Fels
Ryerson University

    Purpose
    Research concerning music and vibration has primarily focused on the role of feedback in music performance.
    Spurred by rapid developments in tactile displays and multimodal integration, we have been investigating the potential
    for vibration to influence auditory judgments and as sensory substitution for the deaf and hard of hearing (Karam,
    Russo, & Fels, under review, IEEE Transactions on Haptics). The goal of the present study was to investigate the use
    of vibrotactile information for discrimination of musical timbre.
    Methods
    Experiment 1: Complex vibrotactile waveforms generated from an acoustic signal were presented to the back via voice
    coils embedded in a conforming chair. Participants made same-different judgments based on timbre (piano, cello,
    trombone). Stimuli were presented at each of 3 levels of fundamental frequency (110, 220, 440Hz). All stimuli were
    equated for vibration intensity. Continuous white noise was presented over headphones to mask residual sound
    created by chair vibration.

                                                             49
    Experiment 2: The method was identical to that used in Experiment 1 except that additional precaution was taken to
    mask sound that may have resulted from bone conduction (i.e., not airborne sound). In addition to presenting the white
    noise as sound over headphones, it was also presented as vibration on the cheekbones via Tactaid skin stimulators.
    Experiment 3: The method was identical to Experiment 2 except that participants made same-different judgments in
    steady-state tones that varied only with regard to spectral centroid (dull vs. bright). All stimuli were presented at 220
    Hz.
    Results
    Experiment 1 and 2: Results in both experiments indicated that participants were able to discriminate timbre (piano,
    cello, trombone) significantly better than chance. Optimal discrimination performance occurred at 220 Hz.
    Experiment 3: Results indicated that participants were able to discriminate timbre significantly better than chance on
    the basis of spectral centroid alone.
    Conclusion
    Participants were able to make reliable discriminations of complex vibrotactile waveforms. This ability does not depend
    on overall intensity or the amplitude envelope, which is suggestive of some type of Fourier transform. Performance
    was optimal at 220 Hz (A3), which approximates the peak sensitivity of the Pacinian corpuscles.
     Research and/or Educational/Clinical Implications
    The findings suggest that vibration is an important candidate modality for the development of sensory substitution
    systems that may enable better access to music for deaf and hard of hearing populations.
    Acknowledgement of Research Funding
    Natural Science and Engineering Research Council of Canada grants awarded to the first and last authors supported
    this work.



7B) 11:00 – 11:20

The spectra of average orchestral instrument tones

Joseph Plazak, David Huron, and Benjamin Williams
The Ohio State University

    Purpose:
    The spectrum for an average orchestral instrument tone is presented based on timbre data from the Sandell Harmonic
    Archive (SHARC). This database contains static (time invariant) spectral analyses for 1,338 recorded instrument tones
    from 23 Western instruments ranging from contrabassoon to piccolo. Although several caveats apply, music
    perception researchers might find the calculated ―average tone‖ and ―average instrument‖ more ecologically
    appropriate than other common technical waveforms, such as sine tones or pulse trains.
    Methods:
    An average of all 1,338 spectral analyses in the SHARC database was calculated, yielding what might be considered
    an ―average orchestral instrument tone.‖ Additionally, average spectra were calculated individually for 81 pitches
    ranging from B0 to G7. Each of these tones represents the average of all instruments in the SHARC database capable
    of producing that pitch. These latter tones reveal common spectral changes with respect to pitch register, and taken
    collectively, might be regarded as an ―average orchestral instrument.‖
    Results:
    Synthesized versions of both the ―average orchestral tone‖ and the ―average orchestral instrument‖ are permanently
    archived at the Knowledge Bank website (https://kb.osu.edu/).
    Research Implications:
    Average orchestral tones may prove useful in a variety of research applications where representative, yet highly
    controlled stimuli, are needed.


7B) 11:20 – 11:40

Predicting perceptual differences between musical sounds: A comparison of Mel-Band and MFCC
based metric results to previous harmonic-based results
                         1                    2,                          3
James W. Beauchamp , Hiroko Terasawa and Andrew B. Horner
                      1                     2                                    3
University of Illinois , Stanford University , Hong Kong Univ. of Sci. and Tech.

    Purpose
    In order to arrive at an objective measure of ―perceptual distance‖ between similar musical sounds, listeners were
    tested for their ability to discriminate between eight original sustained musical instrument sounds and modified


                                                             50
    versions of these sounds. Results for metrics based on harmonic amplitudes measured from the musical sounds were
    reported by Horner et al., in J. Audio Eng. Soc., Vol. 54, No. 3, 2006. It was found that a metric based on relative
    Cartesian distance between harmonic spectra performed best of the metrics tested with a maximum R2
    correspondence of 91%. This paper‘s objective is to compare the harmonic-based results with more recently obtained
    results based on mel-bands and mel-frequency cepstral coefficients (MFCCs). MFCCs have had much success for
    categorization of speech sounds in speech recognition programs and show promise for compact representation of
    sound spectra in general and musical sounds in particular.
    Methods
    Eight 2-s Eb4 instrumental sounds (bassoon, clarinet, flute, horn, oboe, saxophone, trumpet, and violin) were
    spectrally modified. Each sound‘s time-varying sound spectrum was randomly altered under constraints that preserve
    the sound‘s spectral centroid and loudness. Modification was done by filtering the original spectrum by a fixed function
    of frequency, which randomly varies around unity in the range [1 - 2ε to 1 + 2ε], so that the relative standard deviation
    error was approximately ε. Spectral alterations were applied at 50 levels of ε (0.01 to 0.50), resulting in one original
    sound and 50 modified sounds for each instrument. Discrimination between each original sound and each of the
    corresponding spectrally altered sounds was tested with a 2AFC procedure, with 20 participants (10 musicians, 10
    non-musicians), resulting in eight plots of percentage correct discrimination vs. epsilon. Perceptual trends were
    analyzed using a 4th-order regression model employing various spectral representations as explanatory variables.
    The spectral representations include variations of mel-band-based and MFCC-based metrics. In addition to these
    representations, exponents were varied in distance calculations from 0 to 3, where 2 is the standard Euclidean
    distance exponent.
    Results
    Preliminary results indicate that mel-band measurements give R2 correspondences of up to 90% while MFCCs give
    correspondences up to 89%.
    Conclusion
    Mel-band and MFCC results almost exactly match the results obtained from harmonic-based metrics.
    Acknowledgement of Research Funding
    This work is supported in part by Hong Kong Research Grant Council‘s Projects 613806 and 613508.


Session 8A                 Pitch                                   (Chair: Frank Russo)

8A) 1:30 – 1:50

Are pitch-class profiles really „key for key‟?

Ian Quinn
Yale University

    Purpose
    Most current approaches to key-finding, either from symbolic data such as MIDI or from digital audio data, rely on
    pitch-class profiles. I will present an alternative approach for a specific repertoire (four-part chorales) that is based on
    two ideas: first, that chord progressions, understood rather loosely as pairs of neighboring harmonic states
    demarcated by note onsets, are sufficient as windows for key-finding, at least in the chorale context; and second, that
    the encapsulated identity of a chord progression (modulo pitch-class transposition and revoicing) is sufficient to do key
    finding – even without enabling the algorithm to reduce progressions to their constituent pitch-class distributions. The
    system has no access to explicit information about a chord progression other than its transpositional distribution in the
    training corpus, yet it is able to reach a remarkable degree of subtlety in its harmonic analysis of chorales it‘s never
    heard before. This suggests that pitch-class reductionism might not be necessary for a principled account of key.
    Methods
    We asked ten upperclass music majors at Yale University to analyze five randomly-selected chorales, identifying keys
    and locating key changes. We compared our algorithm‘s key-finding judgments, and the results of several other key-
    finding algorithms, against these experts‘ judgments.
    Results and Conclusion
    While all algorithms proved equally good at identifying the overall key of the chorales, our algorithm outperformed the
    others at identifying and localizing modulations (localized key changes). Part of this is due to the fact that our
    algorithm is designed to operate with a window of just two chords, enabling it to make quick key judgments and to
    localize a key change at a single chord. We also establish that the algorithm‘s privileging of the bass line is a key
    component of its high performance, and that a version of the algorithm that treats the bass line as structurally
    equivalent to the other voices fails to perform as well.
    Research Implications
    While the success of this algorithm has obvious implications for research in key-finding – which has generally been
    based on an architecture involving pitch-class profiles – it is not obvious how this algorithm could be generalized to


                                                              51
   styles of music that are not as rhythmically and harmonically regular as homophonic chorales. Some suggestions for
   future research along these lines will be given.
   Acknowledgement of Research Funding
   This research was made possible by a fellowship from the Center for Advanced Study in the Behavioral Sciences at
   Stanford University.

8A) 1:50 – 2:10

Hearing Interval Patterns in Atonal Melodies

Jenine Lawson
Eastman School of Music

   Purpose
   This study focuses on implicit learning of interval patterns in an atonal musical language. It investigates whether
   listeners can abstract interval patterns into trichords (three-note groups).
   Methods and Results
   Pilot subjects (n=21) were freshmen music majors at Ithaca College. In experiment 1, listeners (n=11) heard a
   familiarization phase consisting of the 48 versions of a derived 12-tone row. The row, <014_985_632_7te>, has four
   consecutive trichords, all of which are set-type [014]. Each [014] contains adjacent-intervals (AIs) 1 and 3. The
   nonadjacent-interval (NAI) between first and third notes of each [014] spans 4 semitones. During each trial phase
   (randomly ordered), listeners heard an [014] and some other trichord and were asked to identify which trichord was
   more idiomatic of the familiarization phase. Stimuli were synthesized piano tones between F2-A5, and each note
   lasted .33 seconds. Performance on correct discrimination between [014] and other trichords heard within the row
   ([037], [013], and [015] that occur half as often) is 72.7% (SE=.12). This is significantly higher than listeners‘ ability to
   correctly discriminate between [014] and lures constructed with AIs 1 and 3 but arranged such that the NAI 4
   (M=34.1%, SE=.19). The procedure in experiment 2 was the same as experiment 1, but listeners (n=10) heard a
   different row during the familiarization phase. The row <014_958_632_7et> contains four [014] trichords, each
   illustrating the three possible ways to order a trichord:
                   <014>,<632>        AIs=1,3; NAI=4
                   <958>              AIs=4,3; NAI=1
                   <7et>              AIs=4,1; NAI=3
   Performance on correct discrimination between [014] and lures never heard in the row is 90.0% (SE=.04). These
   lures (e.g.<t01> and <037>) contain two of [014]‘s intervals (1, 3, or 4). This is significantly higher than listeners‘
   ability to correctly discriminate between [014] and lures containing two AIs heard within the row such as <586> and
   <327> (M=74.3%, SE=.04).
   Conclusions
   Experiment 1: When listeners hear a row consisting of repeated interval patterns, they extract adjacent-intervals
   patterns but not the set-type. Experiment 2: When listeners hear a row consisting of a set-type arranged with different
   NAI patterns, they learn the set-type itself, an initial step in hearing structure in trichordal array music.
   Research Implications
   Each of these experiments attempts to create a simplified ―atonal language‖ similar to the artificial languages in J.R.
   Saffran‘s (2003) research, except that transitional probabilities are based on interval patterns rather than note or
   syllable patterns. In addition to Kuusi (2003), this study is one of the first to demonstrate that listeners have an ability
   to hear set-types.
   References
   Kuusi, T. (2003). The role of set-class identity in the estimation of chords. Music Theory Online, 9(3).
   Saffran, J. R. (2003). Musical learning and language development. Annals New York Academy of Sciences, 999, 1-5.


8A) 2:10 – 2:30

Temporal pitch perception – An ERP study of iterated rippled noise

Blake E. Butler
McMaster University

   Purpose
   Pitch processing is essential for understanding music. Pitch is derived in the auditory system through complex
   spectrotemporal processing. There is considerable evidence that a temporal code is particularly important for the
   perception of pitch. The purpose of this study was to isolate the mechanism of temporal pitch perception using EEG
   and iterated rippled noise (IRN) stimuli.

                                                              52
   Methods
   The stimuli for this study were 485ms iterated rippled noise samples consisting of a segment of frozen white noise,
   repeated in 5 and 6 ms intervals to create sounds with either 200 Hz or 167 Hz pitch sensations, respectively. The
   stimuli were then high pass filtered at 2600 Hz to remove the resolvable harmonics of the fundamental frequency, and
   low pass filtered at 4500 Hz to remove noisy areas of the spectrum that failed to contribute to the sensation of pitch.
   The 167 Hz sound was presented on 85% of trials (standards), while the remaining 15% (deviants) consisted of the
   200 Hz sound. EEG was collected from adult participants using an EGI system with 128 channel HydroCel GSN nets.
   Following data collection, EEG trials were averaged within each individual to create difference waves representing the
   difference between the deviant and the standard stimuli. We expected a mismatch negativity (MMN) component in
   response to occasional changes in the perceived pitch of the IRN. MMN reflects processing in secondary auditory
   cortex and is typically seen in response to a change in a repeating sound; it is thought to reflect the updating of
   auditory sensory memory traces.
   Results
   The difference waves showed a mismatch negativity response peaking at 250ms after onset of the deviant tones. The
   MMN had a right, frontal focus on the surface of the head with a left posterior reversal.
   Conclusion
   Although the shift in perceived pitch in this experiment was relatively difficult to hear, it succeeded in eliciting a
   mismatch negativity response. Furthermore, since both stimuli contained information in the same spectral range, with
   no cues at resolvable harmonics of the frequency of the perceived pitch, it is apparent that the temporal information
   coded within the IRN stimuli is responsible for the pitch sensation.
   Research and/or Educational/Clinical Implications
   EEG measures do not require a response from subjects, so we can now investigate the development of the temporal
   mechanism for pitch perception in children and infants.
   Acknowledgement of Research Funding
   This research was funded by a grant to LJT from the Canadian Institutes of Health Research.


8A) 2:30 – 2:50

The effect of task and pitch structure on pitch-time interactions in music
          1                     1                       2
Jon Prince , Mark Schmuckler , and Bill Thompson
                     1                       2
University of Toronto , Macquarie University

   Purpose
   Musical pitch-time relations were explored by investigating the effect of temporal variation on pitch perception (Prince,
   Schmuckler, & Thompson, in press). Previous research (Prince, Thompson, & Schmuckler, 2008) demonstrated an
   asymmetric effect of pitch structure (tonality) on temporal judgments, but not vice versa. These findings conflict with
   dynamic attending theory research that demonstrates effects of temporal variation on pitch perception. The goal of the
   present research was to reconcile the aforementioned research and explore the factors that affect pitch-time
   integration.
   Methods
   In Experiment 1, musicians heard a standard tone followed by a tonal context and then a comparison tone. The
   participants then performed one of two tasks. In the cognitive task, they indicated whether the comparison tone was in
   the key of the context. In the perceptual task, they judged whether the comparison tone was higher or lower than the
   standard tone. For both tasks, the comparison tone occurred early, on time, or late with respect to temporal
   expectancies established by the context. Experiment 2 used the perceptual task, and varied the pitch structure by
   employing either a tonal or an atonal context. Experiment 3 replicated the results of Experiment 2, controlled potential
   confounds and added a very early and very late comparison tone timing.
   Results
   In Experiment 1, temporal variation did not affect accuracy for either the cognitive or perceptual task. In Experiment 2,
   temporal variation did not affect accuracy for tonal contexts, but did for atonal contexts. The findings of Experiment 3
   replicated those of Experiment 2 and also showed evidence of the metric hierarchy (Palmer & Krumhansl, 1990)
   influencing accuracy in atonal contexts.
   Conclusion
   The presence versus absence of a tonal framework, and not the task type (cognitive or perceptual), drove the
   appearance of pitch-time interactions in the form of temporal expectancies. We argue that tonal contexts bias attention
   toward pitch and eliminate effects of temporal variation, whereas atonal contexts do not, thus fostering pitch-time
   interactions.
   Research Implications
   The present study supports the notion of an inherent focus on pitch variation in Western musical contexts. Previous
   work (Prince et al., 2008) demonstrated asymmetric pitch influences on temporal judgments in typical musical
   contexts; the present study demonstrated that systematic manipulation of the tonality of the stimuli affected the role of

                                                            53
    temporal expectancies on pitch judgments. The pitch structure of typical Western musical contexts appears to
    involuntarily invoke greater attention to pitch than time, obscuring otherwise observable interactions between pitch and
    time.
    Acknowledgment of Research Funding
    Grants from the Natural Sciences and Engineering Council of Canada to W.F.T. and M.A.S. supported this research.


Session 8B                 Evolution of Music                     (Chair: David Huron)
8B) 1:30 – 1:50

Experimental evidence for synchronization to a musical beat in a nonhuman animal
                      1                   1                       2                   3
Aniruddh D. Patel , John R. Iversen , Micah R. Bregman , and Irena Schulz
                            1                               2                                  3
The Neurosciences Institute , Univ. of California, San Diego , Bird Lovers Only Rescue Service

    Purpose
    Synchronization to a musical beat is a universal of human music, but is not commonly observed in other species.
    Does this ability reflect a brain specialization for music, or is it an outgrowth of some non-musical brain function? This
    question is relevant to debates over the evolutionary status of music, and can be addressed by asking whether other
    species are capable of musical beat perception and synchronization (BPS). According to the ―vocal learning and
    rhythmic synchronization hypothesis‖ (Patel, 2006), BPS builds on the neural circuitry for complex vocal learning,
    because vocal learning requires a tight auditory-motor interface in the nervous system. This hypothesis predicts that
    only vocal-learning species (such as humans, parrots, and dolphins, but not nonhuman primates) are capable of BPS.
    Methods
    We studied a sulphur-crested cockatoo (Cacatua galerita eleonora) named ―Snowball‖ who moves rhythmically in
    response to human music. A ~78 second excerpt of a song was computer-manipulated to create versions at 11
    different tempi (original, +/- 2.5%, 5%, 10%, 15%, and 20%), without changing the pitch. These versions were
    presented multiple times across several sessions, and movements were videotaped for analysis. During taping, we
    ensured that no humans were dancing (in order to rule out imitation of human movement). Analysis focused on the
    timing of head bobs in the vertical plane. These head bob times were compared to musical beat times in order to
    determine if there were periods of sustained synchrony to the beat (―synchronized bouts‖).
    Results
    Snowball showed synchronized bouts at 9 of the 11 different tempi. In trials with synchronized bouts, the bouts
    accounted for ~25% of the head bobs, meaning that Snowball was not able to maintain synchrony for long periods of
    time (i.e., there were long stretches where his dance tempo did not match the musical tempo). Even though Snowball
    showed sporadic synchronization, a Monte Carlo test indicated that his degree of synchrony was unlikely to have
    occurred by chance (p = .002).
    Conclusion
    A nonhuman animal is capable of genuine synchronization to a musical beat, though his abilities are not comparable
    to a human adult, who can sustain synchrony to music for long stretches of time (Snowball‘s ability may be more
    comparable to a human child).
    Research Implications
    Complex vocal learning may provide the neural and evolutionary foundations for synchronization to a musical beat.
    Further comparative work with other species can help test this hypothesis.
    Acknowledgement of research funding
    Supported by Neurosciences Research Foundation.


8B) 1:50 – 2:10

Statistical regularities of human melodies reflect perceptual-motor constraints: evidence from
comparisons with bird song
                  1                   2                               3
Adam T. Tierney , Frank A. Russo , and Aniruddh D. Patel
                             1                    2                              3
Univ. of California San Diego , Ryerson University , The Neurosciences Institute

    Purpose
    Researchers have discovered statistical regularities in a wide range of vocal music, including 1) lengthening of the last
    note of a phrase, 2) arch-like melodic contours, and 3) post-skip reversals: the tendency for jumps in pitch ("skips") to
    be followed by a change in direction. These patterns may be due to basic perceptual and motor constraints on the
    vocal production of pitch sequences. To test this hypothesis we examined whether these patterns also occur in bird


                                                             54
    song. We also examined whether these patterns occur in the pitch patterns of speech intonation and in instrumental
    musical themes.
    Methods
    We encoded 56 bird songs (from a wide range of bird species), 81 sentences, and 718 musical themes as sequences
    of discrete pitches. For bird song, each note‘s mean pitch was computed. For speech, intonation patterns were
    converted to pitch sequences using the prosogram model of intonation perception. For music, pitches were encoded
    relative to A440.
    Results
     Final lengthening and post-skip reversals were found in bird song, speech, and instrumental music. Arch-shaped
    contours were found in speech and instrumental music, but not bird song (where pitch contours were generally
    descending). However, we found that the individual notes of bird songs tended to have an arch-shaped contour.
    Unlike singing humans, singing birds tend to breathe between the individual notes of a melody, suggesting that the
    arch shape may be a motor consequence of breath patterns, which occur at the phrase level in human song but at the
    note level in bird song.
    Conclusion
    Some widespread features of human vocal melody also occur in bird song and speech intonation patterns, and hence
    may be due to basic perceptual and motor constraints on pitch sequence production. For example, final lengthening
    may reflect a motor tendency to slow down before stopping at a phrase boundary. Post-skip reversals may reflect the
    fact that pitch ranges are bounded, and skips that approach the edge of the range must be followed by reversals (as
    suggested by Von Hippel & Huron [2000]). Arch-shaped contours may reflect the breath cycle during song. Final
    lengthening and post-skip reversals in instrumental music may also have a perceptual-motor origin, but the arch
    shaped contours may be a case of instrumental music reflecting vocal music.
    Research Implications
    This research suggests that comparisons of human and avian singing can help uncover the bases for certain
    widespread features of melody.
    Acknowledgement of research funding
    Supported by Neurosciences Research Foundation.


8B) 2:10 – 2:30

Rhythmic structure in humpback whale (Megaptera novaeangliae) songs: Preliminary implications for
song production and perception
                  1                2                      2
Stephen Handel , Sean K. Todd , and Ann M. Zoidis
                       1                         2                              3
University of Tennessee , College of the Atlantic , Cetos Research Organization

    Purpose
    Roughly 40 years ago, Payne and McVay (1971) described the hierarchical repetitive ―songs‖ produced by male
    humpback whales (Megaptera novaeangliae) on their mating grounds. The basic sounds are grouped into phrases,
    the phrases are grouped into themes and the themes are organized into a repetitious pattern that recycles in rigid
    order. One complete cycle has been termed a song and the song may be repeated several times in a session.
    However, it is unknown how singers keep these intricate songs intact over multiple repetitions or how they learn
    variations that occur sequentially within each mating season. Here we investigated whether the rhythmic structure
    defined by the durations of individual sounds and the duration of the silent intervals among the sounds can expose
    some production constraints, and/or reveal how the whale organizes the sounds into phrases and themes.
    Method
    The humpback whale songs analyzed here were recorded in 1994 and 2001 off the island of Kauai, Hawaii by Cetos
    Research Organization. Each recording was collected from a different individual whale. The program Praat was used
    to construct spectrograms and determine the start- and end-time for every sound. The sounds were classified aurally
    and visually.
    Results
    In all songs, nearly every sound occurs in more than one phrase. The role of rhythm in structuring the song into
    phrases was determined by analysis of the timing of the same sounds in different phrases. In every song, the silent
    interval between two sounds changed as a function of the other sounds within the phrase, although the duration of the
    sounds did not change consistently. In a phrase, two or three sounds are heard together as a rhythmic unit, separated
    from other sounds that are heard individually. There was no evidence that the silent intervals at the transition between
    themes were any longer than the intervals within a phrase.
    Conclusions
    These results demonstrate that the sounds are not produced independently; the following sound influences the
    temporal properties of the present sound. The rhythmic timing is local. Individual sounds are grouped into rhythmic
    units that make the production and perception of the lengthy complex songs tractable by yielding a set of simple units
    that, although arranged in rigid order, can be repeated multiple times to generate the entire song.

                                                              55
   Research and/or Clinical Implications
   The rhythmic structure, an invariant cue in the ocean environment, may identify pod membership, provide a way to
   identify changes in the mating song, and yield a measure of reproductive fitness.



Poster Session II
3:00 – 5:00

22. Mental rotation in visual and musical space: Comparing pattern recognition in different modalities
    Marina Korsakova-Kreyn and W. Jay Dowling
    University of Texas, Dallas

   It is plausible that spatial abilities—such as mental rotation of shapes—are related to certain kinds of musical
   abilities—such as the mental manipulation of musical ―shapes‖ in the virtual space of pitch and time. Cupchik, Phillips,
   and Hill (2001) had found such a correlation between spatial mental rotation and recognition of inverted and
   retrograde transformations of melodies. Here we replicated their results, including less extreme melodic
   transformations than the mirror images they used, obtaining a correlation of r = .37 between the spatial and musical
   tasks. Participants performed 122 trials of a Shepard-type task, judging the congruency of rotations of three-
   dimensional objects. In this music perception task, the participants judged the congruency of 27 melodic contours in
   their standard and transformed versions, namely, tonal answer, inversion, and an incomplete inversion. The musical
   transformations were collected from keyboard compositions by J. S. Bach. Unlike Cupchik et al., we included a non-
   spatial musical control task involving timbre judgments, which correlated almost as strongly with mental rotation (r =
   .33). Hence we doubt that the correlation between mental rotation and melodic transformation tasks should be
   attributed to underlying spatial abilities common to both domains, but rather to more general processing abilities.


23. Music videos: Effects of visual information on music perception and remembering
    Marilyn Boltz
    Haverford College

   Purpose
   Although a substantial amount of research has revealed ways in which music can influence the film-viewing
   experience, relatively few studies have examined the reverse relationship. The purpose of the present research was
   to investigate whether the affect and structure of visual information influence the way a tune is heard (Experiment 1)
   and remembered (Experiment 2).
   Methods
   In both studies, listeners were presented with a set of five ambiguous tunes (i.e. with a neutral affect and moderate
   activity level) paired with four visual displays that varied in their affect (positive vs. negative) and format (video of
   scenes that smoothly transitioned from one to another vs. a montage (i.e. slideshow) of still images), or as a control
   condition, no visual information at all. Immediately after each, participants rated different characteristics of the melody
   (i.e. affect, activity, tempo, rhythm, flow, tonality, harmony, and loudness) on a set of 7 pt. scales. In Experiment 2, S‘s
   viewed and listened to the 5 stimuli and immediately afterwards, were given a surprise recognition memory task. On a
   given trial, they were required to discriminate an old melody from a new version that had been systematically
   transformed along one of three structural dimensions. In particular, each tune was increased or decreased by 15% in
   either tempo, pitch, or both together to reflect a more positive or negative mood, respectively. The main measure of
   interest was the false alarm data (―old‖/new) which is predicted to reflect biases from visual affect.
   Results
   In Experiment 1, the most important finding was a 3-way interaction between the different musical dimensions, visual
   affect, and format F(7, 295) = 10.32, p < .001. In general, visual affect influenced music perception in a mood-
   congruent fashion. Melodies pre-rated as neutral in overall affect were judged to have a positive affect in the
   presence of positive scenes but a negative affect in the presence of negative scenes. More interestingly, the
   perceived acoustical qualities were influenced in a corresponding manner. In the presence of positive (negative)
   scenes, tunes were perceived as louder (softer), faster (slower), more (less) rhythmic, and more (less) active than
   these same tunes presented in the absence of visual information. Although the effect of visual affect applied to all
   audiovisual pairs, visual format also had an impact on perceptual ratings. In contrast to the videos which were judged
   smoother and more passive, visual montages were rated ―choppier‖ and more staccato in nature.
   In Experiment 2, there was a significant interaction between visual affect and distracter melody affect, F(2, 57) =
   11.72, p < .001. Within each transformation condition, mood congruency effects were once again observed in that
   relative to the control melodies, false alarms for positive (vs. negative) distracters were higher in the positive visual
   affect condition, while the reverse was observed in the negative condition. Hence, memory was distorted by visual
   information, an effect that generalized across both the video and montage formats.

                                                             56
   Conclusions and Implications
   The present set of results parallel the demonstrated effects of musical soundtracks on the cognitive processing of film.
   In both cases the mood of film or music provides an interpretative framework that then biases the nature of perception
   and memory. Consistent with the Congruence-Associationist Model (Cohen, 1999), one can argue that the effects of
   visual affect and format on music cognition reflect a type of stimulus overgeneralization effect in order to achieve
   structural congruity. For format, melodies were perceived to have the same qualities as an accompanying montage or
   video, namely, a rapid, rhythmic, and staccato flow of information vs. a slower and smoother presentation of material,
   respectively. Visual affect exerted a similar influence: the perception and remembering of certain acoustical qualities
   were altered to reflect the mood of the visual display. As this relatively unexplored area of research continues to
   develop, its finding may better illuminate the mediational mechanisms of cross-modal perception, and better inform
   performing artists how to construct a music video for its desired effects.


24. Effects of background music and built-in audio on performance in a role-playing video game
                1          1                          2
    Siu-Lan Tan , John Baxa , and Matthew P. Spackman
                       1                           2
    Kalamazoo College and Brigham Young University

   Purpose
   Few studies have examined the role of sound in video games (Hébert et al., 2005; Tafalla, 2007). Further, as some
   studies have used built-in sound provided by the game and others have supplied an external soundtrack, it is difficult
   to synthesize findings. Our aims were a) to examine the effects of various sound conditions on performance in a role-
   playing video game, and b) to determine whether sound must be contingent on player‘s actions, or whether unrelated
   background music can also enhance performance.
   Methods
   Twenty-one undergraduates played ‗Twilight Princess‘ on a Wii console in individual sessions on four consecutive
   days, in the following conditions (randomized between participants): Full Sound (Screen and Wiimote), Partial-Sound
   (Wiimote only), No Sound, and Non-Contingent Music (unrelated background music). Performance measures included
   number of tasks completed, amount of time to complete tasks, and use of ‗continues‘ (which extends play when the
   gamer has run out of power).
   Results
   A series of one-way repeated measures ANOVAs revealed no significant effects for sound condition on most
   performance measures. However, significant differences were found for the effect of sound condition on the use of
   ‗continues,‘ F (3, 57) = 3.34, p < .026. Most ‗continues‘ were requested in the no-sound condition (M = 2.6, SD = 2.21),
   followed by partial-sound (M = 2.1, SD = 1.41), full-sound (M = 1.45, SD = 1.67), and non-contingent music (M =1.0,
   SD = 1.52) conditions. Follow-up repeated measures ANOVAs based on the performance scores participants had
   earned up to the first ‗continue‘ also revealed a main effect of sound condition on the number of completed tasks, F (3,
   57) = 3.166, p < .05, and length of play, F (3, 57) = 3.696, p < .05. Specifically, participants completed more clusters
   (M = 2.6) and played longest (M = 33.3 minutes) in the non-contingent music condition, followed by full-sound
   condition (M = 2.05 clusters, M = 30.3 minutes). The lowest scores were obtained in the partial-sound (M = 1.64
   clusters, 19.41 minutes) and no sound (M = 1.36 clusters, 20.5 minutes) conditions.
   Conclusion and Research Implications
   Participants‘ use of ‗continues‘ and performance scores before the use of the first ‗continue‘ varied with sound
   condition, with background music and full-sound leading to higher scores than partial-sound and no-sound. Full sound
   (i.e., contingent audio cues from screen and remote) seemed to facilitate top-scoring players‘ performance. However,
   the highest performance scores on most measures were yielded in the non-contingent music condition, regardless of
   proficiency. While the best players may successfully integrate audio and visual cues during play, background music
   may enhance performance through more general mechanisms (e.g., modulation of mood or arousal), and may
   therefore be demonstrated across proficiency levels. Our study reconciles mixed findings in this area, contributes to
   research on positive effects of background music on gamers‘ performance (e.g., Cassidy & MacDonald, 2008), and
   links more generally to the literature on the facilitative effects of music.
   Acknowledgment of Research Funding
   Funding provided by the Psychology Departments at Brigham Young University and Kalamazoo College.


25. Evaluating the impact of music video games on musical skill development
    Patrick Richardson and Youngmoo Kim
    Drexel University

   Purpose:
   This research project seeks to track and quantify any effect by which routine exposure and play with popular music
   and rhythm-based video games may inspire or bolster the development of music-specific skills (tonal/rhythm memory,
   aural skills, sight-reading/singing, etc). This one-year longitudinal research study provides an evaluation of the impact
   of musical video games that specifically investigates the following questions:

                                                            57
   1. Does game proficiency have a positive impact on musical skill development?
   2. Does avid game play lead to the pursuit of other music making outlets?
   3. Does interest and regular participation in the playing of music video games affect whether a student seeks
   additional formal music education?
   Methods:
   For this longitudinal study, both control and experimental groups complete a computer-based battery of musical skills
   and -sensitivity tests. Each of 20 subjects in the experimental group attends 6 months of weekly after-school game
   play sessions. All 15 Control group participants abstain from play of these games for the study‘s duration. After the
   duration, both groups repeat the full music test battery. Games score and test score are evaluated for learning effects
   and training advantages.
   Results:
   As of 10/1/08, this study is under active funding, following the approved research calendar. As of 1/1/09, the research
   team is has established the web-based test battery and targeted recruitment populations. We plan to end the gaming
   sessions in late May of 2009, and have experimental results available within June of 2009.
   Conclusion:
   This project‘s Results, Discussion, and Conclusion will be available for presentation at the June 2009 SMPC
   conference.
   Research and/or Educational/Clinical Implications:
   In recent years, a variety of video games have been developed based on the premise of simulated music play. These
   games have become extremely popular, and are indicative of a strong demand within society for some form of musical
   experience and expression. At the same time, particularly in at-risk communities, traditional music education has been
   a lesser priority within the public school curriculum. For students within school districts that provide little or no funding
   for fine arts programs, music video games may represent the foremost form of musical interaction to which these
   students are exposed. Consequently, it is crucial to understand the impact and the potential for these music games to
   serve as a tool for learning musical skills and stimulating music appreciation.
   Acknowledgement of Research Funding:
   We would like to thank the NAMM Foundation for their support.


26. Segmentation of music and film and the nature of their interaction and integration
                    1                     2               1
    Cindy Hamon-Hill , Annabel J. Cohen , and Ray Klein
                        1                                     2
    Dalhousie University , University of Prince Edward Island

   Purpose
   In a previous study, Hamon-Hill and Barresi (2008) showed that social and emotional interpretations of an animated
   visual stimulus varied with different accompanying music. Viewers were exposed to simple line-drawn animated
   objects (Heider & Simmel, 1944) without music or with one of three musical excerpts and rated each of the objects on
   a social-characteristics scale. Ratings of characteristics, such as ―lonely‖ or ―threatening‖, differed between objects
   and the intensity of ratings varied under different music backgrounds. To explain how the music influences the
   meanings assigned to the visual objects, we are focusing on the parsing or segmentation of the visual and musical
   stimuli when processed individually and together. Segmentation studies reveal how people organize and parse stimuli
   from different modalities into meaningful events or units of information (Zacks, 2004). Krumhansl and Schenck (1997)
   revealed similar patterns of segmentation for music and dance, each contributing additively to the experience of
   tension and expression of emotion. An investigation of visual and auditory information representing clarinet
   performance revealed that information from each modality was processed structurally similarly yet the visual
   information could both augment and dampen the emotional interpretation of tension in the performance (Vines,
   Krumhansl, Wanderley & Levitin, 2006).
   Methods
    In the current study, participants segment 1) the Heider and Simmel animation, 2) a music excerpt of the same
   duration as the animation, and 3) the animation and music combined, by marking breakpoints between meaningful
   units of information (Newtson, 1977). Frequency and temporal patterns of breakpoints are collected from each
   participant for all three experimental conditions.
   Results
   The data will show the segmentation patterns for the visual and musical information when experienced independently,
   and how their joint encoding modifies the pattern. A primary question of interest is the relative strength of the
   segmentation from the two modalities, as the results of Krumhansl and Schenck (1997) for dance showed less visual
   dominance than did the results of Vines et al. (2006) for music performance.
   Conclusion & Research Implications.
   To the best of our knowledge, this is the first study of parsing of music and film animation and their interaction. The
   study should further our understanding of how music and film interact at a structural level so as to influence
   interpretations when making cognitive and emotional judgments. As such the research will help to inform the lower
   (sensory analysis) level of Cohen‘s (2008) Congruence-Association Model of the role of music in media which
   proposes a matching process between sensory-based analysis at a low level and generation of semantic

                                                              58
   interpretations at a higher level. Such knowledge and theory may ultimately aid the film industry or educational
   multimedia sector in having greater control over the impact of music scores on audiences.
   Acknowledgement of Research Funding
   Social Sciences and Humanities Research Council awarded to A. J. Cohen.


27. An experimental investigation into the effects of stereo versus surround sound presentation in the
    cinematic and music listening experiences
               1                        2
    Mark Kerins and Scott D. Lipscomb
                                 1                          2
    Southern Methodist University , University of Minnesota

   Purpose
   5.1-channel surround sound presentation is ubiquitous in theatrical releases and DVDs; most television programs and
   video games also utilize soundtracks mixed for this sonic environment. This broad deployment demonstrates the
   media industries‘ implicit assumption that changing sound presentation mode significantly alters the experience of
   audience members. There has been, however, little experimental investigation into this relationship. This study is part
   of an ongoing series of experiments designed to fill this void.
   Methods
   In previous studies (Kerins & Lipscomb, 2003; Lipscomb & Kerins, 2004), the present authors studied the influence of
   the sound systems used in motion picture exhibition (monophonic, 4-channel surround, and 5.1-channel surround) and
   music listening (stereo and 5.1 surround) on audience judgments of the films/songs themselves. In the present study,
   the authors reexamined the effect of presentation mode using a method similar to, but with important differences from,
   that of the previous studies.Seventy participants rated each excerpt on a series of 11 verbal scales, including the
   scales employed in the previous studies and additional scales designed to highlight the ―immersion‖ factor of the
   experience. Most importantly, presentation mode was incorporated into the experimental design as a within-subjects
   variable within a block-design paradigm so that all participants were exposed to excerpts representing two
   presentation modes (stereo and 5.1 surround).
   Results
   In a preliminary analysis of this data (Kerins & Lipscomb, 2007), responses to each verbal scale were subjected to
   analyses of variance, consisting of two between-subjects variables (musical training and visual training) and two
   within-subjects variables (cinematic/musical genre and presentation mode). After much consideration of the results,
   alternative means of statistical analysis have been pursued to clarify the outcomes observed, revealing important
   findings that were not apparent in the initial interpretation. Details concerning statistically significant differences that
   emerged will be provided during the presentation.
   Conclusion
   The present study reveals that 5.1-channel surround sound‘s effects are not the same for every genre or media form,
   and suggests specific elements whose influence on the cinematic or music listening experience with respect to sound
   presentation mode warrant further study.
   Research and/or Educational/Clinical Implications
   Relationships between audience responses and sound presentation mode emerge for some genres of music/cinema
   but not for others, suggesting the significance of stereo versus 5.1-channel presentation varies. Further research
   based on the present results and designed to isolate the specific elements that determine whether the 5.1-channel
   experience differs significantly from the stereo experience is suggested.


28. Intersensory attention
    Matt Rosenthal and Erin Hannon
    University of Nevada, Las Vegas

   Purpose:
   This study investigates the applicability of an internal oscillator model of attending to multimodal rhythms.
   Method:
   Participants were presented with either an auditory or visual context rhythm. A cross was then briefly presented that
   either conformed to or violated the temporal expectation created by the context rhythm and was oriented to the left,
   right, or center. Participants were instructed to indicate whether the orientation of the cross changed from an initial
   viewing at the beginning of the trial. Accuracy in this task was the dependent variable.
   Results:
   We expect participants exposed to auditory context rhythm will be more accurate in the visual attention task at
   expected locations than participants exposed to the visual context rhythm and that both conditions will be more
   accurate than a baseline condition of no rhythmic context. If this hypothesis is supported, this would support the
   internal oscillation model for attending to external rhythms but with the constraint that rhythms in the visual modality do
   not entrain as readily as auditory rhythms. If accuracy scores for both rhythmic context conditions are not different
   from each other but higher than the baseline condition, this would suggest that visual context rhythms are just as
                                                             59
    effective as auditory context rhythms at inducing temporal expectancies or that there is a cost of switching attention
    between the auditory and visual modalities even when the rhythmic pattern of the context is preserved. In the case
    where one or both accuracy scores from the rhythmic context conditions are not different than baseline, this would
    suggest that auditory rhythm patterns do not create temporal expectancies for visual events; visual rhythms do not
    create temporal expectancies for visual events or both.
    Research Implications: The internal oscillation model may provide the mechanism for attending to external rhythms.
    Although this model predicts oscillatory entrainment to visual rhythms, the visual modality‘s shortcoming in temporal
    processing suggests attenuation of entrainment to visual rhythms relative to auditory rhythms. If oscillatory
    entrainment is the mechanism by which attending and the creation of temporal expectancies occur, then the temporal
    expectancies created by an auditory rhythm should modulate accuracy in a visual task based on the degree to which
    the timing of the visual task conforms to the temporal expectancy created by the auditory rhythm. This effect should
    be reduced when the temporal expectancy is created by a visual rhythm. This study should make clear the extent to
    which the internal oscillator model applies to multimodal events.


29. Aural versus visual cues in communicating tactus
                 1               2                   2
    Peter Martens , Stefan Tomic , and Petr Janata
                          1                                 2
    Texas Tech University , University of California, Davis

    Purpose
    The purpose of this study was to investigate whether or not performers‘ intended metrical interpretation of a piece of
    music can be communicated to an audience in the form of a main beat or tactus, and to what extent this
    communication depends independently on a performance‘s aural and visual components.
    Methods
    A student string quartet was coached to perform a set of seven musical excerpts twice, keeping the same tempo in
    each performance but feeling and attempting to express a different main beat (tactus) in each. These two beat levels
    were adjacent in the metric hierarchy, and related by either a duple or triple ratio. The initial group of study
    participants viewed these videorecorded performances and were asked to tap their dominant hand along with the main
    beat of the music. A second group completed the same tapping task in response to either audio-only or video-only
    versions of the same performances. Finally, the audio and video of these performances were analyzed separately
    using the meter-finding computer model of Janata & Tomic (2008) and ImageJ video analysis software, respectively.
    The three human response conditions and the two computer analysis conditions were compared in a 3x2 matrix.
    Results
    Overall, the quartet‘s intended tactus significantly influenced listeners‘ choice of tactus under the A/V condition, but
    much less so under the audio- or video-only conditions.
    In some excerpts, the computer analysis matched the human responses closely; in others, the computer ―heard‖ or
    ―saw‖ differences in the quartet‘s intention that human listeners seemed not to notice, and vice versa.
    Conclusions
    Communicating a specific metrical interpretation of music to an audience is a multi-sensory act. The quartet‘s success
    with its intended tactus was significantly greater when video was present, but ―objective‖ visual and aural cues
    uncovered by computer analysis were not relevant to human listeners in all cases.
    Educational/research implications
    Performers in training should be made more aware of their physical presence onstage, and should practice and be
    evaluated on whether or not the gestural components of their performances contribute to their desired effect. In order
    to model meter perception more ecologically, tapping studies should involve performance visuals.
    Acknowledgement of research funding
    Metanexus Institute grant (UC-Davis), Texas Tech University Growing Graduate Programs grant (Texas Tech
    University Music Research Lab)


30. A comparison of auditory and visual perception via a novel auditory search task
             1                 1                     1                1                     2
    Kat Agres , Spencer Topel , Stephen Moseson , Sarah Victoria Brown and Michael J. Spivey
                      1                                   2
    Cornell University , University of California, Merced

    Purpose
    Visual search tasks have elucidated fundamental properties of visual perception, such as attention and efficiency of
    processing. Until this time, few studies have utilized the search task paradigm to test whether auditory perception
    shares certain phenomena (e.g. the ―pop-out effect‖) in common with visual perception. The present experiment
    sought to address the following question: Do the auditory and visual systems exhibit similar constraints of attention on
    perception?
    Methods
    A new ambisonics-based technology was developed to assess this question. This interactive technology, which
    features a headphone-mounted magnetometer, enabled listeners to actively direct and influence their auditory

                                                             60
   environment. In the experimental task, listeners were given a target tone at the beginning of every trial. The target
   tone consisted of a particular timbre (the instrument playing) and pitch. After hearing the target, either two, three, or
   four tones were presented in distinct auditory locations within the front hemisphere of space. The listeners‘ task was
   to locate the target among distractors that could feature either the same pitch or timbre as the target. Analogous to
   some visual search tasks, the search type could either be simple or complex. In simple searches, the distractor(s) did
   not have either feature (timbre or pitch) in common with the target. In complex searches, one of the distractors
   exhibited either the same timbre or pitch as the target.
   Results
   The results provide evidence that listeners do require more time for complex searches than for simple searches. A 2
   X 3 ANOVA of Search Type X Set Size (the number of sounds per trial) yielded a significant main effect of Search
   Type (p < .05), with complex search eliciting longer reaction times than simple search, and a significant main effect of
   Set Size (p < .01), with reaction times increasing with the number of distractors. The interaction of Search Type and
   Set Size was not significant. In separate analyses, simple search exhibited only a mild (marginally significant) effect of
   Set Size on reaction time (p < .1), whereas complex search showed a robust and highly significant effect of Set Size (p
   < .01).
   Conclusion
   These data provide evidence that simultaneous musical tones compete with one another for attentional resources as a
   function of their degree of perceptual similarity. The results thereby support the stance that attention in the auditory
   domain is akin to visual processing, because attention acts as a major constraint to efficient processing.
   Research Implications
   This research approach provides a novel way to test seminal work from visual perception in the auditory domain.
   More research is needed to investigate the degree to which attention within the auditory domain shares common
   mechanisms to attention within the visual domain.
   Acknowledgement of Research Funding
   Supported by NSF grant BCS-0721297 to MJS


31. Musical familiarity and early social preferences
    Gaye Soley and Elizabeth Spelke
    Harvard University

   Purpose:
   Native structures play a significant role in guiding attention (Moon et al., 1993; Kelly et al., 2005) and social
   preferences (Kinzler et al., 2007) early in life. Music is a ubiquitous, culture-specific stimulus; yet the role of shared
   culture-specific musical experiences in cementing social bonds early in life are largely unexplored. We investigated
   whether preferences for familiar music guide children‘s social preferences.
   Methods:
   Seventy-two children in Boston participated in three experiments (mean age= 56.8 months, SD=6 months). Subjects
   were shown photographs of two 5-year-old children. Photographs were matched based on adult ratings on
   attractiveness and friendliness. After hearing two songs identified as ―the photographed child‘s favorite song‖,
   participants were asked, whom they would like to be friends with. The order and lateral positions of photographs, and
   photograph-song pairings were counterbalanced. In Experiment 1, half of the songs were popular American children‘s
   songs (e.g., Mary Had a Little Lamb), and the other half consisted of unfamiliar Balkan folk songs. In Experiment 2,
   half of the songs were unfamiliar American folk songs from the 18th century, with similar rhythmic and melodic
   structures as the familiar songs from the first experiment and the other half were unfamiliar Balkan folk songs. In
   Experiment 3, musical excerpts were consonant or dissonant versions of unfamiliar American folk songs. All songs
   were created as MIDI files using the same instruments and were matched for length and tempo.
   Results:
   Two-tailed one-sample-t-tests revealed that subjects tended to choose children whose favorite songs were a familiar
   children‘s songs (t(23) = 3.102, p <.01) (Experiment 1); children did not show a preference when songs were familiar
   at a culture-specific level (t(23) = .569, p>.5) (Experiment 2); and subjects chose children who were associated with
   the consonant songs (t(23) = 32.498, p <.05) (Experiment 3).
   Conclusions:
   These results suggest that musical preferences can serve as a social cue among children. These preferences seem to
   be modulated by familiarity and universal rules governing music. Yet, when the songs exhibit a more abstract level of
   familiarity by conforming to a culture-specific style of music, children do not appear to transfer this information to the
   social domain, despite their sensitivity, since infancy, to such information (Soley & Hannon, under revision).
   Research and/or Educational/Clinical Implication:
   These studies may begin to shed light on the origins of music‘s social functions and of the role music plays in forming
   one‘s ethnic and social identity.
   Acknowledgement of Research Funding:
   This research was funded by a grant from NIH to E.S.S.



                                                            61
32. Auditory and visual attention networks in musicians and non-musicians
    Kathleen A. Corrigall and Laurel J. Trainor
    McMaster University

   Purpose:
   Attention research is largely focused on the visual modality. For example, the Attention Network Test (ANT; Fan et al.,
   2002) is a visual reaction time/accuracy task that measures three different aspects of attention concurrently. Our first
   goal was to create an auditory ANT to test whether attentional networks differ across modality. Our second goal was
   to test whether adult and child musicians differ from non-musicians on each aspect of attention, and whether such
   differences are modality-specific.
   Methods:
   In the original ANT, each trial consists of a fixation point, a visual-spatial cue, and then a target. The ANT measures
   reaction time and accuracy to the target and can assess three attention networks: alerting, orienting, and executive
   control. Alerting, or maintaining an alert state, occurs when the cue prepares participants for the target, but does not
   predict where or when that target will occur. Orienting, or shifting attention, occurs when the cue does predict where or
   when the target will occur. Executive control involves selectively attending to one aspect of a target (central arrow)
   while ignoring conflicting information (flanking arrows). In our auditory ANT, (1) all stimuli are auditory, (2) the task is to
   judge whether the word "day", "loud" or "soft" is said loudly or softly, (3) alerting is measured by comparing
   performance between trials with and without a warning sound, (4) orienting is measured by comparing performance
   between trials where the target occurs at an expected or unexpected time following a cue, and (5) executive control is
   measured by comparing performance on trials where the semantics ("loud", "soft") and physical features (spoken
   loudly or softly) are congruent or incongruent.
   Results:
   Preliminary results with 8 participants on the auditory ANT revealed significant effects of alerting [t(7) = 3.12, p = .02],
   orienting [t(7) = 2.94, p = .02], and executive control [t(7) = 3.63, p = .01]. We are currently testing musicians and non-
   musicians on the visual and auditory ANT.
   Conclusion:
   Our results suggest that alerting, temporal orienting, and executive control can be measured in the auditory modality.
   We expect that musicians will exhibit attentional processing advantages, especially in the auditory modality.
   Research and/or Educational/Clinical Implications:
   Since temporal orienting involves attending to a particular point in time, our auditory ANT has implications for musical
   rhythm and meter processing. Furthermore, if musical training leads to attentional benefits, this research has
   significant educational and clinical implications.
   Acknowledgement of Research Funding:
   This research was funded by the Natural Sciences and Engineering Research Council of Canada.


33. A method for studying music practice: SYMP (Study Your Music Practice)
    Topher Logan, Alexander Demos and Roger Chaffin
    University of Connecticut

   Purpose
   Music practice provides a natural laboratory for studying the development of complex mental and motor skills.
   Musicians can provide insightful reports about their practice and memorization strategies. Musicians‘ starts, stops and
   hesitations also provide a detailed behavioral record of the complex problem solving involved in learning a new piece.
   Together, these different perspectives provide a rich source of information about learning, problem solving, and
   memorization strategies (Chaffin & Logan, 2006). Here we describe procedures for obtaining reports from musicians
   and a new software tool for summarizing recorded music practice. These easily adaptable methods provide musicians
   and psychologists with tools for studying effective practice strategies and the learning/memorization process.
   Method
   We describe a new tool we have created, SYMP (Study Your Music Practice) for relating music practice to musicians‘
   decisions about technique, interpretation, and performance. SYMP provides a format for recording both music practice
   and musicians‘ reports of their decisions during practice and their understanding of the music. Practice is first recorded
   on audio or videotape. The researcher then transcribes the location of starts and stops. Musicians provide reports
   about their decisions regarding technique and interpretation. They also report the locations that they try to attend to
   during performance—the performance cues they need to direct their attention in order for the performance to unfold as
   planned. The researcher enters the reports into the data base. SYMP provides graphical summaries of both practice
   and reports and the relationships between them. The program outputs data files for analysis of these relationships
   using standard statistical packages such as SPSS. SYMP is written in Microsoft Excel 2007 and its use requires a
   basic knowledge of this program.



                                                              62
   Results
   Examples will be provided of musicians‘ self-reports, of how the data are entered into the program, and of the
   graphical summaries that SYMP auto-generates. The graphs visually summarize the pattern of starts and stops in
   each practice session and show their relationship to the musical structure of the piece and to the musicians‘ decisions
   about technique, interpretation, and performance.
   Conclusion
   The described methods provide new ways for researchers and musicians alike to view the learning and memorization
   process.
   Research and/or Educational/Clinical Implications
   By helping musicians to study themselves, as well as other musicians, we can learn more about what musicians
   actually do when they practice. We expect that empirical study of practice will lead to strategies for making practice
   more effective and rewarding, and less frustrating and time consuming.


34. Impact of musical experience on measures of top-down auditory processing
    Dana L. Strait, Richard Ashley, Alexandra Parbery-Clark, and Nina Kraus
    Northwestern University

   Purpose
   Long-term musical experience is known to significantly modulate lower-level auditory function, although mechanisms
   by which this occurs remain uncertain. By reinforcing top-down mechanisms, music could selectively benefit
   perceptual abilities subject to cognitive control. In light of the extensive perceptual acuity and cognitive demands that
   musical activity requires, we investigated the impact of musical experience on auditory perceptual performance—
   especially for tasks linked to cognitive abilities like auditory attention and backward masking.
   Methods
   We administered a standard battery of perceptual and cognitive tests to I.Q.- and hearing-matched adult musicians
   (N=18) and non-musicians (N=15), including tasks more vs. less susceptible to cognitive control (e.g., backward vs.
   simultaneous masking). Measures were administered individually via an animated computer game developed by the
   Hearing Research Center of the Medical Research Council. Tasks addressed auditory attention, visual attention,
   frequency discrimination, frequency selectivity (simultaneous masking with and without notched filters), and temporal
   resolution (backward masking with and without a temporal gap between the target and masker).
   Results
   Overall, musicians performed a subset of tasks with greater proficiency than non-musicians. Enhanced musician
   performance was observed for frequency discrimination (FD), auditory attention and both backward masking
   measures (BM and BMgap). Musicians demonstrated lower thresholds than non-musicians for the frequency
   discrimination and backward masking measures (FD F=14.03, P<0.001; BM: F=9.52, P<0.005; BMgap: F=5.60,
   P<0.03) and faster reaction times to targets in the auditory attention paradigm (F=4.67, P<0.04). These
   enhancements in musicians were observed in the absence of between-group differences for the simultaneous
   masking and visual attention tasks.
   Conclusions
   Our data indicate that long-term musical practice specifically strengthens auditory-related perceptual abilities by
   bolstering cognitive mechanisms that, when impaired, relate to language and learning deficits. Thus, musical training
   may serve to lessen the impacts of a variety of cognitive deficits by strengthening the corticofugal system for hearing.


35. Advancing interdisciplinary research in singing through a short test battery: Progress update
    Marsha Lannan, Jenna D. Coady, Emily Gallant, and Annabel Cohen
    University of Prince Edward Island

   Purpose
   Although much research has been directed to the acquisition of language, little attention has been directed to the
   acquisition of singing. Yet both abilities develop universally early in life. Several researchers have conducted case
   studies of singing acquisition of their children (e.g., Dowling, 1984; Papoušek & Papoušek, 1981). Davidson,
   McKernon and Gardner (1981) studied groups of children in Boston, and Kreutzer (2001) studied Zimbabwean
   children. Stadler Elmer (2001) emphasized that children link singing with motor activity and play, de-emphasizing the
   significance of pitch accuracy. Together the studies are equivocal in regard to the early significance of scale
   segments, major triad, intervals, and tonality.
   Methods
   In consultation with Simone Dalla Bella and Stefanie Stadler Elmer, a test battery was developed having 11
   components. Components 1 and 11 engaged conversation to assess language unobtrusively. The remaining
   components assess (2) pitch range; (3) minor third call-back; (4) musical interval, triad, and scale; (5) singing the
   familiar melody ―Are you sleeping‖ in segments; (6) singing a favourite song; (7) improvising the ending to an unknown
   song; (8) composing a song to a picture prompt; (9) repeating an unknown song; and (10) singing from memory ―Are

                                                            63
    you sleeping‖. Experiment 1 tested 20 participants --two females and two males of ages 3, 5, and 7 years and young
    adults having no or considerable voice training-- at five monthly intervals resulting in 100 examples of each
    component. Analysis was directed to memory of ―Are you sleeping‖ (10) and free composition (8). Renditions of ―Are
    you sleeping‖ were analyzed with Stadler Elmer‘s (2001) pitch extraction technique. For free composition, the
    structure and content of the transcribed prose was analyzed. Experiment 2, in progress, refined the protocol and is
    being administered to young and older adults and persons with Alzheimer‘s disease.
    Results
    Systematic improvement in pitch accuracy was observed across the children of age 5 and 7 years, and across the
    young adults with and without training in singing. Representing the hierarchical structure of ―Are you sleeping‖ was
    apparent as early as five years from the pitch timing and frequency analysis. Composition skills were evident in
    young children and showed similarity across all age groups.
    Conclusion
    The test battery provides a wealth of information. The results encourage longitudinal studies over a longer course.
    The enjoyment of the session found for children is reflected in preliminary observations with senior participants,
    however, in contrast to the children and young adults, persons with Alzheimer‘s disease are challenged in regard to
    composing a new song.
    Educational and Clinical Implications
    The battery may assist in defining natural singing skills and help in the teaching of singing and the maintenance of
    musical creativity. It may help in defining preserved musical ability in Alzheimers Disease and retention of singing
    skills with normal aging. It may also benefit crosscultural studies and comparisons of music and language skills and
    interactions.
    Support
    The Research was supported by a Standard Research Grant from the University of Prince Edward Island and an
    MCRI Proposal Development Grant from the Social Sciences and Humanities Research Council of Canada. The
    project is part of the SSHRC Major Collaborative Research Initiative entitled Advancing Interdisciplinary Research in
    Singing (AIRS).


36. Modeling meter and key implication
                1              2
    Eric Nichols and Elton Joe
                      1                    2
    Indiana University , Hampshire College

    Purpose
    Music cognition researchers have previously considered both key determination and meter determination in the
    context of listening to a monophonic melody. Our research takes into account both problems at once, based on the
    intuition that in listening to the beginning of a melody, listeners quickly make judgments about both the key and meter
    of the melody. Specifically, we investigate the hypothesis that the initial interval in a melody has significant implications
    for both perceived meter and key, and that these two perceived musical features are not independent.
    Methods
    We recruited 14 music students and recorded their improvised, sung responses to two-note melodic fragments. For
    each of 24 initial intervals (both ascending and descending), each participant was instructed to sing a short response
    beginning with the given interval and continuing to a point of repose. We transcribed the resulting melodies and
    extracted the meter and tonal function of the initial notes.
    Results
    We studied the effect of the initial interval on the implied key, meter, and metric placement of the first notes, and noted
    that while there is a preference for duple meter in general (only 21% of responses are in triple meter), certain intervals
    such as an ascending half-step, ascending perfect fourth, or descending perfect fifth result in more triple-meter
    responses (32% triple for the ascending half step, 26% triple for the other two). Additionally, we identify several factors
    which complicate key and meter induction, including the direction and quality of initial intervals, the natural preference
    for duple meter, and a preference to hear the first note as a tonic.
    Conclusion
    The data suggest certain links between implied tonal functions and meter of an initial pair of notes; for example, we
    can infer the rule that if the second note sounds like a tonic, it also sounds more like a downbeat. Our results build on
    prior work such as Carlsen‘s (1981) expectation study, but the metric results are novel.
    Research Implications
    The computer model Musicat simulates the process of inferring metric structures, tonal functions, and motives while
    listening to a sequence of notes. Musicat already has mechanisms for incorporating multiple conflicting factors in
    perception such as those identified above; we suggest that our findings should be used to validate the results of the
    existing model when given two-note melodic beginnings as input; discrepancies will indicate opportunities for
    improving Musicat.
    Acknowledgement of Research Funding
    This work was supported by the Center for Research on Concepts and Cognition as well as NSF grant IIS-0738384.



                                                              64
37. Prevalence of congenital amusia
                       1                1                 2                        1
    Mélanie A. Provost , Isabelle Peretz , Benoit A. Bacon , and Nathalie Gosselin
                          1                      2
    University of Montreal , Bishop’s University

    Purpose:
    Congenital amusia is a disorder in musical pitch perception in the absence of more general deficits in audition,
    language and cognition (Peretz & Hyde, 2003). Its prevalence has been estimated at 4% (Kalmus & Fry, 1980) using
    the Distorted Tunes Test (DTT). However, because the DTT uses familiar melodies, congenitally amusic participants
    are a priori disadvantaged compared to normal subjects, since their knowledge of the familiar melodies might be
    limited. Furthermore, there is evidence of a ceiling effect in the results at the DTT. These limitations motivate a re-
    examination of the prevalence of congenital amusia using a better adapted test. Towards this aim, we used the
    Amusia on-line test (Peretz et al., 2008) as this test uses unfamiliar melodies, is sensitive and includes a control
    (rhythm) condition.
    Methods:
    Participants aged between 18 and 40, not preselected for their musical abilities, and for whom education level was
    controlled, were asked to fill out the online Amusia test, which includes three subtests. The «Scale» and «Out-of-Key»
    subtests measure the participant‘s ability to detect out-of-key notes, whereas the «Out-of-Time» test evaluates their
    ability to detect rhythmic deviations. The rhythmic test also serves as a control condition, since it detects whether the
    participant‘s difficulties are the result of a more generalized music problem or of another deficit (ex: ADHD). The
    criteria used to determine a problematic result is a score on the «Scale» subtest that falls two standard deviations
    below the mean (Peretz et al., 2003). Furthermore, amusic participants should show a distinct pattern of performance.
    That is, their performance on both the melodic subtests should be impaired, whereas their scores on the rhythmic
    component should be relatively normal, since congenital amusia is primarily a pitch deficit. The prevalence is
    determined by the proportion of participants that scores below this cutoff.
    Results and conclusion:
    To date, 977 participants have completed the online test, and a prevalence rate of 4.5% has been determined.
    Recruitment of participants is ongoing, and we are aiming for at least 1000 participants. A prevalence rate between
    4% and 5% can be expected.
    Research Implications:
    The re-evaluation of the prevalence of congenital amusia is crucial for the genetic study of this disorder. In fact,
    heritability estimates rely on the prevalence rate.
    Acknowledgement of funding:
    Isabelle Peretz: Canadian Institutes of Health Research.
    Mélanie Provost: Natural Sciences and Engineering Research Council of Canada.
    References:
    Peretz, I. & Hyde, K.L. (2003). What is specific to music processing? Insights from Congenital Amusia. Trends in
       Cognitive Sciences, 7(8). 362-367
    Kalmus, H. & Fry, D.B. (1980). On Tune Deafness (Dysmelodia): Frequency, Development, Genetics and Musical
       Background. Annals of Human Genetics, 43. 369-382
    Peretz, I.; Gosselin, N.; Tillman, B.; Cuddy, L.L.; Gagnon, B.; Trimmer, C.G.; Paquette, S. & Bouchard, B. (2008). On-
       Line Identification of Congenital Amusia. Music Perception, 25(4). 331-343


38. Subcortical correlates of consonance, dissonance, and musical pitch hierarchy in the human
    brainstem
    Gavin M. Bidelman and Ananthanarayan Krishnan
    Purdue University

    Purpose:
    Psychophysical data has revealed that listeners rank musical intervals along a hierarchical scale, judging some as
    more consonant (pleasant) than others (dissonant). The aim of this study was to determine if there are
    electrophysiological correlates of consonance and dissonance as reflected by the pre-attentive, subcortical processing
    in the human brainstem.
    Methods:
    Frequency-following responses (FFRs), reflecting sustained phase-locked neural activity in the brainstem, were
    recorded from 10 nonmusicians in response to the dichotic presentation of 9 musical intervals with varying degrees of
    consonance and dissonance. For each FFR, the autocorrelation function (ACF) was computed to understand the
    dominant periodicities present in the response. The relative strengths of all pitch related periodicities were then
    calculated by sliding a series of harmonic pitch sieves across the ACF and recording the output for each sieve
    template. This procedure essentially implements a time-domain analogue of the classic ―pattern recognition‖ model of
    pitch perception. The template with maximal output was taken as the heard pitch and its output value was taken as the
    neural pitch salience for that musical interval. A ―neural consonance ranking‖ was then constructed by comparing the

                                                             65
   pitch saliences across all intervals. In a perceptual task, individuals were asked to judge the same stimuli according to
   their degree of consonance (i.e. pleasantness) in order to obtain a behavioral consonance ranking for each musical
   interval. Consonance estimates based on the neural data were then compared to those obtained behaviorally to
   assess the relationship between the perceptual and neurophysiologic responses.
   Results:
   Neural responses to consonant intervals were generally more robust than those to dissonant intervals. Moreover,
   FFRs showed differential encoding such that the neural pitch salience across musical intervals followed the pitch
   hierarchy stipulated by Western music theory. Finally, neural pitch salience showed a high degree of correspondence
   to behavioral consonance judgments.
   Conclusion:
   We conclude that brainstem mechanisms underlying pitch encoding preserve information regarding the perceptual
   attributes of consonance and dissonance. Furthermore, our results suggest that innate, subcortical mechanisms
   subserving pitch are geared toward processing consonant musical relationships over dissonant ones. This may be one
   reason why such intervals are preferred behaviorally.
   Research Implications:
   Our results demonstrate that the extraction of pitch information relevant to music processing may begin as early as the
   brainstem. Moreover, the basic pitch relationships governing music are encoded even in individuals without music
   training.
   Acknowledgement of Research Funding:
   NIH/NIDCD predoctoral traineeship to G.B.


39. Quantitative viewpoints on orchestration
    Randolph Johnson
    The Ohio State University

   Purpose
   Orchestration treatises largely discuss the first-order properties of individual instruments (e.g. range, tone color, and
   dynamic characteristics); higher-order concerns are given little consideration (e.g. instrumental combinations and pitch
   distribution among ensemble units). Timbre perception research suggests that instrument family classification (e.g.
   woodwinds) does not necessarily reflect acoustic similarities between instruments or begin to describe how
   instruments are functionally combined (Grey, 1977). The present study supplements data concerning instruments‘
   acoustic similarities with analyses of instrumental behavior in actual musical works.
   Methods
   Chords from randomly selected nineteenth-century symphonies comprise the sample. Multidimensional scaling of
   instrument pair correlations yields a three-dimensional space that shows instruments‘ proximity according to their
   frequency of combination. Elucidation of the geometric model comes from a hierarchical clustering analysis that
   identifies three instrumental deployment groups: the groups can be interpreted as Standard, Power, and Color
   instruments (hereafter, the SPC model). Examples of these three groups include violin (Standard), tuba (Power), and
   harp (Color). The specific attributes of each of the three groups are uncovered via hypothesis testing through the lens
   of the SPC model.
   Results
   Five SPC model predictions are tested to examine its robustness and musical significance. Two experimental
   hypotheses fail in significance tests; therefore the following null hypotheses are retained: 1) Color and non-color
   instruments have equal numbers of solo occurrences; and 2) When power instruments are present in the musical
   texture, the pitch range spanned by the entire orchestra does not become more extreme. Three SPC model
   predictions are supported through hypothesis tests: 1) Color instruments are included less often in symphonic works;
   2) When color instruments are included, they perform less often than the average instrument; and 3) Power
   instruments are positively associated with louder dynamic levels.
   Conclusion
   Instruments cluster together according to criteria other than instrumental family or even apparent acoustic similarity.
   The SPC model encapsulates some of these functional causes; it accounts for patterns of nineteenth-century
   orchestration; and it generates predictions concerning instruments‘ deployment.
   Research Implications
   The present study‘s method shows how instrumental function can be examined using a hybrid artistic-empirical
   approach. It has broad applications for the development of new musical analysis techniques and also to the
   construction of orchestration theory. Although the method of the present study defines a model of instrumental
   combination in orchestration, this approach is descriptive and should not be interpreted as prescribing fixed patterns of
   orchestration.




                                                            66
40. Neural processing of pitch as revealed by magnetoencephalography (MEG)
    Roger Dumas, AC Leuthold, Scott Lipscomb and AP Georgopoulos
    University of Minnesota

   Purpose
   We used MEG to investigate the dynamic brain mechanisms underlying pitch processing.
   Methods
   Ten human subjects listened to 24 randomized permutations of a sequence of the 24 pitches between C3 (Middle C
   on a piano) and B4 (approx. 2 octaves higher), for a total of 576 pure tones. Each of these tones was presented for
   500 ms, inclusive of 20 ms amplitude ramps up (fade in) and down (fade out). We recorded brain activity using 248
   axial gradiometers (Magnes 3600WH, 4-D Neuroimaging, San Diego, CA) at a sampling rate of 1017 Hz (bandpass:
   DC-400 Hz) . We then computed the means for each of the 24 notes (across subjects and repetitions) to obtain a
   stimulus-response curve, or pitch-processing vector (PPV), for each of the 248 MEG sensors. Finally, we calculated all
   pairwise correlation coefficients (N = 30628) between sensor PPVs: positive correlations would indicate similar PPVs,
   i.e. a similar way of processing pitch information, and vice versa for negative correlations.
   Results
   We found the following: (a) Highly significant positive and negative correlations were found. (b) Positive correlations
   occurred ~4x more frequently than negative correlations. (c) Pairs of sensors with positive correlations were clustered
   in the frontal (bilateral) and left temporo-occipital regions. Finally, (d) pairs of sensors with negative correlations were
   typically in opposite hemispheres; these negative correlations occurred more frequently in frontal regions than in
   posterior regions.
   Research Implications
   Results indicate that pitch is being processed by segregated neural networks with intricate interrelations that are
   currently being further analyzed.


41. Approaches to research in electroacoustic music perception
    Lonce Wyse
    National University of Singapore
   Purpose
   Much music written today is of the electroacoustic (―EA‖) variety, and draws on the entire domain of sound. Since the
   art is generally accepted as music, it is likely to share some perceptual mechanisms with tonal music such as those
   involved with expectation. Other mechanisms are probably shared with the processing of everyday sounds. One of the
   distinguishing features of EA music is the perceptual presence of sound sources. Whether they correspond to real
   world objects such as dogs or trains, or are entirely abstract, perceived sources play an important role in the
   organization of sound environments. Research into ―auditory object‖ formation is thus directly relevant. One goal of
   this presentation is to identify research already being conducted in related areas which are relevant to the
   understanding of electroacoustic music perception. This includes priming, distributed neural processing of objects and
   events, ecological acoustics, language, tonal music, sound processing, activation patterns derived from ERP, fMRI
   and other imaging studies, and particularly ―auditory object‖ formation. The ultimate goal is to identify directions for
   research that can more directly address electroacoustic music perception.
   Methods
   To see how sound sources can play a musical role in electroacoustic music, we consider two key aspects of music
   restated here in note-free form. The first is that music involves understanding the relationship between sounds. In
   traditional music it is harmonic and intervallic contexts that constitute these relationships. The second is that music
   involves the perceptual dynamics of expectation. Far from being musical distractions, sound sources may, through
   qualities such as structural and transformational invariances and object permanence, be the key to understanding how
   the relationships between sounds and the dynamics of expectation can still function musically when tonality is no
   longer the medium of support.
   Conclusion
   There is already a wide variety of research with important implications for understanding electroacoustic music
   perception, very little of which has been considered, let alone conducted in this context.
   Research and Educational/Implications
   An agenda for research into EA music perception should include studying the relationship between auditory object
   formation and musical experience. Among the music educational implications is the need for a new kind of ear
   training, analogous to Hindemith‘s (1946) classic Elementary Training For Musicians, which incorporates the basic
   skills important for EA creation and appreciation.
   Acknowledgement of Research Funding
   This work is supported by a grant from the Academic Research Fund, National University of Singapore for a project
   entitled, ―Listening Strategies for New Media; Experience and Expectation‖ (T208A4105).




                                                             67
42. Differential perception of pitch-ambiguous stimuli in Filipino and American listeners: The Tritone
    Paradox with new methods of data analysis
    Michael Maquilan and Barbara Luka
    Bard College

   Purpose
   Previous studies used complex tones that belong to one of the pitch classes in the musical scale (e.g., C, C-sharp, D),
   but are ambiguous as to pitch height. Individual listeners show preferences to judge some tones as higher than others
   despite their equivalent log frequencies – the ―Tritone Paradox‖. Deutsch has suggested that an individual‘s ―peak
   pitch‖ is determined by early language experience, but speakers of only three languages have been examined in
   published work, with mixed results. Other investigators suggest that stimulus construction methods have a large
   influence on peak pitch effects. We develop and apply improved methods of statistical analyses for pitch judgments on
   ambiguous tritones.
   Methods
   Participants were three groups of 15 college students defined by native language and geographic region (Cebuano,
   Cebu City, the Philippines; American English of mixed geographic origin, or American English of local geographic
   origin all currently living in the northeastern U.S.). Following audiometric screening and a pure tone test, participants
   listened to and rated pairs of ambiguous tritones as ascending or descending in pitch. The octave-related complex
   tones were taken from a CD for public use published by Deutsch (1995). Each participant completed a second
   session, administered approximately 48 hours after the first.
   Results
   The experiment yielded four principal results. We observed 1) geographic differences in judgments of pitch-ambiguous
   stimuli that 2) were stable across repeated testing. We also observed two influences of the spectral envelopes used
   to generate the tones: 3) weaker differences among the pitch classes and between the geographic groups when the
   tones were higher in overall frequency (centered on the fifth octave rather than the fourth), and 4) a reduction in the
   tendency to consider a tone high in pitch when it is near the center of the spectral envelope. The influences of
   geographic group and stimulus construction methods proved to be dissociable and roughly equal in their impact on
   relative pitch judgments.
   Conclusion
   Our finding of additive effects of a geographic peak pitch and spectral characteristics of the auditory stimuli clarifies a
   relationship that might otherwise obscure group comparisons. We believe that stronger statistical tools for defining
   group differences, developed here, will contribute to stronger tests of the relationship between peak pitch effects in
   ambiguous tritone perception and early influences of speech community.
   Implications
   Our study offers a statistical analysis that permits the comparison of magnitude of peak pitch differences across
   individuals. The new analysis permits an investigation of axial symmetry in pitch class perception.
   Acknowledgement of funding
   This research was supported by a grant from the Bard Research Fund.


43. Measurement of music intelligibility under hearing-loss and aided-hearing-loss conditions
    Martin F. McKinney
    Starkey Laboratories

   Purpose
   The perception of music is adversely affected by hearing impairment, e.g., sensorineural hearing loss, as well as by
   some audio amplification strategies intended to improve speech perception. In order to improve music transmission
   through hearing aids, we need to develop a quantitative objective measure of music transmission. There exist such
   measures for speech intelligibility (along with associated methods for predicting them from the acoustical signal), but
   there are no standard methods for quantitatively measuring the transmission of music information under the conditions
   of a corrupted music signal. The purpose of this study is to develop a method for measuring perceptual music
   intelligibility under conditions where the audio signal or its perception is degraded, either through detrimental audio
   processing or hearing impairment.
   Methods
   We first examine related measures, including test batteries for amusia, measures of general audio quality, and
   measures of music perception for cochlear implantees. The measures include tests for melodic and rhythmic
   perception, timbre perception, memory, and subjective audio quality. It is expected that some or parts of these
   measures may be sensitive to the degradation in music intelligibility caused by hearing impairment and non-optimal
   audio processing, but other aspects of the measures will not be appropriate. In addition to compiling relevant parts of
   current related measures and protocols, we will add specific tests to make the music intelligibility test battery sensitive
   to a wide range of perceptual deficits incurred through audio processing and hearing impairment. The test battery will
   be verified through a set of experiments to systematically investigate scores across a range of degradations, both due
   to hearing impairment and audio amplification and processing.

                                                             68
   Results
   The most common forms of perceptual degradation in music due to hearing impairment and processing in hearing aids
   occur in timbral aspects. Severe degradations can also affect tonal and melodic aspects of music as well as rhythmic
   aspects in the most severe cases. From the current set of related perceptual tests, the methods for measuring music
   perception through cochlear implants are the most relevant to our case (e.g., Looi et al. Ear Hear, 2008, V29, 421-
   434). Other types of tests are less relevant: tests for amusia focus primarily on melodic, rhythmic and memory
   deficiencies and are sensitive to only the most severe degradations in our case; measures of general audio quality
   tend to be too vague and not specific enough to be useful in determining specific deficits in music perception. With
   work still in progress, the final complete form of the test battery along with validation results will be presented at the
   conference.
   Conclusion
   There are currently no existing methods to test for music intelligibility under conditions of degraded perception due to
   hearing loss or suboptimal audio amplification strategies. Methods for measuring music perception in cochlear implant
   wearers offer a good starting point and can be extended, particularly in the area of timbre perception, to become
   sensitive to perceptual degradations associated with moderate and severe hearing loss and processing through
   hearing aids.
   Research and/or Educational/Clinical Implications
   A standardized method for quantitatively and objectively measuring music intelligibility enables a systematic approach
   to developing audio amplification strategies aimed specifically at improving music perception for those with hearing
   loss.


SMPC Business Meeting
5:10 – 6:00 (all SMPC members are invited to attend)




                                                            69
                                           Thursday, August 6th



Keynote II
9:00 – 10:00              Dr. Elaine Chew, Univ. of Southern California


Session 9A                Models and Theories                    (Chair: Christopher Bartlette)

9A) 10:20 – 10:40

A unified probabilistic model of polyphonic music analysis

David Temperley
Eastman School of Music

   Purpose
   This paper presents a probabilistic model of polyphonic music analysis. Taking a polyphonic note pattern (midifile) as
   input, the model identifies the metrical structure, harmonic structure, and stream structure (grouping the notes into
   melodic lines). The model also yields an estimate of the probability of the note pattern itself. The model is ―unified‖ in
   two senses. First of all, it integrates three aspects of music analysis—metrical analysis, harmonic analysis, and stream
   segregation—that in previous work have only been addressed individually. This allows the model to capture the
   complex interactions between these structures: for example, the effect of metrical structure on harmony. Secondly, the
   model unifies the general problem of structural analysis with the problem of estimating the probabilities of note
   patterns—a problem that, in turn, has implications for the modeling of ―surface‖ processes such as transcription (pitch
   identification) and expectation. Here, a Bayesian probabilistic framework is particularly helpful. In considering the
   probabilities of different note patterns, it stands to reason that we bring to bear structural considerations: roughly
   speaking, a probable note pattern is one that implies and adheres to a clear metrical structure, harmonic structure,
   and stream structure. The Bayesian approach provides a natural way of capturing these dependencies.
   Methods
   We assume a generative model in which a metrical structure is generated first; a harmonic structure and stream
   structure are then generated, where the stream structure is simply a set of streams that span certain portions of the
   piece. (The stream structure and harmonic structure are dependent on the metrical structure, but not on each other.) A
   note pattern is then generated, dependent on all three structures. In the analytical process, the model is given a note
   pattern and must recover the metrical, harmonic and stream structures; this is done using a complex search process
   which relies heavily on dynamic programming.
   Results
   The model has been tested on harmonic and metrical analysis, and performs competitively with other models in these
   domains. Work is also ongoing to integrate the model into a transcription system, which can identify pitches in a
   polyphonic audio signal.
   Conclusion and Research Implications
   This project shows the viability of a unified approach to music analysis which combines structural processes and
   surface processes into a single integrated system.


9A) 10:40 – 11:00

Bayesian inference of musical grammars using hidden markov models

Panayotis Mavromatis
New York University

   Purpose
   The problem of algorithmic grammatical inference is addressed using Hidden Markov Models (HMMs) (Rabiner &
   Juang 1986, Manning & Schütze 1999). The HMM training algorithm receives as input a sample representative of a
   musical corpus, whose features of interest are encoded as symbolic sequences. The output is a probabilistic Finite
   State grammar that models stylistic constraints among the values of musical variables.

                                                            70
    Methods
    We have adapted a HMM training algorithm whose goal is two-fold: (i) determination of the HMM topology (i.e.,
    number of states and how they are connected by transitions); and (ii) for a given topology, determination of the HMM
    parameters (i.e., transition and output probabilities). The first goal is achieved by systematic search over topology
    space using state merging (Stolcke & Omohundro 1993), state splitting (Ostendorf & Singer 1997), or a combination of
    both. The second goal is accomplished by the Baum-Welch algorithm (Rabiner & Juang 1986, Manning & Schütze
    1999). We perform the above HMM training in the framework of Bayesian model selection, using a model complexity
    prior. This prior is derived using the Minimum Description Length principle (Rissanen 1989, Grünwald 2007); it
    achieves optimal balance between goodness-of-fit and model simplicity, thereby preventing over-fitting.
    Results
    We present two applications that successfully tackle two classic problems in music computation, namely (a)
    algorithmic statistical segmentation, and (b) meter induction from a sequence of durational patterns. Application (a): A
    HMM was able to identify grouping boundaries in various samples of symbolic sequences based solely on the patterns
    of occurrences of symbol combinations. No explicit cues or annotations were present in the sequences. Among the
    test samples used was the one employed in the study by Saffran et al. (1999). In all cases, grouping boundaries were
    identified as specific states in the HMM graph. Application (b): A sample of melodies taken from Palestrina‘s vocal
    music were encoded as sequences of note durations alone, without any annotations as to their metric placement. A
    HMM was able to recover the metric structure of these melodies, with each metric position represented by a particular
    HMM state.
    Conclusion
    Our HMM training algorithm offers a powerful and flexible tool for algorithmic inference of musical grammars.
    Moreover, a HMM can go beyond expressing relations among explicitly encoded variables; the technique is also
    capable of uncovering significant latent variables (in our examples, grouping boundaries, metric placement), whenever
    the latter are implicitly manifested in statistical regularities of the explicitly represented variables, and are crucial in
    shaping their syntactical constraints.
    Research Implications
    HMM-based grammatical inference can be used to (i) refine and quantify our understanding of well-explored musical
    styles: (ii) model unexplored repertories, such as non-Western music (Mavromatis 2005). Moreover, HMMs are a
    powerful generalization of fixed-order Markov models, and as such are amenable to entropy-based analysis of musical
    style.
    Acknowledgement of Research Funding
    New York University Research Challenge Fund, 2007-08.
    References
    Grünwald, P. D. (2007). The Minimum Description Length Principle. Adaptive Computation and Machine Learning.
       Cambridge, MA: MIT Press.
    Manning, C. D. and H. Schütze (1999). Foundations of Statistical Natural Language Processing. Cambridge, MA: MIT
       Press.
    Mavromatis, P. (2005). ―A Hidden Markov Model of Melody Production in Greek Church Chant.‖ Computing in
       Musicology 14, 93-112.
    Ostendorf, M. and H. Singer (1997). ―HMM Topology Design Using Maximum Likelihood Successive State Splitting.‖
       Computer Speech and Language 11, 17-41.
    Rabiner, L. R. and B.-H. Juang (1986). ―An Introduction to Hidden Markov Models.‖ IEEE ASSP Magazine 3 (1), 4-16.
    Rissanen, J. (1989). Stochastic Complexity in Statistical Inquiry. Volume 15 of Series in Computer Science.
       Singapore; New Jersey; London: World Scientific.
    Saffran, J. R., E. K. Johnson, R. N. Aslin, and E. L. Newport (1999). ―Statistical Learning of Tone Sequences by
       Human Infants and Adults.‖ Cognition 70, 27-52.
    Stolcke, A. and S. M. Omohundro (1993). ―Hidden Markov Model Induction by Bayesian Model Merging.‖ In S. J.
       Hanson, J. D. Cowan, and C. L. Giles (Eds.), Advances in Neural Information Processing Systems 5, pp. 11-18. San
       Mateo, CA: Morgan Kaufmann.


9A) 11:00 – 11:20

An expansion of music theory through perceptual analysis in „Pictures at an Exhibition‟

Tim Bass
University of California, Santa Barbara

    Purpose
    This study attempts to increase ecological validity in perception and music theory research by using real musical
    excerpts for comparing timbral similarity and blend. The goal of this is to provide the music theorist with a new set of
    tools for analyzing timbre and orchestration.


                                                              71
   Methods
   The perceptual study consisted of three parts that measured two separate timbral characteristics. The first part
   gathered similarity ratings of the eleven primary instruments used in Ravel's orchestration of Pictures at an Exhibition
   by Modeste Mussorgsky in order to establish a basic timbral palette. These instrument sounds were taken from Finale
   2008 using the Garritan Orchestral Library. Following work by Kendall (2004), the second part of the study gathered
   similarity and blend ratings of six contrived triadic harmonizations of the initial three-note motive from the opening of
   Pictures, orchestrated using bassoon, clarinet, and oboe. The third part of the study obtained blend and similarity
   ratings for musical excerpts taken from Ravel's orchestration.
   Results
   The single instrument similarity ratings were subjected to multidimensional scaling. As in previous research, the
   primary dimension of the two-dimensional configuration correlated strongly with spectral centroid. Groupings of the
   triadic configurations from the second part of the study were based on the position of the oboe with respect to the
   other two instruments. This expands on Kendall's study (2004) that found that the top voice shifted the triads along a
   ―nasality‖ dimension in the MDS representation, but which did not address the clustering of triads based specifically on
   the position of the highly nasal oboe within the triad. The blend ratings of the second and third part of the study agreed
   with Kendall and Carterette (1990) and Sandell (1995) that single instrument similarities are a good predictor of blend.
   Conclusion
   The data gathered can be used in the analysis of Pictures at an Exhibition to more precisely define and describe the
   perceptual effect of Ravel's orchestration. Ravel's orchestration takes the trumpet, one of the more salient
   instruments, and uses it as a timbral tonic, relating all other instruments to it. Ongoing research is addressing the
   blend of the trumpet specifically with other instruments used by Ravel.
   Research Implications
   This research has the potential to expand the scope of music theory by basing analysis on empirical perception
   studies rather than intuition.
   Acknowledgement of Research Funding
   This study was partially funded by a grant from the Faculty Research Assistance Program, Undergraduate Research
   and Creative Activities, College of Letters and Science, UCSB.


9A) 11:20 – 11:40

A dynamic field theory of tonality

Edward W. Large and Marc J. Velasco
Florida Atlantic University

   Purpose
   The universality of certain musical features suggests that general principles of neural computation may underlie
   particular aspects of music perception. One universal feature of music is tonality. In tonal music, a single tone provides
   a focus around which other tones are dynamically organized. This paper argues that neural oscillation gives rise to the
   perception of tonality. Neural oscillation can arise from the interaction of excitatory and inhibitory neural populations, a
   common architectural feature of the auditory nervous system.
   Methods
   Despite the fact that the physics of oscillators vary greatly, all share many universal properties, on which the current
   theory is based. These properties provide certain degrees of freedom but also significant constraints in modeling the
   perception of tonality. We exploit the universal properties of nonlinear resonance to provide predictions about the
   behavior of a dynamic resonant field that could arise from and respond to stimulation with tone sequences. Our
   predictions are based on both mathematical analyses and computer simulations of neural oscillation.
   Results
   Universal properties of nonlinear resonance predict preferences for small integer ratios and perceptual categorization,
   as well as perceptions of stability and attraction in tonal sequences. These predictions are compared with several
   empirical results including nonlinear auditory neural responses, infant and adult preferences for small integer ratios,
   and Krumhansl-Kessler stability profiles for Western and Indian listeners. The theory appears to provide satisfactory
   explanations in each case.
   Conclusion
   This approach provides a novel, neurally plausible theoretical framework for thinking about the perception of tonality.
   We further propose a Hebbian mechanism capable of learning sets of frequency relationships.
   Research Implications
   This theoretical approach challenges many current assumptions about tonality. Most importantly, it asserts that
   important aspects of tonal relationships may be intrinsic to the physics of neural oscillation. Thus, it predicts what
   kinds of tonal systems would be easy to learn, and which would be more difficult. In other words, neural resonance
   may underlie a kind of ―universal grammar‖, a set of innate constraints that shape human musical behavior and enable
   the acquisition of musical knowledge.

                                                             72
    Acknowledgement of Research Funding
    J. William Fulbright Foreign Scholarship Board and AFOSR Grant FA9550-07-C0095


Session 9B                Emotion I                              (Chair: E. Glenn Schellenberg)

9B) 10:20 – 10:40

Experienced tension in response to atonal melodies
                  1             2                  3
Alexander Rozin , Lily Guillot , and Paul Rozin
                        1                 2                            3
West Chester University , Yale University , University of Pennsylvania

    Purpose
    This study is the first in a series that investigates how the various musical parameters generate experienced tension.
    Real music is a complex web of parametric information and so does not provide well-controlled stimuli. To begin the
    series of experiments, we eliminated as many parametric variables as possible: rhythm, meter, scale-degree function,
    dynamic level, timbre, dissonance, density, and registral span. The resulting stimuli (described below) generate
    tension through pitch height and interval. Results allow us to determine how listeners‘ experienced tension derives
    from pitch height and interval and serve as a basis upon which further experiments involving more complex stimuli can
    stand.
    Methods
    In several sessions in a large classroom, participants (n = 114) listened to 24 atonal and isochronous melodies and
    rated the experienced tension of the final note of each. The 24 melodies derive from six parent melodies, each of
    which generated four variants that were identical except for the penultimate note, that is, the note preceding the one
    rated by participants. For each session, the melodies were broken into two blocks, separated by other experimental
    stimuli. The order of the melodies in each block was randomized.
    Results
    The results from the experiment demonstrate that both pitch height (r = 0.57) and interval (r = 0.44) contribute
    significantly to experienced tension of atonal melodies. A multiple regression, accounting for 72% of the variance,
    indicates that listeners depend on interval twice as much as pitch height.
    Conclusion
    Both pitch height and interval contribute to experienced tension. The results suggest that the syntactical information
    (i.e., interval) is twice as important as the statistical information (i.e., pitch height), providing a window into how
    listeners integrate absolute characteristics of musical events (e.g., loud) with contextual ones (e.g., louder than what
    happened before).
    Implications
    This study opens a line of research examining how the various musical parameters evoke felt tension. With a better
    understanding of how pitch height and interval contribute, further studies can open stimuli to include rhythm and
    meter, timbre, dynamics, dissonance, and so on, eventually leading to a better understanding of how all of the
    parameters interact to create the complexity and subtlety of musical emotion.


9B) 10:40 – 11:00

Determining feature relevance for subject responses to musical stimuli

Morwaread Farbood and Bernd Schoner
New York University

    Purpose
    This work presents a method for detecting and quantifying the relevance of individual musical features in subject
    responses to complex stimuli. Rather than using linear correlation methods, we allow for nonlinear relationships and
    multidimensional feature vectors. We first provide a methodology based on polynomial functions and the least-mean-
    square error measure then extend the methodology to arbitrary nonlinear function approximation techniques and
    introduce the Kullback-Leibler Distance (information-theoretic cross-entropy) as an alternative relevance metric. In
    applying this method to data collected in a study on musical tension, we formulate a mathematically sound approach
    to determining the relative importance of individual musical features to subjects' perceptions of tension.
    Methods
    The method is demonstrated first with simple artificial data and then applied to experimental data. The artificial data
    was generated using Matlab and the experimental data was collected in a study where subjects were asked to move a
    slider on a computer interface in response to how they felt tension was changing in musical excerpts taken from the

                                                            73
   classical repertoire. Analysis of subject responses was based on the assumption that perceived tension is a function of
   various musical parameters varying over time, such as harmonic tension, loudness, pitch height, and onset frequency.
   Each excerpt was analyzed and quantified according these musical parameters. We analyzed the data using a new
   estimator, termed the Relevance Ratio, derived from arbitrary nonlinear function approximation techniques and the
   least-mean-square error metric. Additionally, we employed a probabilistic metric known as the cross entropy (KL
   Distance) as an alternative estimator.
   Results
   Functionality of the Relevance Ratio was clearly demonstrated in the case of artificial test functions, where the
   estimator correctly identified relevant features. In the case of the experimental data, salient features were extracted for
   each of the complex musical excerpts (by Brahms, Bach, and Beethoven) and matched qualitative observations.
   Conclusion
   The result of determining feature relevance using our method was successful for the artificial data as well as the
   complex empirical data. In the latter case, we were able to gain valuable insights into the importance of certain salient
   features that contributed to the perception of musical tension.
   Research and/or Educational/Clinical Implications
   Our method can be used with most types of auditory or visual stimuli and most types of responses. For example, the
   response signal can be brain activity, as measured by imaging technology, a general biological response such as skin
   conductivity, or direct subject input by means of a computer interface.


9B) 11:00 – 11:20

Facial expressions and emotional singing
                       1                          2               3                   3
Steven R. Livingstone , William F. Thompson , Lisa Chan and Frank Russo
                 1                      2                     3
McGill University , Macquarie University , Ryerson University

   Purpose
   Facial expressions are used in music performance to communicate structural and emotional intentions. Exposure to
   emotional facial expressions may also lead to subtle facial movements that mirror those expressions. This study
   focused on the nature and significance of facial expressions during the perception, planning, production, and post-
   production of emotional singing.
   Methods
   In experiment I, seven participants were recorded with motion capture as they watched and imitated phrases of
   emotional singing. In experiment II, four different participants were recorded using facial electromyography (EMG)
   while performing the same task. Participants saw and heard recordings of six musical phrases each sung with happy,
   sad, and neutral emotional connotations. They then imitated the target stimulus, paying attention to the emotion
   expressed. Facial expressions were monitored: (a) during the target; (b) prior to imitation; (c) during imitation; and (d)
   after imitation. In experiment I, two facial features were relevant to conditions: eyebrows & lip corners. A third feature,
   lower lip, acted as a control to separate vocalization from emotion-related movement. In experiment II, zygomatic
   (positive) and corrugator (negative) muscle activity were captured.
   Results
   Experiment I: Movement was observed in all epochs. There was significant eyebrow movement in the sad condition,
   and lip corner movement in the happy condition. Lower lip movement did not differ across emotion conditions. In
   experiment II, activity was reported as a composite of zygomatic and corrugator activity (sum of zygomatic in happy
   trials and corrugator in sad trials). The neutral composite was the sum of zygomatic in neutral trials and corrugator in
   neutral trials. Emotionality was significant across all four epochs, with more activity in the emotion composite.
   Conclusion
   Expressive activity was observed in all epochs, implicating a role of facial expressions in the perception, planning,
   production, and post-production of emotional singing.
   Research Implications
   Facial expressions of emotion supplement the acoustic channel of music in multiple ways. Movements were evident in
   all four epochs, suggesting that facial expressions function for (a) motor mimicry and/or emotional synchronization (b)
   motor and emotional planning; (c) direct emotional communication; and (d) extending the time frame of emotional
   communication beyond that conveyed by acoustic cues.
   Acknowledgement of Research Funding
   ARC Discovery and MQSIS Infrastructure grants awarded to the second author. Natural Science and Engineering
   Research Council of Canada grant awarded to last author.




                                                             74
9B) 11:20 – 11:40

Emotional communication in music: Relative contributions of performance expression and melodic
structure
Lena Quinto and William F. Thompson
Macquarie University

   Purpose
   Emotional meaning is communicated by performance expression (Juslin, 2003) and melodic structure (Thompson &
   Robitaille, 1992). Given an emotionally neutral phrase, what is the scope of emotional communication that can be
   achieved by performers? How is emotional communication altered when musicians manipulate both melodic structure
   and performance expression? Experiment 1 examined the ability of performers to convey emotions through
   performance expression. These data were compared to the capacity of speakers to convey emotions prosodically.
   Experiment 2 addressed the capacity of vocalists and violinists to communicate emotion using (a) performance
   expression; (b) melodic structure; and (c) performance expression and melodic structure.
    Methods
    Experiment 1. Three musicians (guitarist, flutist, violinist) performed an emotionally neutral seven-note composition
   with the intention to express anger, fear, sadness, happiness, and tenderness. Performances were presented to 25
   listeners. Experiment 2 compared emotional communication by performance expression, melodic structure, or both.
   Twenty vocalists or violinists performed emotionally neutral phrases with the intention to express the above emotions.
   They also composed emotional melodies (controlling the number of notes) and performed them with varying emotional
   intentions. Finally, composed melodies were presented with neutral expression (via MIDI). All performances were then
   decoded by a sample of listeners.
   Results
   When only cues of performance expression were available, happiness, sadness and tenderness were decoded well,
   but anger and fear were decoded below chance levels. There was no overall correlation between decoding rates for
   music (expression alone) and speech prosody. However, signal detection analyses revealed that individuals who
   discriminated anger and tenderness well in music also discriminated this pair well in speech. Experiment 2 is in
   progress but results will be available soon.
   Conclusion
   For brief phrases with an emotionally neutral structure, performers have a surprisingly restricted capacity to
   communicate emotion. The skill of decoding musical emotion may overlap somewhat with the skill of decoding
   emotion from prosodic cues.
   Research implications
   This project directly evaluates the capacity of musicians to communicate emotion through performance expression,
   melodic structure, or both. Our data suggest that the capacity of performers to introduce emotionality to an emotionally
   neutral melodic phrase is more restricted than traditionally assumed.
   Acknowledgement of research funding
   Supported by a Macquarie Postgraduate Research Fund awarded to LQ, and by a Discovery Grant by the Australian
   Research Council awarded to WFT.
   References:
   Juslin, P.N. (2003). Five facets of musical expression: A psychologist‘s perspective on music performance.
      Psychology of Music, 31, 273-302.
   Thompson, W.F., & Robitaille, B. (1992). Can composers express emotion through music? Empirical Studies of the
      Arts, 10, 79-89.


Session 10A              Harmony                                (Chair: Siu-Lan Tan)

10A) 1:30 – 1:50

The perception of predominant chords

Jenine Lawson
Eastman School of Music

   Purpose
   In tonal theory, certain chords are described as being ―pre-dominant‖ in function, as they generally precede the
   dominant chord (V). It is often stated that some predominant chords are ―stronger‖ or more attracted to V than others.
   This study asks listeners to rate the perceptual attraction of predominant chords as they resolve to V, and compares
   these responses with theoretical measurements of attraction.


                                                           75
    Methods
    Twenty-one freshmen music majors at Ithaca College served as pilot subjects; further experimentation is expected in
    2009. In each of 40 randomized trials, listeners heard three sonorities. All chord progressions were in C major/minor
    and heard in a synthesized piano timbre. Sonority 1 was tonic, sonority 2 was some predominant chord, and sonority 3
    was V, each lasting 1.5 seconds. After each trial, listeners were asked to rate how well sonority 2 was attracted to the
    final sonority on a seven-point Likert scale.
    Results and Conclusions
    Results support typical theoretical assumptions. Predominant chords containing dissonances were more attracted to V
    than those without (t(20) = 3.5, p = .002). Chords from the minor mode were more attracted to V than those from the
    major mode (t(20)=3.1, p=.005). Chords in inversion were more attracted to V than chords in root position (t(20) = 2.6,
    p = .019). Chords containing chordal sevenths were more attracted to V than chords without sevenths (t(20) = 3.05, p
    = .006). Finally, chords containing non-diatonic notes such as #^4 and b^2 were more attracted to V than chords
    without these tones (t(20) = 6.0, p = .000). Listener responses were compared with theoretical measurements of
    attraction: There was no significant correlation between listener responses and predominant chords that move to V by
    the fewest number of semitones (r = -.198, p 0 =.233). Neither was there a significant correlation between listener
    responses and Cohn‘s (1998) DVLS (see also Santa, 2003) or Monahan‘s (2008) KDIs. Interestingly, a moderate
    correlation was found between listener responses and Lerdahl‘s (2001) attraction algorithm (r = .454, p = .004).
    Educational Implications
    Music theory textbooks attempt to address the issue of which predominant chords are more attracted to V, but there
    are often inconsistencies. For example, Laitz (2001) implies that predominants are stronger when they share notes of
    the V chord: When describing the difference between IV and ii6, he states that ii6 is more attracted to V because it
    shares a note in common with V (i.e.-^2) (p.227). He also states that the German6/5 chord ―is usually the last event
    before the dominant,‖ even though they do not share any notes in common (Laitz,-2001, p.507). By better
    understanding which predominants are more strongly attracted to the dominant, we can improve pedagogy of
    common-practice harmony, dictation, and composition.
    List of References
    Cohn, R. (1998). Square dances with cubes. Journal of Music Theory, 42(2), 283-296.
    Laitz, S. (2001). The complete musician. New York: Oxford University Press.
    Lerdahl, F. (2001). Tonal pitch space. New York: Oxford University Press.
    Monahan, S. (2008). The Tristan progression as an energetic voice-leading paradigm: A study in kinetic displacement
       intervals (KDIs).‖ Presented at the 31st Annual Meeting of the Society for Music Theory: Nashville, TN.
    Santa, M. (2003). Nonatonic systems and the parsimonious interpretation of dominant-tonic progressions. Theory and
       Practice, 28, 1-28.


10A) 1:50 – 2:10

Preparing unexpected harmonies in piano and organ performances

Christopher Bartlette
Baylor University

    Purpose
    In a previous study (Bartlette, 2008), I examined the effect of harmonic distance – the extent to which a harmony is
    ―closely‖ or ―distantly‖ related to a context – on expressive timing and dynamics in piano performance. I concluded that
    distant chords were performed with delayed onsets and greater chord lengths, when compared to close chords in the
    same contexts. In this paper, I present a post hoc analysis of Bartlette (2008) – as well as a new study of organ
    performance – that considers alterations in performance expression before and after a distant, unexpected harmony
    occurs.
    Methods
    Twelve graduate piano students from the Eastman School of Music, and six undergraduate and graduate organ
    performance majors at Baylor University, performed ten short musical excerpts, grouped into five pairs. Within each
    pair of excerpts, one chord was altered such that the chord was either ―close‖ or ―distant‖ from its preceding chord; all
    other chords were held the same. The participants briefly prepared each excerpt before recording a performance in a
    ―musical fashion.‖ Organists performed using only the manuals; pedals were not used for the performances. The
    participants performed on MIDI-equipped instruments, so that performance data could be recorded and analyzed. The
    analysis considered data within one musical phrase, up to six beats before and after an altered chord.
    Results
    In the piano performances, distant altered chords – and their adjacent chords – had significantly greater chord lengths
    (p < .001). However, the lengths of the chords 4 and 5 beats before distant altered chords were significantly shorter
    than the same chords when played before close altered chords (p = .02 and .04, respectively). The organ
    performances have similar outcomes, although the results do not have the same level of significance: distant altered
    chords were lengthened (p = .09), and chords 4 beats before distant altered chords were shortened (p = .07).

                                                             76
   Conclusion
   Unexpected harmonies have an effect on performance that reaches beyond a single chord. In this study, altering one
   chord resulted in changes in timing several beats before the chord. It is intriguing that distant chords – which were
   lengthened in performance – were accompanied by an acceleration in the preceding chords (relative to their close
   counterparts).
   Research Implications
   Previous research on the relationship between harmony and performance expression has focused exclusively on the
   performance of single chords. This study serves as a reminder that we must consider phenomenological implications,
   as well as the role of the performer in the preparation of unexpected events.


10A) 2:10 – 2:30

Visualization of the Harmonic Structure of Music

Norman D. Cook
Kansai University

   Purpose
   The purpose of the present study was to use a psychophysical model of harmony perception (Cook, 2002; Cook &
   Hayashi, 2008) in order to visualize the overall harmonic structure of extended musical pieces on a 2D grid.
   Methods
   Music, by its very nature, is extended over time. Schenker (1954) showed that the temporal ―unfolding‖ of music could
   be compressed to three different levels – each of which retained certain fundamental tonal features of the original
   piece. In the present study, a similar 3-level compression of music to two-dimensional ―harmonic grids‖ has been
   achieved using a psychophysical model of harmony perception. The model includes both 2-tone interval effects
   (dissonance) and 3-tone triad effects (tension and modality). Using those quantitative features, the harmonic structure
   of a musical piece can be compressed onto a 2D grid on which foci of activation correspond to the chords and chord
   sequences used. The 2D grid is colorized according to the affective character (modality) of the harmony.
   Results
   The harmonic ―footprint‖ that is obtained for a given musical piece is a unique 2D representation of its tonal structure.
   Several examples of the similarity/dissimilarity of the harmonic footprints of several songs in popular music are
   discussed: e.g., George Harrison‘s ―My Sweet Lord‖ and the Chiffons‘ ―He‘s So Fine‖.
   Conclusion
    Unlike various ―screen-saver‖ visualizations of music, the present visualization technique has a solid foundation
   based upon three psychophysical properties of harmony. The first is the well-known consonance or dissonance of
   intervals – quantifiable using the 2-tone ―dissonance curves‖ developed in the 1960s. The second is the ―tension‖ of
   triads – quantifiable as the relative symmetry of 3-tone chords. The third is the major/minor ―modality‖ of triads –
   quantifiable as the relative asymmetry of 3-tone chords.
   Research and/or Educational/Clinical Implications
   The success of our psychophysical model containing both 2-tone and 3-tone components in describing well-known
   features of simple triads (major and minor modality, and chromatic tension) indicates the importance of ―bottom-up‖
   acoustical effects in the perception of music. Without the 3-tone component, neither the stable/unstable character of
   triads nor their major/minor modality can be explained. I conclude that there is a strong acoustical foundation
   underlying both popular and classical Western diatonic music.


Session 10B     Emotion II                            (Chair: Gabriela Ilie)

10B) 1:30 – 1:50

Like it or lose it: Listeners remember what they like

Stephanie M. Stalinski and E. Glenn Schellenberg
University of Toronto

   Purpose
   One of the most well established findings in memory research is that items that are processed deeply at time of
   encoding are remembered better than items that are processed in a more shallow manner. Here we examined
   whether such depth-of-processing effects may be driven by subject-specific mechanisms, such as preference, in a
   musical task.


                                                            77
   Methods
   Approximately 40 participants were tested in each of four experiments. Participants initially heard 24 music excerpts
   and rated how much they liked each excerpt on a scale from 1 (dislike a lot) to 7 (like a lot). Subsequently, they heard
   the same 24 music excerpts as well as 24 previously unheard excerpts and judged whether they recognized the
   excerpts and how confident they were in their judgments. Experiment 1 examined whether there was a memory
   benefit for liked items. Experiment 2 examined whether this effect was a consequence of making the initial evaluative
   ratings. Experiment 3 examined whether the effect extended across a 24-hour delay. Experiment 4 examined whether
   the effect was eliminated when deep processing was encouraged for all items.
   Results
   In Experiment 1, items that were liked were remembered better than items that were disliked or items that were neither
   liked nor disliked. In Experiment 2, the liking effect on memory was evident even when the initial encoding task
   involved rating the complexity of the excerpts, which shows that the effect is not due to focusing participants‘ attention
   on the dimension of liking. In Experiment 3, the liking effect on memory was maintained across a 24-hour delay, which
   shows that the effect is not due to short-term rehearsal of liked items. The liking effect was also evident in Experiment
   4, which shows that the effect is not due to intentional deep processing of liked items.
   Conclusion
   Across all experiments, music excerpts that were liked were remembered better than excerpts that were disliked or
   neither liked nor disliked. The effect was evident across a variety of manipulations, which highlights the importance of
   liking on memory for music.
   Research Implications
   Listeners‘ immediate subjective responses to musical items (e.g., I like this) have profound implications for how much
   attention is paid to those items, how deeply the items are processed, how strongly the items are stored in memory,
   and how easily the items are retrieved. Subjective response factors should be taken into account in future research on
   memory for music.
   Acknowledgement of Funding
   National Sciences and Engineering Research Council of Canada; Ontario Graduate Scholarships


10B) 1:50 – 2:10

Openness to experience moderates the effect of exposure on liking

Patrick Hunter and E. Glenn Schellenberg
University of Toronto

   Purpose
   Previous studies have shown an inverted-U association between exposure and liking for music (Schellenberg, Peretz,
   & Vieillard, 2008; Szpunar, Schellenberg, & Pliner, 2004). Berlyne (1970) suggested that this was due to a preference
   for stimuli with moderate arousal potential: very unfamiliar stimuli are too arousing, overly familiar stimuli are not
   arousing enough. In the present study we sought to determine whether personality moderates this association. We
   expected that higher scores on Openness to Experience would be related to greater liking for novel pieces and lower
   liking of pieces that were over-exposed.
   Methods
   In an initial exposure phase, 80 undergraduate participants heard six excerpts from recordings of classical music with
   different exposure frequencies (2, 8, or 32). They subsequently heard the same pieces again along with six novel
   excerpts and made liking ratings for each. Participants also completed the Big Five Inventory (John & Benet-Martinez,
   1998).
   Results
    Using median splits on the personality factors, we ran mixed-design ANOVAs with personality as a between-subjects
   variable and number of exposures as a within-subjects factor. Of the big five factors, only Openness interacted with
   exposure. Results confirmed hypotheses: high levels of openness were associated with greater liking for novel pieces
   and lower liking for very familiar pieces.
   Conclusion
   The effect of exposure on liking varies with the listener‘s degree of openness. Listeners who were low in openness
   showed the usual inverted-U association: an initial increase in liking as a function of exposure followed by a decrease.
   Listeners who were high in openness did not show a significant increase in liking with more exposures, although liking
   decreased after a large number of exposures.
   Research Implications
   The results highlight the role of individual differences in personality on liking for novel pieces of music. Openness to
   experience is of particular interest because it is linked to cognitive variables (e.g., IQ). Future research could examine
   the role of personality on liking for music from familiar and unfamiliar musical styles.
   Acknowledgement of Research Funding
   Social Sciences and Humanities Research Council of Canada

                                                            78
10B) 2:10 – 2:30

Affective responses to tonal modulation to selected steps

Marina Korsakova-Kreyn and W. Jay Dowling
University of Texas, Dallas

   Purpose
   The study used bipolar adjective scales to measure the intensity of affective responses and perceived tension to
   modulating stimuli.
   Methods
   Only three steps were selected as targets of modulation: the Subdominant (5), Dominant (7), and step 8. Each step
   was represented by eight progressions that were balanced for melodic contours of the soprano and bass lines, and by
   eight real music excerpts. All stimuli were in the major mode. Sixty five participants, 49 females and 16 males, heard
   two sets of stimuli: 24 eight-chord progressions written by the principal experimenter and 24 brief real music excerpts
   selected from classical piano compositions.
   Results
   The results indicate differentiated affective responses to the different modulations and their dependence on key
   proximity. Participants perceived modulations to the relatively distant step 8 as the most tense, compared to the close
   modulations to the Subdominant (5) and Dominant (7). This association between increase in key proximity and
   increase in perceived tension is in agreement with a theoretical model of key proximity based on the circle of fifths. In
   addition to the higher tension ratings, modulations to step 8 were perceived as ―colder‖ and ―darker‖ than modulations
   to the Subdominant (5) and Dominant (7), thus showing a link between negative synaesthesia-related ratings and
   higher tension ratings. Modulations to the Dominant (7) were perceived as ―happiest," and modulations to the
   Subdominant (5) were heard as ―weaker‖ than modulations to the Dominant. These findings are associated with
   asymmetry in perceived pitch proximity related to the direction around the circle of fifths, and are in agreement with
   musicological research recognizing a subdominant sphere as ―weaker‖ than the dominant region. This finding also
   provides corroborating evidence to previous studies showing asymmetry in perceived key proximity; the asymmetry is
   related to clockwise motion around the circle of fifths versus counterclockwise motion.
   The listeners demonstrated sensitivity to contour patterns in modulations to the Subdominant (5) and Dominant (7):
   modulations with simultaneous upward motion in soprano and bass lines were perceived as ―happier‖ and ―brighter‖
   than modulations with the simultaneously falling soprano and bass lines. However, the contour patterns did not affect
   responses to the relatively distant step 8, which suggests that the effect of key proximity overrides the effect of contour
   patterns.
   Research Implications
   Overall, the results demonstrated a general similarity of responses in the real music excerpts and the harmonic
   progressions.


10B) 2:30 – 2:50

Mode, timbre, musical training, and personality influence emotional reactions to music

Laura Edelman, Patricia Helm, Benjamin Katz, and Serena Hatcher
Mulhenberg College

   Abstract:
   Musician and non-musicians listened to recordings of classical music, differing in mode and instrumentation.
   Participants rated each piece on happiness and arousal. Participants completed a Big 5 personality inventory. Mode
   and timbre both affected happiness. Timbre affected excitement. Personality and musical training interacted with the
   effects of mode and timbre.
   Purpose:
   Extensive research has been done on the effects of music on emotion (see Jackendoff and Lerdahl, 2006 for a
   review). However, studies exploring the effects of instrument timbre on mood have shown conflicting results. It is
   hypothesized that the timbre of an instrument playing a piece will cause the piece to be happier or sadder depending
   on the number of overtones that instrument has. The timbre is also is hypothesized to affect the perceived excitement
   of a piece.
   Method:
   Seventy-eight participants listened to twenty-five audio recordings of classical music, differing in mode and
   instrumentation. The instruments used were violin, flute, horns, and electronic synthesizer sounds, which were
                                                             79
   recorded using a keyboard synthesizer and played on a CD player. For each piece, participants rated perceived
   happiness and arousal on a five-point scale. Participants completed a Big 5 personality inventory and answered
   questions related to years of musical training.
   Results:
   Mode and timbre both showed significant main effects on happiness. Pieces in the major mode were perceived as
   happier than pieces in the minor mode. Also, pieces played on horns were perceived as happiest and pieces played
   on violins were perceived as saddest. Interaction effects were also found between mode and timbre. While trumpets
   made major pieces sound happiest, violins made minor pieces sound saddest. There was a main effect for timbre on
   perceived excitement. Horns were rated as most exciting and violins were rated as least exciting. Musical training
   interacted with mode for excitement in which non-musicians found minor pieces more exciting than major pieces,
   differences between the modes were smaller for musicians. Musical training and extroversion had separate
   interactions with mode and timbre on perceived excitement. Conscientiousness also interacted with musical training
   and mode on excitement. A final interaction effect was found for mode, timbre, musical training, and extroversion on
   excitement.
   Conclusions and Implications:
   These findings have practical application in the daily lives of music listeners. Factors of music such as mode and
   timbre of instruments have a large effect on the emotional response and excitement to music. Also, listeners‘
   individual characteristics, such as their personality and amount of musical training, also have a large effect on how
   they perceive music. These findings can be generalized to fields such as music therapy and advertising which use
   music to induce mood in target audiences. In knowing the personality of a target, different musical factors might be
   used to alter responses to ads or therapy sessions.


Poster Session III
3:00 – 5:00

44. Evolutionary origin of music from Lucy to Bach: from first steps to sound pleasure and emotions
    Mark Riggle
    Casual Aspects, LLC

   A new theory for the evolutionary origin of music is proposed that starts with a pleasure that motivates learning to
   walk, then shows the evolutionary chain that leads to music and the role of music emotions. To motivate learning to
   walk, we have a pleasure from rhythmic vestibular stimulation where the brain mechanism created to generate that
   pleasure has a side-effect of generating pleasure from rhythmic sounds and entrainment. This may originate 5 million
   years ago with Australopithecus. Males who can supply a pleasure to females will gain a reproductive advantage.
   This leads to a strong selection force on the ability to create rhythmic sounds and the behavior of creating those
   sounds. These evolutionary forces lead our ancestor species to music along with great cognitive abilities and
   pleasures of creation. One stage of evolution began selecting for generating better music because it indicated better
   cognitive capacity. A strategy to improve that musical generation is to learn new music generating rules from other
   people's music which requires remembering music details. Emotions increase memory storage and recall through
   interactions of the hippocampal formation, amygdala, nucleus accumbens, and ventral tegmental area. Dopamine is
   needed in those interactions. Episodic memory works differently for happy and sad events and the structure of music
   is different for happy and sad music. Musical structure for sad music may be best remembered by the sad oriented
   detail memory and happy music by a more gist oriented semantic memory. Thus, to generate better music, evolution
   reused parts of the emotional circuits to prime musical memory so that we may learn new rules for music generation.
   A musical chill is likely dopamine and it lets the hippocampus remember. The emotions we feel with music are a side-
   effect of that memory enhancing emotion reuse. A wide variety of studies provide supporting evidence. The theory
   shows multiple distinct pleasure pathways for music. The conclusion is music making was a major factor in evolving
   our species over 5 million years.


45. Feeling the music: development of a new scale to predict strong physiological responses to music
    listening
    Gillian M. Sandstrom and Frank A. Russo
    Ryerson University

   Purpose
   In a previous study, we induced stress by telling participants they would need to make a speech. After dismissing the
   speech task, we administered a music intervention. Our goal was to examine the effect of musical characteristics
   (valence and arousal), and individual differences. We found that absorption, a measure of how intensely one tends to
   experience emotional events, was a significant predictor of the extent of physiological recovery from stress. However,
   the Tellegen Absorption Scale (TAS) is not specific to music, and participants found many of the questions difficult to
   understand. In the current study, we take the first steps towards developing a new, music-specific scale, intended to
                                                           80
   predict strong physiological responses to music. We developed a long list of preliminary questions, based on the
   original TAS questions as well as concepts that have been empirically linked to absorption. We hypothesize that there
   are at least two factors involved in absorption in music: the ability to feel the music (empathy) and the willingness to
   allow oneself to feel strong emotions.
   Methods
   Participants will listen to 12 music excerpts (3 excerpts conveying emotion from each quadrant of Russell‘s
   circumplex) while we monitor their galvanic skin response (GSR), heart rate, respiration rate, and facial
   electromyography (EMG). Each music excerpt will be preceded by a baseline period of white noise, and followed by a
   washout period of silence. Participants will also be asked to fill out questionnaires: our new scale, the TAS, two
   alternative music absorption scales (Kreutz, Schubert & Mitchell‘s Empathizer/Sympathizer scale, 2008; Nagy &
   Szabo‘s Musical Involvement Scale, 2002) and a standard, multi-dimensional empathy scale (Davis, 1980).
   Results
   We will present a subset of questions that warrants further testing. Derivation of the subset will be based on
   correlations with physiological reactivity as well as factor loadings. We will look at relationships between the new scale
   and possible alternative scales, and will report the feasibility of using these alternative scales as predictors of
   physiological reactivity to music.
   Research and/or Educational/Clinical Implications
   This new scale will be useful to researchers looking at emotional reactions to music. It can be used to assess the
   impact of individual differences, or for screening purposes, to pre-select participants that are predicted to show strong
   physiological responses to music.
   Acknowledgement of Research Funding
   This study was supported by a SSHRC training grant awarded to the first author and an NSERC research grant
   awarded to the second author.


46. Gender and aging affect experiencing arousal in lyrics
    Hui Charles Li, Psyche Loui and Gottfried Schlaug
    Beth Israel Deaconess Medical Center and Harvard Medical School

   Purpose
   Singing is enjoyed by people around the world, regardless of age or gender. One reason people enjoy songs is for
   their emotional content. While the sharing of resources between language and music has been examined in various
   studies, little is known about the effects of lyrics on our musical experiences, especially of the perception of emotional
   arousal in music. The present study was designed to investigate the effects of lyrics on the perceived level of arousal.
   Additionally, we examined the modulation of lyrics-induced differences in arousal by demographic variables of age and
   gender.
   Methods
   Fifty participants (25 female, 25 male; median age: 37, range: 19-83) were recruited from the greater Boston area.
   Participants listened to 32 one-minute-long musical excerpts from karaoke box sets from contemporary Billboard hits.
   The 32 stimuli consisted of 16 excerpts of the full song with lyrics, and 16 excerpts of the same songs with the lyrics
   removed. For each excerpt, participants provided self reports of arousal ratings on a five-point scale, chills, and
   intense emotional responses experienced.
   Results
   Musical excerpts with lyrics were significantly more arousing than the instrumental version (F(1,96)=1389, p<0.001).
   An interaction between effects of lyrics and gender was also significant, with females rating excerpts with lyrics as
   more arousing in general (F(1,96)=11.9, p<0.01). A two-way interaction was found for arousal ratings between gender
   and age (F(1,96)=4.17, p<0.05), with older females (aged 37 – 83) being more aroused by lyrics than younger females
   (age 19 – 37), but older males being less aroused by lyrics than younger males. Music with lyrics also significantly
   elicited more chills (t(49)=-4.13, p<0.001) and intense emotional responses (t(49)=-5.29, p<0.001) in participants.
   Conclusion
   Music with lyrics is perceived as more emotionally arousing than instrumental music alone. Females were more
   influenced by lyrics than males, with older females being disproportionally more sensitive to the emotionally arousing
   effects of lyrics. This finding suggests that gender and aging both affect our perception of arousal in music, even in
   response to songs that are controlled in all aspects but the presence of lyrics.
   Research Implications
   Results suggest that the lyrics added to instrumental music stimulate and enhance emotional reactions to music, with
   older females being most susceptible. The neural interactions between music and language, and how they combine to
   elicit emotional responses, may depend on gender and aging factors.
   Acknowledgement of Research Funding
   This work was supported by a grant from Sourcetone, LLC to BIDMC (PI: GS). PL also acknowledges support from
   the Grammy Foundation.




                                                            81
47. Individual differences in the effects of spectral centroid on perceived pitch
    Michael D. Hall, Jonathan Schuett, Christopher Becker, and Elyse Ritter
    James Madison University

   Purpose
   There are numerous demonstrations of individual differences in pitch given spectral changes in timbre as a function of
   musical training, such as the tritone paradox (e.g., Repp, 1997), pitch of the missing fundamental (Seither-Preisler,
   Johnson, Seither and Lütkenhöner, 2007), and examples from speeded classification (Pitt, 1994). Yet, a unified
   explanation for these findings has not been considered. Timbre research has revealed that brightness (which highly
   correlates with spectral centroid) makes separate contributions to timbre relative to spectral envelope shape (Hall &
   Beauchamp, in press), and pitch is affected by the locus of harmonics, which impacts brightness (Singh & Hirsh,
   1992). Some listeners (particularly those without musical training) may frequently attribute changes in brightness,
   presumably in response to spectral centroids, to pitch changes. Two experiments were conducted to evaluate this
   potential role of the spectral centroid.
   Methods
   In both experiments listeners judged whether pitch descended, remained the same, or ascended across pairs of 500
   ms tones. A standard tone, A4, was present on every trial (with order counterbalanced). Comparison pitches included
   A4, A#4, and F5. In Experiment 1 listeners responded to tones with a static spectral envelope derived from a violin,
   and comparison stimuli were presented with, or without, the fundamental frequency. The same listeners completed
   Experiment 2, a corresponding evaluation for some Shepard tones from Deutsch‘s (1995) CD and used to
   demonstrate the tritone paradox (e.g., Deutsch, 1987). In addition to a no-change condition (the standard twice), there
   were two timbre-change conditions. One contained a shift in spectral centroid produced by either a missing
   fundamental (Exp. 1) or a shift in spectral envelope (Exp. 2); in the other condition these comparison timbres were
   low-pass filtered to match the standard‘s spectral centroid. Pitch-change trials (p=0.5) were accompanied by either
   congruent (different centroid) or incongruent (same centroid) timbre changes (p=0.5).
   Results
   Preliminary results confirmed individual differences in the degree to which pitch judgments were influenced by timbre
   shifts, even in the absence of pitch changes. Furthermore, across both experiments certain listeners perceived the
   direction of particular pitch changes to reverse when the comparison tone was filtered to match the standard‘s
   centroid.
   Research and/or Educational/Clinical Implications and Conclusion
   These findings suggest that brightness perception could explain individual differences in pitch observed for Shepard
   tones and missing-fundamental stimuli. Brightness might also account for poor pitch-matching abilities, suggesting that
   initial training might benefit from eliminating spectral differences in timbre. Possible dependencies on musical training
   and other factors will be discussed.


48. The minor 3rd conveys sadness in speech prosody, but interacts with pitch height
    Meagan E. Curtis and Jamshed J. Bharucha
    Tufts University

   Purpose
   Previous research has revealed that music and speech utilize the interval of a minor 3rd in the communication of
   sadness. The prosody of sad, bisyllabic American English speech samples tends to contain a downward interval
   approximating a minor 3rd. Emotional ratings of the speech samples revealed that the occurrence of the minor 3rd
   was associated with the perception of sadness more strongly than cues such as intensity, mean fundamental
   frequency (F0), and duration. However, given that the minor 3rd was a typical feature of the speech samples tested,
   we questioned whether the minor 3rd actually caused the raters to perceive sadness, or if the correlation was simply
   due to the typicality of this feature in the sad speech samples.
   Methods
   We tested the association between the minor 3rd and perceived sadness by creating synthetic ―sad‖ speech samples
   in which the size of the interval was modulated (as was the mean F0), but all other acoustic factors were controlled
   and were consistent with the acoustic properties of sad speech identified in our previous research. Participants rated
   these speech samples on a scale from 1 to 7 for how strongly each conveyed anger, happiness, and sadness.
   Results
   Mean F0 predicted a large proportion of variance on each rating scale. Sadness was associated with a low mean F0
   and happiness was associated with a high mean F0. Even speech samples that had an interval of a minor 3rd were
   perceived as happy when vocalized with a relatively high mean F0. The variance attributed to the mean F0 was
   regressed out of the ratings to determine whether interval size was a significant predictor when mean F0 was
   controlled. The minor 3rd was positively associated with perceived sadness. The major 3rd was positively associated
   with perceived happiness.



                                                            82
   Conclusions
   Given the controlled nature of the experiment, we can conclude that interval size had a causal effect on the perception
   of sadness and happiness, but that it is secondary to mean F0.
   Clinical Implications
   It is possible that the minor 3rd may be a typical feature of speech in depressed individuals, although further research
   is needed to assess this possibility. Given the implicit association between the minor 3rd in speech and perceived
   sadness, it is likely that clinicians can be taught to attend to this feature of speech and, if it proves to have diagnostic
   value, use it as a measure of depression.


49. Events in music: Audience activity analysis through continuous ratings of experience
    Finn Upham and Stephen McAdams
    McGill University

   Purpose
   Continuous ratings of musical experience have been collected over the past several decades, using a number of
   devices to rate tension, aesthetic response, and different dimensions of emotion in real time. When considering a set
   of responses simultaneously, it has been common practice to study the mean rating as a function of time. This paper
   proposes an alternative measure to explore the population's, or audience's, experience over the course of the musical
   stimulus by considering the proportion of participants showing changes in rating as a function of time. This approach is
   referred to as "activity analysis" because it describes the distribution of active expressions of experience from
   participants moment by moment.
   Methods
   Examples of this approach use the continuous ratings of experienced emotional force collected by Stephen McAdams,
   Daniel Levitin, and others from participants attending a live concert of the Boston Symphony Orchestra and a digital
   audiovisual reproduction of the same concert in Montreal. We consider the proportion of participants changing their
   ratings, either increasing or decreasing the rated emotional force, in time intervals of three seconds for each
   population and piece. Numerical transformations are applied to compare the two populations' activity and the degree
   of disagreement among participants' rating changes as functions of time.
   Results
   Numerical and graphical presentations exhibit patterns of the populations' behavior over time. Despite the wide
   variation in individual rating profiles, many similarities emerge between the distributions of rating changes of the two
   populations. This result suggests that music has some predictable effects on audiences as a whole.
   Conclusion
   This measure of activity in time enables the comparison of emotionally catalytic moments in music and the exploration
   of a pluralistic representation of musical experience. Activity analysis presents different information than mean ratings
   in time by describing the popularity of changes rather than the sum of their strength.
   Research Implications
   Studies of subjective ratings in real time could extract more information from responses by considering the distribution
   of variation in the measures of experience as a function of time. Temporal concentrations of changes in ratings point
   to where music strongly excites change and where it maintains some degree of stability of experience, giving reasons
   to explore musical stimuli at specific moments for their consistency of effects on participants.
   Acknowledgement of Research Funding
   Natural Sciences and Engineering Research Council of Canada (NSERC), Canada Research Chair Program


50. An association between breaking voice and grief-related affect in Country & Western songs
    Brandon Paul and David Huron
    The Ohio State University

   Purpose:
   A feature of crying or grieving speech is the so-called "breaking voice" -- an abrupt shift in pitch register arising from
   alternations between modal (normal) and falsetto phonation. Although rare, this distinctive auditory cue can also be
   heard in sung music, notably in operatic and Country & Western vocal practice. The purpose of this study is to
   establish whether breaking voice is associated with grief-related musical content. Our hypothesis is that breaking
   voice provides one (or potentially many) affective cues in sad or grieving music.
   Methods:
   In order to minimize experimenter bias, informants (Country & Western fans) were recruited to identify commercial
   sound recordings where the singer's voice breaks. Again, in order to minimize experimenter bias, an independent
   researcher unfamiliar with the purpose of the study was asked to confirm each of the purported instances of breaking
   voice. Each confirmed example of breaking voice was paired with a control song randomly selected from the same
   album in which the vocalist's voice did not break. Printed lyrics were assembled for both the target and control songs
   and independent judges were asked to assess which of the paired lyrics (target or control) exhibited greater grief-

                                                             83
   related content.
   Results:
   Results are pending.
   Conclusion:
   We predict a significant association between breaking voice and assessments of greater grief-related content in the
   lyrics for the sampled Country and Western songs.
   Research Implications:
   A number of researchers have investigated music in the context of mourning as well as specific grief-related
   repertoires, such as Russian laments (Mazo, 1994). In the extant research literature, as yet there has been no explicit
   demonstration implicating "breaking voice" as an affective auditory cue linked to grief. This study offers the first
   empirical test of this assumption.


51. Music and goosebumps: the how and the why
    Hui Charles Li, Psyche Loui and Gottfried Schlaug
    Beth Israel Deaconess Medical Center and Harvard Medical School

   Purpose
   Music has a powerful ability to induce moods and sensations. One of these intense emotional sensations is chills,
   which involves having goosebumps or shivers down the spine (Grewe et al., 2007; Panksepp, 1995). While chills
   perceived in music have been reported with their physiological and neural correlates (Blood & Zatorre, 2001), the
   psychological conditions and musical stimuli that induce chills are unclear. We aimed to address two questions: 1) are
   there musical excerpts taken from a wide variety of genres (not described as chill inducing before) that can reliably
   induce chills, and if so, 2) what are the predictors for experiencing intense emotional responses and chill sensations in
   musical excerpts.
    Methods
   140 total participants (79 females, 61 males; median age=29, range=18-82, SD=15) were recruited from the greater
   Boston area. In three different experiments, various subgroups listened to a total of 309 different one-minute musical
   excerpts varying in 11 genres, tempi, instrumentations, etc. For each excerpt, participants gave subjective ratings of
   chills, intense emotional responses (e.g., tearing) , continuous and overall emotional ratings of music in valence and
   arousal, overall liking, and familiarity. Of the 140 participants, the majority preferred rock/pop music; 21 reported their
   preferred genre to be classical music, whereas another 10 reported classical music to be their second-most preferred.
   Results
   Ten excerpts elicited chills in over 20% of participants, with one excerpt (Righteous Brothers‘ Unchained Melody)
   eliciting chills in 30% of participants. Each excerpt elicited chills from an average of 7% of participants. Positive
   correlations were observed between chills and intense emotional responses (r=0.73), and between chills and self-
   reported familiarity (r=0.42). Classical music disproportionately induced more chills than other genres (Χ²=26.6,
   p(10)=0.003).
   Conclusion
   We have identified and validated sets of musical excerpts that elicit chills and intense emotional responses in
   participants. Music that elicited chills was more familiar and induced more intense emotional responses. Additionally,
   despite classical stimuli accounting for approximately 25% of all stimuli, classical music elicited over 40% of all chills.
   Research Implications
   We identified music that elicited chills in a relatively high percentage of the population. Familiarity, liking, and overall
   valence and arousal ratings are strong predictors of chills. Results suggest that preferred pieces of music are more
   likely to induce chills due to recognition of musical structures. The identified musical excerpts may be usable for future
   studies of chills in music, and of intense emotions more generally.
   Acknowledgement of Research Funding
   This work was supported by a grant from Sourcetone, LLC to BIDMC (PI: GS). PL also acknowledges support from
   the Grammy Foundation.


52. Darker is sadder: The effects of Sul G timbre on perceived sadness
    Kelly Jakubowski and Gary Yim
    The Ohio State University

   Purpose:
   We will test the hypothesis that listeners perceive music with darker timbre as sadder than music with brighter timbre.
   Method:
   For this study, timbre will be controlled using the sul G technique on the violin, which is a musical indication to perform
   a given passage solely on the G string (the lowest string). A given pitch played in a high position on the G string,
   rather than in a lower position on another string, results in a darker timbre because the G string is thicker in diameter.
   A two-alternative forced choice paradigm will be used. Subjects will be undergraduate music students and not

                                                             84
   violinists. They will be presented with thirty passages of solo violin music, ranging from four to eight measures in
   length. Each passage will be performed live by a violinist in two versions: once solely on the G string, and once on the
   higher strings. Other than this difference, performers will be instructed to play both versions in exactly the same way.
   (The absolute pitches of the two versions are identical.) The order of the passages will be randomized, and the
   occurrence of the sul G passage as first or second will also be random. The two versions of each passage will be
   paired, and subjects will be instructed to identify which version they perceive to be sadder.
   Results:
   We anticipate that subjects will tend to select the sul G versions to be sadder than the version played on higher
   strings.
   Conclusions:
   We expect the results to be consistent with the hypothesis that music with a darker timbre is perceived as having a
   sadder affect than music with a brighter timbre.
   Research and/or Educational/Clinical Applications:
   The research of musicologists and evolutionary psychologists is often concerned with the origins of music and its
   affective influences. One theory suggests that cognitive processes related to the affective cues of music were
   developed from existing processes for discerning affect in speech. One such cue is timbre: speech researchers have
   shown that one of the characteristics of sad speech is a darker timbre than usual. We anticipate that our findings will
   support a link between perception of affect in music and speech by demonstrating that sad music is also characterized
   by a darker timbre. This extends existing research on the prosodic features of sad speech in music. These studies
   show that lower average pitch, less pitch variance, softer volume, and slower pace are all features evident in music as
   well as speech. Areas of further research may include i) the specific interactions of the above prosodic features ii)
   affective states other than sadness and iii) a comparison of neural activation patterns in perceiving affective speech
   and music.


53. Application of signal detection theory to the Montreal Battery of Evaluation of Amusia
    Molly J. Henry, Bryan T. Grushcow, and J. Devin McAuley
    Bowling Green State University

   Purpose
   The Montreal Battery of Evaluation of Amusia (MBEA) is used to assess both acquired and congenital forms of amusia
   (Peretz, Champod, & Hyde, 2003). Four of the subtests (Scale, Contour, Interval, Rhythm) involve presenting pairs of
   melodies and a same/different judgment. The remaining two subtests (Meter, Memory) involve presenting a single
   melody; listeners make a march/waltz judgment for the Meter subtest and an old/new judgment for the Memory
   subtest. Previous studies using the MBEA have relied on proportion correct (PC) as the primary measure of
   performance. However, PC measures of performance are not ideal for comparing performance across subtests,
   especially for comparisons between two-stimulus and single-stimulus designs; moreover, PC does not permit a
   distinction between measures of perceptual sensitivity and response bias. The present study used signal detection
   measures in conjunction with a confidence rating version of the MBEA to provide a more comprehensive assessment
   of MBEA performance than has been previously undertaken.
   Methods
   Listeners with a range of musical training completed confidence rating versions of the six subtests of the MBEA.
   Confidence ratings were given according to a 1-6 scale; for the same-different tasks, 1 corresponded to a ‗Sure Same‘
   rating and 6 to a ‗Sure Different rating, for the Meter task, 1 corresponded to ‗Sure March‘ and 6 to ‗Sure Waltz‘, and
   for the Memory task, 1 corresponded to ‗Sure Old‘ and 6 to ‗Sure New‘. Receiver operating characteristic (ROC)
   curves were constructed for each listener on each subtest; associated signal detection measures of performance were
   then calculated in order to take into account differences between two-stimulus and single-stimulus designs.
   Results
   There were two main results. First, slope values for z-transformed ROC curves for five subtests were close to 1,
   indicating that the variances of the two distributions comprising the decision space are similar; however, Memory slope
   was less than 1. Second, sensitivity measures derived from ROC curves showed that performance is worse for the
   two-stimulus subtests than for the single-stimulus subtests.
   Conclusions and Research Implications
   Performance on the subtests of the MBEA varies across individuals; moreover, individuals assessed to be ‗tone-deaf‘
   differ in the profile of their deficits. The use of signal detection measures with a confidence rating version of the MBEA
   establishes a more detailed set of normative data, which has the potential to provide greater insight into the nature of
   acquired and congenital amusia.
   Reference
   Peretz, I. Champod, A. S. & Hyde, K. (2003). Varieties of musical disorders: Montreal battery of evaluation of amusia.
     Annals of the New York Academy of Sciences, 999, 59-75.




                                                            85
54. When is music communication? A music communication matrix based on assumption, intention,
    and meaning construction
    Mark Shevy
    Northern Michigan University

   Purpose
   One‘s position on the nature of music meaning and communication has strong implications for the type of research
   questions one will ask or consider as valid. Some say that music does not communicate but creates experiences for
   listeners, and thus music communication research is a waste of time. Others say that music does communicate
   meanings, either musical or extra-musical. This paper provides a means of reconciling these differences, helping
   scholars and researchers of differing perspectives understand one another, and perhaps increase collaboration.
    Methods
   The paper first defines meaning in terms of cognitive psychology (Osgood, Suci, & Tannenbaum, 1957) and film
   theory (Bordwell, 1989), then it examines models of communication such as the SMR transmission model and a model
   that places emphasis on sender and receiver intentions and assumptions (e.g., Sperber & Wilson, 1995). In this, the
   paper defines communication as the sharing of intended meaning between a sender and receiver(s). Based on these
   theories and models, the paper argues that music can have meanings that may or may not be intended by the sender
   (i.e., composer or performer), and receivers (listeners) will typically construct musical and extra-musical meanings
   regardless of the intention of the sender. The conditions that arise from the combinations of these variables are then
   organized and labeled according to whether they meet the definition of communication established in the paper.
   Results
   The analysis results in a 4x4 matrix of sender and receiver assumptions and intentions, where the intersections of
   these variables result in six types of music effects: communication, accidental communication, miscommunication,
   experience, mis-experience, and accidental experience.
   Conclusion
   There are indeed times when music should not be classified as communication, and there times when it should. There
   is also ample opportunity for receivers to construct meanings that are not intended by the sender by assuming that
   communication was intended when it was not, misinterpreting information, or constructing meaning from information
   the sender sent unintentionally. In fact, of the 16 matrix cells, only four have outcomes that meet the sender‘s goals for
   the music.
   Research and/or Educational/Clinical Implications
   The music communication matrix helps researchers to develop questions suitable for the various communication
   scenarios that exist. It can help scholars of varying views to understand one another‘s positions and see where there
   is room for each perspective in the range of communication possibilities. It can also help composers and performers
   identify how their pieces may fall short of their goals and make adjustments accordingly.


55. Cross-validation of a model for classifying musical sophistication
    Joy Ollen
    Douglas College

   Purpose:
   This study tested the predictive accuracy of a 10-item questionnaire to successfully categorize individuals as being
   more or less musically sophisticated compared with a musical expert‘s judgment of their level of musical
   sophistication.
   Method:
   The current study served to cross-validate previous work in which the researcher administered a 36-item
   questionnaire to 633 participants in three countries. The previous study sought to determine which of the questions
   would best correlate with experts‘ subjective ratings and serve as useful indicators of musical sophistication. A logistic
   regression analysis yielded a significant model with nine indicators (model chi-square = 296.133, df = 32, p < .001)
   that was able to classify 79.5% of the sample accurately. In the current study, the 10-item questionnaire was
   administered to a new sample of approximately 300 participants who ranged from being musically naïve to highly
   experienced professional musicians and who belonged to various types of groups involved in music-related
   behaviours (e.g., university music courses for non-majors, amateur choirs, professional orchestras, etc.). Group
   leaders—musical experts who work with the participants--supplied ratings of their level of musical sophistication.
    Results:
   Final results are not yet available as data collection is ongoing. Preliminary analysis performed on the current data set
   has shown the model to be classifying 70% of the sample accurately.
   Conclusion:
   The results will provide information about the generalizability of the model.
    Research Implications:
   Music researchers regularly test the hypothesis that participants will respond differently based on their levels of
   musical sophistication and use simple survey-type questions related to their musical background to classify their

                                                            86
   participants. A survey of 743 published research studies and experiments showed that the most frequently used
   indicator was participants‘ formal musical training (e.g., years of music lessons). Yet, in this researcher‘s original
   study, this indicator only categorized 62% of the participants correctly compared with the expert ratings—a much lower
   accuracy rate than the 79.5% obtained by the complete model. If the current study provides findings that are
   consistent with the original study, researchers may wish to use the 10-item questionnaire as a tool to classify research
   participants.


56. Knowledge representation in an intelligent tutoring system architecture: A computational
    exploration of expertise in counterpoint writing
    Panayotis Mavromatis
    New York University

    Purpose
   We develop a knowledge-based programming framework for implementing an Intelligent Tutoring System (ITS)
   (Polson & Richardson 1988, Forbus & Feltovich 2001) for counterpoint writing. The knowledge representation
   developed in this connection is used (i) to code counterpoint rules in Prolog from natural language specifications; and
   (ii) to implement in Prolog effective counterpoint writing strategies used by experts, as reported in concurrent verbal
   protocols. The effectiveness of such strategies is evaluated by computer simulation.
    Methods
   The ITS‘s knowledge base and inference engine are coded in the programming language Prolog. The knowledge
   representation developed is based on the theory of ontologies (Brachman & Levesque 2004, Gasevic et al. 2006) and
   expressed as RDF Schemas (W3 Consortium 2004). Experimental subjects at various levels of expertise log on to the
   ITS and are assigned a counterpoint problem. The system monitors the solution process and reports to the subjects
   any rule violations at every step. Moreover, the ITS records the order and timing of the subjects‘ actions. At the same
   time, subjects are asked to ―think out loud‖ and we obtain an audio recording of this concurrent verbal protocol (Newell
   & Simon 1972).
    Results
   The subjects‘ protocol is analyzed. The verbal descriptions they use to express their counterpoint writing strategies are
   cast into our knowledge representation scheme and are coded into Prolog. In this way, a rich set of heuristics is
   identified that dramatically improves the problem-solving process over brute-force search. These heuristics are
   subsequently tested by computer simulations of counterpoint writing as problem-space search (Newell & Simon 1972,
   Mavromatis & Brown 2008). We quantify the (in)effectiveness of each strategy as the number of decisions needed to
   complete the task, or alternatively, as the number of steps wasted in pursuing an unsuccessful solution path.
    Conclusion
   Our method offers a way of modeling computationally the knowledge and strategies that underlie expert counterpoint
   writing. Our simulations allow us to evaluate the effectiveness of human problem-solving heuristics, assessing their
   appropriateness for deployment in actual instruction.
    Research and Educational Implications
   When properly employed, ITSs can become a powerful educational tool that supplements human instruction, allowing
   the latter to focus on more creative aspects of counterpoint writing. Moreover, an ITS can be used as a tool for
   gathering data that document the problem-solving process in real time, offering a unique window into expert skill and
   its acquisition. A more fine-grained analysis of these problem-solving records, including timing data, will be pursued in
   a future work. Finally, we believe that the well-defined problem of counterpoint writing, where composers have honed
   their skills for centuries (Mann 1994), can shed valuable light on broader questions concerning the nature and
   development of compositional expertise.
   Acknowledgement of Research Funding
   New York University, Steinhardt Faculty Research Challenge Grant (Technology Award)
   References
   Brachman, R. J. and H. J. Levesque (2004). Knowledge Representation and Reasoning. The Morgan Kaufmann
       Series in Artificial Intelligence. San Fransisco, CA: Elsevier/Morgan Kaufmann.
   Forbus, K. D. and P. J. Feltovich (Eds.) (2001). Smart Machines in Education: The Coming Revolution in Educational
       Technology. Cambridge, MA: The MIT Press.
   Gasevic, D., D. Djuric, and V. Devedzic (2006). Model Driven Architecture and Ontology Development. New York:
       Springer.
   Mann, A. (1994). The Great Composer as Student and Teacher: Theory and Practice of Composition—Bach, Handel,
       Haydn, Mozart, Beethoven, Schubert. New York: Dover.
   Mavromatis, P. and M. Brown (2008). ―An Intelligent Tutoring System for Tonal Counterpoint: From Process to
       Structure.‖ Proceedings of the Fourth Conference on Interdisciplinary Musicology (CIM08), Thessaloniki, Greece, 2-
       6 July 2008, http://web.auth.gr/cim08/
   Newell, A. and H. A. Simon (1972). Human Problem Solving. Englewood Cliffs, NJ: Prentice-Hall.
   Polson, M. C. and J. J. Richardson (Eds.) (1988). Foundations of Intelligent Tutoring Systems. Hillsdale, NJ: Erlbaum.
   W3 Consortium (2004). Resource Description Framework (RDF) Specification. http://www.w3.org/RDF/ (last visited
       February 2009).
                                                            87
57. Towards a better understanding of the contrasting psychological effects of the subtonic-tonic and
    the leading tone-tonic gestures
    Ivan Jimenez
    University of Pittsburg

    Purpose
    The discussion of the subtonic that moves up one step to the tonic, a very important gesture for folk and popular
    musical styles, has been largely neglected in academic literature. This gesture has a modal connotation, and strongly
    contrasts with its tonal counterpart, the leading tone-tonic gesture. Contrast between the subtonic-tonic and the
    leading tone-tonic gestures is very important for the identity of a myriad of musical styles, and the examination of the
    contrast between these two gestures can greatly expand our understanding of those musical styles, as well as our
    understanding of tonal and modal systems in general.
    Method
    In an experiment I recently conducted with music students, modeled after David Huron‘s work on scale degree qualia
    (2006), I tested the extent to which different types of metaphors can consistently represent the difference of effect
    between the subtonic-tonic (modal) and the leading tone-tonic (tonal) gestures. In this experiment, 12 students of
    Music Theory 1 were asked to classify the effect of the leading tone-tonic and the subtonic-tonic gestures according
    to five dualities: natural vs. artificial, complex vs. simple, collective vs. individual, thick vs. thin, and harmonically
    relaxed vs. harmonically dramatic. Students were given excerpts from four relatively well-known pieces of music. Two
    versions of each excerpt, the original and a modified version, were provided: one with the subtonic-tonic gesture, and
    the other with the leading tone-tonic gesture. Students were asked to determine which side of each duality better
    described each version of the excerpt.
    Result
    The students‘ categorization was largely consistent with my own previous assessment of the effects of the subtonic-
    tonic and leading tone-tonic gestures (81% of the results matched my assessment).
    Conclusion
    The results of this experiment demonstrate that this particular set of metaphors consistently describes the effect of
    the subtonic-tonic and leading tone-tonic gestures.
    Research and Educational Implications
    This experiment also suggests that metaphors, and particularly metaphors presented as dualities can be used as
    powerful tools for the study of different theoretical topics. These dualistic pairs are more effective if the theoretical
    topics are also presented as basic dualities; the opposition between the subtonic-tonic gesture and the leading tone-
    tonic gesture being just one example of a basic duality. In addition, dualistic pairs such as subtonic-tonic vs. leading
    tone-tonic, are directly connected to classical and popular music styles, and for this reason provide a good
    opportunity to obtain a more complete picture of the possibilities of different musical structures by contrasting diverse
    musical practices.


58. Modeling the sonority of chord progressions: Toward a psychophysical explanation of the “rules”
    of traditional harmony theory
                   1                    2
    Marc De Graef and Norman D. Cook
    1                           2
     Carnegie Mellon University, Kansai University

   Purpose
   The purpose of the present study was to calculate the overall ―sonority‖ of chord progressions using a psychophysical
   model of harmony perception (Cook, 2002; Cook & Hayashi, 2008).
   Methods
   The methods were computational. Using a model of harmonic tension that is based on 3-tone effects (the tension
   inherent to triads containing two intervals of the same size, in semitone steps), the tension scores obtained from all
   triadic combinations of partials (n<9, with gradually decreasing amplitudes, 0.88^n) of two triad chords were
   calculated. The basic tension model gives a maximal tension score for any triad with two equivalent intervals (e.g., the
   augmented chord [4-4 semitone structure] or the diminished chord in root position [3-3]) and a tension score of zero
   when the intervals differ by 1.0 semitone or more (e.g., the major chord in root position [4-3] or the minor chord in root
   position [3-4]). Calculation of the effects among all combinations of upper partial triads is non-trivial.
   Results
   The computational results were compared with the cadences of traditional harmony theory (Piston, 1987), with the
   statistics on cadences for both popular and classical Western music (Huron, 2006), and with the results of ―tonal
   distance‖ obtained using the probe-tone technique (Krumhansl, 1979). The chord cadences showing the lowest levels
   of tension were those entailing movement between a tonic chord and its dominant or subdominant.



                                                            88
   Conclusion
   The total tension calculated for pairs of triads reproduced the pattern of key relationships known as the Circle of Fifths.
   The idea that the tonic, dominant and subdominant chords are ―close‖ comes as no surprise within the framework of
   traditional harmony theory, but psychophysical justification for the Circle of Fifths has been lacking. Specifically,
   calculation of the total dissonance between chord pairs does not result in this trio having the greatest consonance. Our
   results indicate that harmony perception is strongly influenced by both 2-tone and 3-tone configurations. We conclude
   that one of the most basic ―rules‖ of traditional harmony theory, as expressed in the Circle of Fifths, is not an arbitrary
   social construct, but, on the contrary, has a firm foundation in the acoustical structure of the chords themselves.
   Research and/or Educational/Clinical Implications
   We have found that the psychophysics of harmony perception can explain one of the most important regularities of so-
   called Western music. Provided only that the effects of 3-tone configurations are examined, there is no justification for
   arguing that the well known regularities of diatonic music are arbitrary cultural artifacts.


59. The effect of style-priming on harmonic expectation
    Bryn Hughes
    Florida State University

   Purpose:
   Music theorists often suggest that chord successions in common-practice music are governed by syntax. In support of
   this, cognitive studies by Krumhansl, Bharucha, and others have shown that listeners expect chord successions that
   adhere to these syntactical rules. There is less agreement among music theorists regarding rules of chord succession
   in non-common-practice music, such as blues or rock. Some suggest that the syntax is the same for both contexts
   while others propose new syntactical rules for blues/rock music. This research investigates whether listeners expect
   chord successions presented in the context of the blues/rock idiom to adhere to common-practice syntax.
   Methods:
   In this experiment, two groups of subjects (N = 56) listened to pairs of triads and rated how good each harmonic
   succession sounded. Each triad pair was primed by a brief key-confirming recording of either blues/rock or classical
   music drawn from commercial recordings. All triads were constructed with Shepard tones to eliminate the effect of
   register and perceived voice-leading. Stimuli were presented in blocks corresponding to the musical style of the prime
   to strengthen listeners‘ notions of stylistic context.
   Results:
   In both contexts, listeners preferred harmonic successions in which the relationship between chord roots reflected
   common-practice syntax. While the results showed a negative correlation between listener ratings and the use of
   chords outside the prevailing key (i.e., non-diatonic chords), the effect was significantly less pronounced in a blues-
   rock context. Additionally, style priming affected listener preference for opening and closing chords. Among the three
   primary triads, listeners preferred tonic and dominant openings over subdominant openings in a classical music
   context. Conversely, in a blues/rock context, listeners rated subdominant openings higher than both tonic and
   dominant openings. While listeners preferred tonic endings in both contexts, dominant endings were preferred over
   subdominant endings in the classical context, whereas subdominant and dominant endings were rated equally in the
   blues/rock context.
   Conclusion:
   Although listeners hold similar expectations of the relationships between successive chords in both contexts, the
   results suggest that subdominant harmony has an elevated status in blues/rock music. This likely reflects the
   statistical prominence of this chord in that repertoire.
   Research and/or Educational/Clinical Implications:
   Hierarchical theories of harmony in common-practice music often privilege tonic and dominant chords. The results of
   this study support speculations made by Ken Stephenson and Richard Middleton, who suggest that subdominant
   harmony should assume a fundamental role in theories of blues/rock music.


60. Probing the minor tonal hierarchy
    Dominique Vuvan, Jon Prince, and Mark Schmuckler
    University of Toronto

   Purpose
   Previous perceptual work on Western tonal hierarchies has not investigated the subtleties inherent in theoretical
   descriptions of the minor key. This study was designed to rigorously test cognitive representations of the three forms
   of the minor scale (natural, harmonic, and melodic), and the effects of musical context type (chordal vs. scalar)
   thereon. It was predicted that participants would be able to differentiate between the three minor types, and
   furthermore that chord contexts might facilitate cognitive representations of the harmonic minor, whereas scale
   contexts might facilitate representations of the melodic minor.


                                                             89
   Methods
   Sixteen musician participants were presented with a musical context (chordal or scalar) that established one of the
   three forms of the minor tonal hierarchy. Next, participants rated how well a probe tone (consisting of one of the 12
   chromatic pitches) fit with the preceding context, on a Likert scale of 1 to 7.
   Results
   Most importantly, and as expected, participants‘ ratings distinguished between the three minor types, producing
   unique probe tone profiles corresponding closely to theoretical descriptions of the natural, harmonic, and melodic
   minor. Contrary to predictions regarding the effect of context, however, the minor tonal profiles did not differ across
   chordal and scalar contexts.
   Conclusion
   These findings demonstrate that musically trained listeners‘ cognitive representations of minor tonalities are sensitive
   to the differences among the three minor types. This finding helps fill an obvious, if neglected, gap in the music
   cognition literature.
   Research Implications
   Previous research into cognitive representations of the minor key has often assumed that listeners perceive all minor
   forms similarly, and has neglected to distinguish between the three types. This study shows that instead, musically
   trained listeners clearly process the natural, harmonic, and melodic minor distinctly.
   Acknowledgement of Research Funding
   This research was funded by a Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery
   Grant awarded to Dr. Mark Schmuckler.


61. An analytic method for atonal music that combines Straus‟s pattern completion and associational
    models with selection criteria based on cognitive criteria
    Yeajin Kim
    The Ohio State University

    Purpose
    In the early 1980s, Joseph Straus proposed the concept of the ―pattern completion,‖ which touched upon an
    interdisciplinary linkage between cognitive psychology and musical analysis. Straus later presented the ―associational
    model,‖ in which tones interspersed in a large-scale tonal space could be interrelated through specific pitch-class
    sets and thereby established theoretical justifications for what he proposed in his earlier articles. However, the
    premises of these concepts raise some issues that still await further logical and theoretical explanation, most
    importantly the establishment of selection and segmentation criteria.
    Methods
    In this respect, I critically examine Straus's concepts and briefly survey analytic methods for twentieth-century music.
    Finally, I suggest my own analytic model, which synthesizes Straus‘s pattern completion and associational models
    with my prerequisite conditions (modified from Lerdahl‘s ―salient conditions‖), which considers the listener‘s cognition
    in the establishment of selection criteria. As a test of my synthesis, I apply this analytic methodology to two atonal
    works by Isang Yun, Glissees for Violoncello Solo (1970) and Gasa for Violin and Piano (1963).
    Research Implications
    This analytical tool may prove useful in illuminating structural secrets latent in other post-tonal music.


62. Tonality perception as auditory object perception
    Ji Chul Kim
    Northwestern University

   Background
   Tonality refers to the organized relationships of tones that give rise to the dynamic quality of musical experience such
   as the sense of orientation, direction, and closure. Although many have acknowledged the importance of the temporal
   structure in tonality perception, the prevalent view in music psychology has been mainly concerned with the pitch
   content of musical structures.
   Aims
   I propose a theory of tonality perception conceived as auditory object perception. Coherent perception requires
   segregation of the perceptual world into structured subunits or objects. A musical surface is segregated and integrated
   at multiple spectral and temporal scales to form a complex of musical objects. I argue that this perceptual process of
   object formation, along with the influence of the previously learned objects, is behind the perceived sense of tonality
   as well as the way coherent tonal structures are shaped.
   Main Contribution
   When the brain segregates a musical surface and integrates individual sound events into auditory objects, the
   elements of an object are given new perceptual qualities in the relations with other elements. I argue that in order to
   form a coherent object with a stable mental representation, perceptually salient elements should serve as the

                                                            90
   ―perceptual reference points‖ of the object and other elements are encoded in relation to these reference elements.
   Along with the pitch structure, the temporal structure of the musical surface, especially the local segmentation
   structure, plays a deciding role in the emergence of the perceptual reference points in both pitch and time dimensions.
   When the reference points of adjacent objects align in pitch and time or at least don‘t disrupt each other, multiple
   objects can be encoded against the same or congruent reference points—to use musical terms, they are heard in the
   same key and meter. The extended perceptual reference points (the induced key and meter) influence the subsequent
   object formation by generating expectations and guiding attention. In addition to the established reference points, the
   previously learned patterns stored in both short- and long-term memory also influence the encoding of the incoming
   information.
   Implications
   The proposed theory suggests that the sense of key and meter comes from the internal structure of perceptual
   objects, thus should be very weak when individual events are not perceptually integrated; temporal structures like low-
   level segmentations contribute to tonality perception; the encoding of musical structures and the induction of key and
   meter are part of the same process of object formation.
   Acknowledgement of Research Funding
   The Dissertation Year Fellowship, The Graduate School, Northwestern University


63. The time course of implied harmony perception: The effects of „what‟ and „when‟ expectations
    Jung Nyo Kim
    Northwestern University

   Purpose
   The importance of harmony perception in understanding tonal melodies has been extensively explored, but little is
   known about the processing of implied harmony. This study aims to explain 1) how each note of a tonal melody is
   harmonically interpreted and integrated with the previous and following notes as the melody unfolds and 2) how
   harmonic expectations of ‗what‘ chord would follow and ‗when‘ the chord change would occur affect the processing.
   Methods
   Thirty-six 18-tone melodies were constructed. Each was divided into six groups of 3 consecutive tones that belonged
   to the same chord. The first 15 tones implied I-V-I-ii-V and the last 3 tones (targets) implied either I (expected) or IV
   (unexpected). Also, the harmonic change to the last chord occurred either on the expected location (on the 16th note-
   position), one beat earlier than expected (on the 15th), or one beat later (on the 17th).
   A gating paradigm was used. In the first of four blocks, participants heard the initial 15 tones of each melody and in
   each following block, one tone was added to the previous fragments. After hearing each melody, they sang the last
   tone as quickly as possible and the reaction times were measured.
   Results & Conclusions
   ‗What‘: RTs for the expected chord tones (C, E, or G in C major) were faster than those for the unexpected ones (F or
   A) on the 16th. Although G can be interpreted either as I or as V, it seemed to be interpreted as I because RTs for the
   other tonic-chord tones on the 17th and 18th got faster. Also, F on the 16th can be interpreted either as IV or as V7.
   The participants seemed to interpret it as V7 because RTs for the following subdominant-chord tones got slower.
   Similarly, RTs for F or A on the 17th after C got slower because the previous C was interpreted as I. However, RTs for
   A were faster than those for F, which confirms that vi is more probable than IV after V. In short, 1) the more expected
   the targets were, the faster RTs were and 2) the clearer the implied chords became over time, the faster RTs became.
   ‗When‘: RTs were faster when the harmonic change occurred on the expected location than when it occurred one beat
   earlier or later. Moreover, RTs for the late change were faster than those for the early change.
   Implication
   This study suggests the importance of not only ‗what‘ but also ‗when‘ harmonic expectations in the online processing
   of implied harmony.
   Acknowledgement of Research Funding
   Graduate Research Grant from the Graduate School of Northwestern University




                                                            91
